
#define CUDNN_BATCHNORM_SPATIAL_PERSISTENT ((cudnnBatchNormMode_t) -1)

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11686083622283596215 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11686083622283596215(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11686083622283596215(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11686083622283596215(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11686083622283596215);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5168337037781766382 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5168337037781766382(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5168337037781766382(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5168337037781766382(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5168337037781766382);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13305954436592017712 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13305954436592017712(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13305954436592017712(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13305954436592017712(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13305954436592017712);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7413494094377169698 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7413494094377169698(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7413494094377169698(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7413494094377169698(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7413494094377169698);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8010611597266966773 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8010611597266966773(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8010611597266966773(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8010611597266966773(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8010611597266966773);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__5784546235054194395 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5784546235054194395(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__5784546235054194395(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5784546235054194395(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__5784546235054194395);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4342585526282724738 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4342585526282724738(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4342585526282724738(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4342585526282724738(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4342585526282724738);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3583236711014649906 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3583236711014649906(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3583236711014649906(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3583236711014649906(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3583236711014649906);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17883669196554119352 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17883669196554119352(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17883669196554119352(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17883669196554119352(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17883669196554119352);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2023707216666139617 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2023707216666139617(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2023707216666139617(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2023707216666139617(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2023707216666139617);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__864989698756323381 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__864989698756323381(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__864989698756323381(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__864989698756323381(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__864989698756323381);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14039327578365124088 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14039327578365124088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14039327578365124088(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14039327578365124088(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14039327578365124088);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16480555954687549610 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16480555954687549610(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16480555954687549610(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16480555954687549610(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16480555954687549610);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1614570570883668744 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1614570570883668744(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1614570570883668744(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1614570570883668744(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1614570570883668744);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2917576257556102925 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2917576257556102925(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2917576257556102925(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2917576257556102925(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2917576257556102925);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2311416209937718012 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2311416209937718012(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2311416209937718012(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2311416209937718012(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2311416209937718012);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__804175758008737418 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__804175758008737418(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__804175758008737418(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__804175758008737418(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__804175758008737418);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11042151827715344967 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11042151827715344967(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11042151827715344967(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11042151827715344967(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11042151827715344967);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1541313172970440215 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1541313172970440215(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1541313172970440215(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1541313172970440215(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1541313172970440215);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1089952119628620689 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1089952119628620689(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1089952119628620689(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1089952119628620689(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1089952119628620689);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1051350371928479748 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1051350371928479748(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1051350371928479748(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1051350371928479748(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1051350371928479748);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_SOFTMAX_FWD
namespace LAYER_CUDNN_SOFTMAX_FWD__17237527966425954981 {

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__17237527966425954981(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__17237527966425954981(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__17237527966425954981(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_INSTANCE)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_CHANNEL)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_FAST);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_ACCURATE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_LOG)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__17237527966425954981);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
}
#endif // ENABLE_LAYER_CUDNN_SOFTMAX_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15002840832598337663 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15002840832598337663(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15002840832598337663(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15002840832598337663(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15002840832598337663);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__17112761550094986552 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17112761550094986552(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17112761550094986552(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17112761550094986552(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17112761550094986552);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15361192667530395838 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15361192667530395838(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15361192667530395838(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15361192667530395838(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15361192667530395838);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2279232864101101791 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2279232864101101791(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2279232864101101791(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2279232864101101791(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2279232864101101791);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6899728321475332036 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6899728321475332036(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6899728321475332036(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6899728321475332036(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6899728321475332036);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6435854306271828225 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6435854306271828225(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6435854306271828225(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6435854306271828225(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6435854306271828225);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8530725903432360044 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8530725903432360044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8530725903432360044(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8530725903432360044(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8530725903432360044);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2783930127650749234 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2783930127650749234(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2783930127650749234(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2783930127650749234(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2783930127650749234);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6566384971007763736 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6566384971007763736(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6566384971007763736(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6566384971007763736(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6566384971007763736);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16565604257710303150 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16565604257710303150(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16565604257710303150(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16565604257710303150(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16565604257710303150);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18063090009823562892 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18063090009823562892(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18063090009823562892(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18063090009823562892(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18063090009823562892);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__11023489420309261996 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11023489420309261996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__11023489420309261996(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11023489420309261996(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__11023489420309261996);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9903265365945150577 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9903265365945150577(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9903265365945150577(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9903265365945150577(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9903265365945150577);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13509798255614053856 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      352 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13509798255614053856(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 352} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13509798255614053856(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13509798255614053856(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13509798255614053856);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4480947227720419228 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4480947227720419228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4480947227720419228(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4480947227720419228(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4480947227720419228);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15213419079718847093 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15213419079718847093(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15213419079718847093(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15213419079718847093(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15213419079718847093);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9712958894972209269 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9712958894972209269(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9712958894972209269(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9712958894972209269(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9712958894972209269);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14817504653234290375 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14817504653234290375(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14817504653234290375(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14817504653234290375(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14817504653234290375);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1407121109007464125 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1280 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1407121109007464125(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1280} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1407121109007464125(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1407121109007464125(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1407121109007464125);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2477841778775647973 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2477841778775647973(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2477841778775647973(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2477841778775647973(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2477841778775647973);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5739129534809863051 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5739129534809863051(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5739129534809863051(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5739129534809863051(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5739129534809863051);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18247238328339571086 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18247238328339571086(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18247238328339571086(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18247238328339571086(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18247238328339571086);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15696073679481186122 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15696073679481186122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15696073679481186122(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15696073679481186122(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15696073679481186122);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7698101481027391450 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698101481027391450(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7698101481027391450(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7698101481027391450(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7698101481027391450);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1758658011504726048 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1758658011504726048(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1758658011504726048(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1758658011504726048(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1758658011504726048);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2694852446426499456 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2694852446426499456(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2694852446426499456(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2694852446426499456(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2694852446426499456);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3393460939419238708 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3393460939419238708(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3393460939419238708(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3393460939419238708(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3393460939419238708);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14130080129730425154 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14130080129730425154(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14130080129730425154(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14130080129730425154(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14130080129730425154);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5438113410877473033 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5438113410877473033(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5438113410877473033(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5438113410877473033(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5438113410877473033);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13593005292103594061 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13593005292103594061(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13593005292103594061(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13593005292103594061(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13593005292103594061);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6581269639386926798 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6581269639386926798(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6581269639386926798(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6581269639386926798(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6581269639386926798);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14329056573050909780 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14329056573050909780(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14329056573050909780(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14329056573050909780(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14329056573050909780);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8388720579659552812 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8388720579659552812(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8388720579659552812(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8388720579659552812(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8388720579659552812);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15604355477260952607 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15604355477260952607(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15604355477260952607(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15604355477260952607(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15604355477260952607);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13006136203067833474 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13006136203067833474(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13006136203067833474(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13006136203067833474(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13006136203067833474);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15752539215687093093 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15752539215687093093(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15752539215687093093(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15752539215687093093(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15752539215687093093);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3624314092221483460 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3624314092221483460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3624314092221483460(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3624314092221483460(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3624314092221483460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9369261394209006972 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9369261394209006972(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9369261394209006972(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9369261394209006972(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9369261394209006972);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14156502050944346559 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14156502050944346559(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14156502050944346559(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14156502050944346559(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14156502050944346559);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11775353048015346253 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11775353048015346253(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11775353048015346253(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11775353048015346253(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11775353048015346253);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5983532500793787787 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5983532500793787787(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5983532500793787787(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5983532500793787787(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5983532500793787787);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5657643296269513495 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5657643296269513495(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5657643296269513495(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5657643296269513495(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5657643296269513495);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11985790484529821265 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11985790484529821265(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11985790484529821265(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11985790484529821265(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11985790484529821265);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12247227092550658758 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12247227092550658758(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12247227092550658758(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12247227092550658758(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12247227092550658758);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3095358230067274998 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3095358230067274998(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3095358230067274998(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3095358230067274998(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3095358230067274998);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18332700975602468078 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18332700975602468078(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18332700975602468078(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18332700975602468078(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18332700975602468078);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3410045787149752877 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3410045787149752877(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3410045787149752877(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3410045787149752877(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3410045787149752877);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1717775992794033565 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1717775992794033565(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1717775992794033565(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1717775992794033565(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1717775992794033565);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8854681464015462551 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8854681464015462551(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8854681464015462551(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8854681464015462551(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8854681464015462551);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2362344932547800011 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2362344932547800011(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2362344932547800011(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2362344932547800011(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2362344932547800011);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16925077793630400142 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16925077793630400142(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16925077793630400142(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16925077793630400142(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16925077793630400142);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17306212751214653324 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17306212751214653324(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17306212751214653324(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17306212751214653324(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17306212751214653324);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3619624142082999028 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3619624142082999028(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3619624142082999028(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3619624142082999028(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3619624142082999028);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10880175206848620000 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10880175206848620000(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10880175206848620000(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10880175206848620000(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10880175206848620000);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16068001316946192427 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16068001316946192427(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16068001316946192427(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16068001316946192427(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16068001316946192427);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3039466886816114627 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3039466886816114627(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3039466886816114627(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3039466886816114627(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3039466886816114627);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5857277102026924187 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5857277102026924187(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5857277102026924187(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5857277102026924187(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5857277102026924187);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16543641711728677217 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16543641711728677217(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16543641711728677217(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16543641711728677217(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16543641711728677217);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5663006606082441685 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5663006606082441685(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5663006606082441685(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5663006606082441685(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5663006606082441685);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17260689652909868289 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17260689652909868289(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17260689652909868289(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17260689652909868289(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17260689652909868289);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5202571189173795279 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5202571189173795279(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5202571189173795279(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5202571189173795279(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5202571189173795279);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15561049931675502103 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15561049931675502103(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15561049931675502103(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15561049931675502103(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15561049931675502103);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16780666553144572728 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16780666553144572728(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16780666553144572728(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16780666553144572728(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16780666553144572728);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7394339589068913705 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7394339589068913705(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7394339589068913705(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7394339589068913705(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7394339589068913705);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7340107353062084513 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7340107353062084513(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7340107353062084513(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7340107353062084513(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7340107353062084513);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12901745575036151448 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12901745575036151448(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12901745575036151448(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12901745575036151448(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12901745575036151448);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__265597826931560139 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__265597826931560139(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__265597826931560139(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__265597826931560139(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__265597826931560139);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17300153826232343073 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17300153826232343073(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17300153826232343073(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17300153826232343073(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17300153826232343073);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3470522249485758886 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3470522249485758886(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3470522249485758886(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3470522249485758886(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3470522249485758886);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__18078856060153794924 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18078856060153794924(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__18078856060153794924(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18078856060153794924(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__18078856060153794924);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14647160206213614709 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14647160206213614709(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14647160206213614709(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14647160206213614709(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14647160206213614709);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1389721694794910676 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1389721694794910676(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1389721694794910676(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1389721694794910676(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1389721694794910676);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1712447387960955507 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1712447387960955507(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1712447387960955507(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1712447387960955507(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1712447387960955507);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16266851521550656640 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16266851521550656640(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16266851521550656640(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16266851521550656640(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16266851521550656640);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14187161978032923495 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14187161978032923495(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14187161978032923495(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14187161978032923495(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14187161978032923495);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4017368326533116138 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4017368326533116138(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4017368326533116138(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4017368326533116138(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4017368326533116138);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13566385938039807308 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13566385938039807308(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13566385938039807308(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13566385938039807308(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13566385938039807308);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10438812585374927871 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10438812585374927871(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10438812585374927871(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10438812585374927871(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10438812585374927871);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4937163360519418008 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4937163360519418008(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4937163360519418008(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4937163360519418008(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4937163360519418008);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5636779741109639990 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5636779741109639990(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5636779741109639990(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5636779741109639990(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5636779741109639990);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2888106991663910990 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2888106991663910990(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2888106991663910990(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2888106991663910990(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2888106991663910990);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15264129212855090242 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15264129212855090242(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15264129212855090242(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15264129212855090242(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15264129212855090242);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16367072278658993476 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16367072278658993476(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16367072278658993476(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16367072278658993476(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16367072278658993476);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1768272341513452968 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1768272341513452968(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1768272341513452968(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1768272341513452968(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1768272341513452968);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9021584340891143959 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9021584340891143959(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9021584340891143959(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9021584340891143959(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9021584340891143959);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13107724780889556907 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13107724780889556907(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13107724780889556907(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13107724780889556907(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13107724780889556907);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5254465180178486637 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5254465180178486637(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5254465180178486637(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5254465180178486637(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5254465180178486637);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14956778045042748725 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14956778045042748725(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14956778045042748725(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14956778045042748725(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14956778045042748725);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5999347807999791218 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5999347807999791218(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5999347807999791218(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5999347807999791218(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5999347807999791218);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14593425139493015580 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14593425139493015580(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14593425139493015580(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14593425139493015580(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14593425139493015580);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2455518023492033954 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2455518023492033954(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2455518023492033954(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2455518023492033954(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2455518023492033954);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11020335666438021409 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11020335666438021409(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11020335666438021409(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11020335666438021409(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11020335666438021409);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3597530246100037031 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3597530246100037031(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3597530246100037031(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3597530246100037031(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3597530246100037031);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12292125525853945791 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12292125525853945791(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12292125525853945791(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12292125525853945791(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12292125525853945791);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8468963182046775153 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468963182046775153(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8468963182046775153(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8468963182046775153(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8468963182046775153);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1701432239712076122 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1701432239712076122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1701432239712076122(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1701432239712076122(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1701432239712076122);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6856097015917667865 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6856097015917667865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6856097015917667865(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6856097015917667865(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6856097015917667865);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15005338629524897659 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15005338629524897659(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15005338629524897659(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15005338629524897659(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15005338629524897659);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7564538076039609667 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7564538076039609667(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7564538076039609667(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7564538076039609667(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7564538076039609667);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5777870862433893018 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5777870862433893018(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5777870862433893018(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5777870862433893018(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5777870862433893018);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6498704860571403632 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6498704860571403632(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6498704860571403632(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6498704860571403632(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6498704860571403632);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3562432250568538674 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3562432250568538674(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3562432250568538674(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3562432250568538674(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3562432250568538674);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14352868599886081542 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14352868599886081542(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14352868599886081542(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14352868599886081542(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14352868599886081542);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2878362319487247689 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2878362319487247689(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2878362319487247689(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2878362319487247689(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2878362319487247689);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2062155021605489756 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2062155021605489756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2062155021605489756(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2062155021605489756(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2062155021605489756);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4538199644975452947 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4538199644975452947(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4538199644975452947(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4538199644975452947(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4538199644975452947);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11177228847706766723 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11177228847706766723(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11177228847706766723(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11177228847706766723(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11177228847706766723);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18049861658057156346 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18049861658057156346(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18049861658057156346(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18049861658057156346(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18049861658057156346);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__484321209274727445 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__484321209274727445(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__484321209274727445(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__484321209274727445(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__484321209274727445);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13512361411475531616 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13512361411475531616(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13512361411475531616(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13512361411475531616(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13512361411475531616);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15586391594748964501 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15586391594748964501(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15586391594748964501(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15586391594748964501(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15586391594748964501);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12906288795987177933 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12906288795987177933(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12906288795987177933(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12906288795987177933(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12906288795987177933);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12277832535850877320 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12277832535850877320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12277832535850877320(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12277832535850877320(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12277832535850877320);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9874875777886554446 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9874875777886554446(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9874875777886554446(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9874875777886554446(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9874875777886554446);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13721201863789511956 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13721201863789511956(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13721201863789511956(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13721201863789511956(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13721201863789511956);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12192471640733255010 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12192471640733255010(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12192471640733255010(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12192471640733255010(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12192471640733255010);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11605370778745285992 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11605370778745285992(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11605370778745285992(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11605370778745285992(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11605370778745285992);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14582421298455097904 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14582421298455097904(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14582421298455097904(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14582421298455097904(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14582421298455097904);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11690952805678151183 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11690952805678151183(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11690952805678151183(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11690952805678151183(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11690952805678151183);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11659464357127235554 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11659464357127235554(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11659464357127235554(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11659464357127235554(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11659464357127235554);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__468758704844047418 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__468758704844047418(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__468758704844047418(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__468758704844047418(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__468758704844047418);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15811641917752709706 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15811641917752709706(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15811641917752709706(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15811641917752709706(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15811641917752709706);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16614838667765462822 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16614838667765462822(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16614838667765462822(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16614838667765462822(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16614838667765462822);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8227460406978277187 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8227460406978277187(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8227460406978277187(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8227460406978277187(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8227460406978277187);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13173686195911700168 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13173686195911700168(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13173686195911700168(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13173686195911700168(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13173686195911700168);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12308430144150489699 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12308430144150489699(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12308430144150489699(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12308430144150489699(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12308430144150489699);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12683244543876136164 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12683244543876136164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12683244543876136164(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12683244543876136164(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12683244543876136164);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6028497936975805881 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6028497936975805881(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6028497936975805881(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6028497936975805881(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6028497936975805881);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12355447869027517842 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12355447869027517842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12355447869027517842(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12355447869027517842(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12355447869027517842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9739497945141921555 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9739497945141921555(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9739497945141921555(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9739497945141921555(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9739497945141921555);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14132338299224254893 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14132338299224254893(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14132338299224254893(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14132338299224254893(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14132338299224254893);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15720103233163516336 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15720103233163516336(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15720103233163516336(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15720103233163516336(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15720103233163516336);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9989006336556609035 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9989006336556609035(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9989006336556609035(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9989006336556609035(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9989006336556609035);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5387731018473242900 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5387731018473242900(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5387731018473242900(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5387731018473242900(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5387731018473242900);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4445999441947431715 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4445999441947431715(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4445999441947431715(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4445999441947431715(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4445999441947431715);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2344508002315444417 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2344508002315444417(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2344508002315444417(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2344508002315444417(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2344508002315444417);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__14714030830247072983 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14714030830247072983(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14714030830247072983(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14714030830247072983(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14714030830247072983);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12587710324499039722 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12587710324499039722(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12587710324499039722(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12587710324499039722(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12587710324499039722);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8590131285928228435 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8590131285928228435(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8590131285928228435(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8590131285928228435(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8590131285928228435);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2248052488621597314 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2248052488621597314(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2248052488621597314(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2248052488621597314(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2248052488621597314);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__352309741546036625 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__352309741546036625(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__352309741546036625(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__352309741546036625(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__352309741546036625);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12255516261635178129 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12255516261635178129(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12255516261635178129(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12255516261635178129(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12255516261635178129);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1041797485604009390 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1041797485604009390(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1041797485604009390(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1041797485604009390(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1041797485604009390);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9220906157098086628 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9220906157098086628(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9220906157098086628(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9220906157098086628(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9220906157098086628);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14666085486518723622 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14666085486518723622(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14666085486518723622(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14666085486518723622(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14666085486518723622);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12859881419715594326 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12859881419715594326(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12859881419715594326(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12859881419715594326(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12859881419715594326);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12155639248595697924 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12155639248595697924(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12155639248595697924(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12155639248595697924(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12155639248595697924);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__421271345599415114 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__421271345599415114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__421271345599415114(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__421271345599415114(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__421271345599415114);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9981978680025688575 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9981978680025688575(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9981978680025688575(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9981978680025688575(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9981978680025688575);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17314625999666891443 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17314625999666891443(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17314625999666891443(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17314625999666891443(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17314625999666891443);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8572945667372611937 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8572945667372611937(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8572945667372611937(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8572945667372611937(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8572945667372611937);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12883413857046833115 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883413857046833115(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12883413857046833115(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12883413857046833115(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12883413857046833115);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18070046521165347254 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18070046521165347254(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18070046521165347254(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18070046521165347254(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18070046521165347254);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9052468865512513132 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9052468865512513132(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9052468865512513132(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9052468865512513132(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9052468865512513132);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13783576303064744689 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      224 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13783576303064744689(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13783576303064744689(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13783576303064744689(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13783576303064744689);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12953814291065492449 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12953814291065492449(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12953814291065492449(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12953814291065492449(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12953814291065492449);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10644689121663246979 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10644689121663246979(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10644689121663246979(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10644689121663246979(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10644689121663246979);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1540120641001243724 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1540120641001243724(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1540120641001243724(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1540120641001243724(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1540120641001243724);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13646845175056706128 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13646845175056706128(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13646845175056706128(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13646845175056706128(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13646845175056706128);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14497498592601568763 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14497498592601568763(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14497498592601568763(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14497498592601568763(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14497498592601568763);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13624862272645634715 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13624862272645634715(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13624862272645634715(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13624862272645634715(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13624862272645634715);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12575439724488283829 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12575439724488283829(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12575439724488283829(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12575439724488283829(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12575439724488283829);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12904923666458712247 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12904923666458712247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12904923666458712247(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12904923666458712247(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12904923666458712247);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13973346521483272909 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13973346521483272909(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13973346521483272909(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13973346521483272909(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13973346521483272909);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15479721158957081729 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15479721158957081729(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15479721158957081729(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15479721158957081729(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15479721158957081729);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16457740743437936706 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16457740743437936706(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16457740743437936706(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16457740743437936706(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16457740743437936706);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13337224261757840989 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13337224261757840989(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13337224261757840989(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13337224261757840989(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13337224261757840989);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__9152737467926222237 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__9152737467926222237(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__9152737467926222237(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__9152737467926222237(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__9152737467926222237)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11419982994286608420 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11419982994286608420(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11419982994286608420(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11419982994286608420(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11419982994286608420);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3237826465459851845 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3237826465459851845(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3237826465459851845(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3237826465459851845(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3237826465459851845);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__145952536273877536 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      109 /* Input2 */, \
      109 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__145952536273877536(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 109} /* Input2 */, 
      {"input[3]", 109} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__145952536273877536(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__145952536273877536(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__145952536273877536);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__18057019144618684055 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18057019144618684055(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__18057019144618684055(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18057019144618684055(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__18057019144618684055);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5793296061504566566 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5793296061504566566(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5793296061504566566(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5793296061504566566(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5793296061504566566);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12662223900669356931 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12662223900669356931(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12662223900669356931(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12662223900669356931(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12662223900669356931);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10852410430584118458 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10852410430584118458(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10852410430584118458(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10852410430584118458(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10852410430584118458);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5504844140271833476 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5504844140271833476(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5504844140271833476(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5504844140271833476(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5504844140271833476);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10637553235709727371 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10637553235709727371(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10637553235709727371(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10637553235709727371(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10637553235709727371);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__605544342672392439 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__605544342672392439(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__605544342672392439(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__605544342672392439(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__605544342672392439);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7883939133643994462 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7883939133643994462(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7883939133643994462(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7883939133643994462(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7883939133643994462);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3722062269358341664 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3722062269358341664(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3722062269358341664(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3722062269358341664(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3722062269358341664);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3077950327928298207 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3077950327928298207(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3077950327928298207(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3077950327928298207(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3077950327928298207);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14626161434898589503 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14626161434898589503(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14626161434898589503(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14626161434898589503(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14626161434898589503);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3672115844504857843 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      960 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3672115844504857843(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3672115844504857843(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3672115844504857843(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3672115844504857843);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15139851827475097337 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15139851827475097337(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15139851827475097337(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15139851827475097337(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15139851827475097337);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12964698586166419691 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12964698586166419691(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12964698586166419691(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12964698586166419691(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12964698586166419691);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4662757378868634267 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4662757378868634267(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4662757378868634267(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4662757378868634267(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4662757378868634267);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4635208150115850776 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4635208150115850776(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4635208150115850776(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4635208150115850776(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4635208150115850776);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11177944975835427646 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11177944975835427646(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11177944975835427646(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11177944975835427646(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11177944975835427646);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12692034943433628300 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12692034943433628300(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12692034943433628300(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12692034943433628300(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12692034943433628300);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3559906980111864447 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      2 /* DilationWidth */, \
      2 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3559906980111864447(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 2} /* DilationWidth */, 
      {"dilation_width", 2} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3559906980111864447(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3559906980111864447(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3559906980111864447);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14901698163958434151 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14901698163958434151(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14901698163958434151(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14901698163958434151(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14901698163958434151);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11350496253718263216 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11350496253718263216(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11350496253718263216(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11350496253718263216(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11350496253718263216);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__7255578618908107626 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7255578618908107626(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__7255578618908107626(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7255578618908107626(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__7255578618908107626);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2296425703226589140 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2296425703226589140(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2296425703226589140(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2296425703226589140(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2296425703226589140);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11420081970127748446 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11420081970127748446(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11420081970127748446(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11420081970127748446(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11420081970127748446);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17644814810890620068 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17644814810890620068(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17644814810890620068(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17644814810890620068(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17644814810890620068);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9630588977036129040 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9630588977036129040(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9630588977036129040(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9630588977036129040(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9630588977036129040);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14712803868279559903 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14712803868279559903(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14712803868279559903(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14712803868279559903(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14712803868279559903);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16263460640666018524 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16263460640666018524(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16263460640666018524(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16263460640666018524(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16263460640666018524);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__471493509890304726 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__471493509890304726(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__471493509890304726(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__471493509890304726(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__471493509890304726);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3131562355085278784 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3131562355085278784(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3131562355085278784(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3131562355085278784(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3131562355085278784);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16996315080380988320 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16996315080380988320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16996315080380988320(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16996315080380988320(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16996315080380988320);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4690866398250308266 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4690866398250308266(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4690866398250308266(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4690866398250308266(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4690866398250308266);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10930834645903345203 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10930834645903345203(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10930834645903345203(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10930834645903345203(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10930834645903345203);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11340125873759572335 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11340125873759572335(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11340125873759572335(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11340125873759572335(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11340125873759572335);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6888722799860095661 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6888722799860095661(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6888722799860095661(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6888722799860095661(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6888722799860095661);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__10312507166730866225 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      111 /* Input2 */, \
      111 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10312507166730866225(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 111} /* Input2 */, 
      {"input[3]", 111} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10312507166730866225(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10312507166730866225(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10312507166730866225);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10629786821451841438 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10629786821451841438(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10629786821451841438(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10629786821451841438(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10629786821451841438);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4513027207653764788 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4513027207653764788(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4513027207653764788(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4513027207653764788(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4513027207653764788);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8235148288015134538 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8235148288015134538(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8235148288015134538(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8235148288015134538(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8235148288015134538);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17883605159644935868 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17883605159644935868(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17883605159644935868(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17883605159644935868(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17883605159644935868);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1904019930813154582 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1904019930813154582(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1904019930813154582(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1904019930813154582(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1904019930813154582);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4348131653364531545 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4348131653364531545(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4348131653364531545(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4348131653364531545(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4348131653364531545);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8082544515099643559 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      17 /* PadHeight */, \
      17 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      17 /* DilationWidth */, \
      17 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8082544515099643559(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 17} /* PadHeight */, 
      {"pad_width", 17} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 17} /* DilationWidth */, 
      {"dilation_width", 17} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8082544515099643559(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8082544515099643559(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8082544515099643559);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7381923691960197503 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7381923691960197503(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7381923691960197503(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7381923691960197503(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7381923691960197503);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__17543224366576972021 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17543224366576972021(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__17543224366576972021(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17543224366576972021(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__17543224366576972021)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__636787277138761458 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__636787277138761458(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__636787277138761458(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__636787277138761458(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__636787277138761458);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17019780716562527827 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17019780716562527827(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17019780716562527827(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17019780716562527827(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17019780716562527827);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8489216220983607186 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8489216220983607186(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8489216220983607186(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8489216220983607186(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8489216220983607186);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2973577506845824042 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2973577506845824042(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2973577506845824042(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2973577506845824042(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2973577506845824042);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13451190950509698615 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13451190950509698615(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13451190950509698615(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13451190950509698615(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13451190950509698615);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4306530061329697571 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4306530061329697571(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4306530061329697571(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4306530061329697571(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4306530061329697571);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1680085376530812317 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1680085376530812317(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1680085376530812317(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1680085376530812317(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1680085376530812317);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4576167691013108088 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4576167691013108088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4576167691013108088(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4576167691013108088(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4576167691013108088);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9506041745575146396 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9506041745575146396(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9506041745575146396(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9506041745575146396(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9506041745575146396);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6346336046427942098 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6346336046427942098(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6346336046427942098(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6346336046427942098(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6346336046427942098);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2497271958792501138 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2497271958792501138(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2497271958792501138(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2497271958792501138(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2497271958792501138);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17706816294252671944 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17706816294252671944(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17706816294252671944(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17706816294252671944(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17706816294252671944);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5385491355974977481 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5385491355974977481(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5385491355974977481(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5385491355974977481(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5385491355974977481);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11851824244154605055 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11851824244154605055(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11851824244154605055(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11851824244154605055(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11851824244154605055);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17443918135012900939 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      544 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17443918135012900939(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17443918135012900939(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17443918135012900939(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17443918135012900939);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13042571298903103087 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13042571298903103087(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13042571298903103087(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13042571298903103087(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13042571298903103087);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2848819449317172435 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2848819449317172435(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2848819449317172435(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2848819449317172435(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2848819449317172435);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2853155095828367827 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2853155095828367827(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2853155095828367827(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2853155095828367827(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2853155095828367827);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16802225839926580599 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16802225839926580599(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16802225839926580599(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16802225839926580599(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16802225839926580599);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4697585330990453197 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4697585330990453197(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4697585330990453197(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4697585330990453197(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4697585330990453197);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__459464387862595832 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__459464387862595832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__459464387862595832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__459464387862595832(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__459464387862595832);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13589597532530125485 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13589597532530125485(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13589597532530125485(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13589597532530125485(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13589597532530125485);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11929818551497494222 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11929818551497494222(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11929818551497494222(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11929818551497494222(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11929818551497494222);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8791159865239153150 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8791159865239153150(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8791159865239153150(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8791159865239153150(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8791159865239153150);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15764236704994813532 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15764236704994813532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15764236704994813532(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15764236704994813532(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15764236704994813532);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7243052991009224659 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7243052991009224659(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7243052991009224659(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7243052991009224659(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7243052991009224659);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__13536075120537619722 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13536075120537619722(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13536075120537619722(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13536075120537619722(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13536075120537619722);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15645762169125942777 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15645762169125942777(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15645762169125942777(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15645762169125942777(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15645762169125942777);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3589491255215340563 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3589491255215340563(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3589491255215340563(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3589491255215340563(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3589491255215340563);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16603920213617485142 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16603920213617485142(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16603920213617485142(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16603920213617485142(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16603920213617485142);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13089961573138772840 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13089961573138772840(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13089961573138772840(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13089961573138772840(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13089961573138772840);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17147496862919105327 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17147496862919105327(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17147496862919105327(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17147496862919105327(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17147496862919105327);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1796169643964208184 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1796169643964208184(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1796169643964208184(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1796169643964208184(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1796169643964208184);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3386876782425718193 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3386876782425718193(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3386876782425718193(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3386876782425718193(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3386876782425718193);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7706114194378138319 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7706114194378138319(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7706114194378138319(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7706114194378138319(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7706114194378138319);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9183102400363520423 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9183102400363520423(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9183102400363520423(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9183102400363520423(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9183102400363520423);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2220749287438270885 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220749287438270885(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2220749287438270885(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2220749287438270885(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2220749287438270885);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10348282210262367879 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10348282210262367879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10348282210262367879(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10348282210262367879(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10348282210262367879);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6248034241540504323 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6248034241540504323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6248034241540504323(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6248034241540504323(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6248034241540504323);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6991840624476787281 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6991840624476787281(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6991840624476787281(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6991840624476787281(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6991840624476787281);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18407130340463513087 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18407130340463513087(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18407130340463513087(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18407130340463513087(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18407130340463513087);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__904293903721364591 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__904293903721364591(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__904293903721364591(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__904293903721364591(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__904293903721364591);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17689791679100998755 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17689791679100998755(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17689791679100998755(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17689791679100998755(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17689791679100998755);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7719404973642994083 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7719404973642994083(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7719404973642994083(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7719404973642994083(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7719404973642994083);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12674182118014295073 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12674182118014295073(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12674182118014295073(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12674182118014295073(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12674182118014295073);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16425521264848044421 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16425521264848044421(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16425521264848044421(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16425521264848044421(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16425521264848044421);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17615983169400888235 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17615983169400888235(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17615983169400888235(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17615983169400888235(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17615983169400888235);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7287091020523307019 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7287091020523307019(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7287091020523307019(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7287091020523307019(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7287091020523307019);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11577528103959316562 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11577528103959316562(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11577528103959316562(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11577528103959316562(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11577528103959316562);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9222198023028424680 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9222198023028424680(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9222198023028424680(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9222198023028424680(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9222198023028424680);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4939483056362541124 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4939483056362541124(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4939483056362541124(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4939483056362541124(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4939483056362541124);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2023580065536996669 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2023580065536996669(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2023580065536996669(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2023580065536996669(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2023580065536996669);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13737940030429517468 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13737940030429517468(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13737940030429517468(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13737940030429517468(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13737940030429517468);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16191244781021064811 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16191244781021064811(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16191244781021064811(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16191244781021064811(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16191244781021064811);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17377415612203884941 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17377415612203884941(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17377415612203884941(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17377415612203884941(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17377415612203884941);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3780793598202513711 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3780793598202513711(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3780793598202513711(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3780793598202513711(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3780793598202513711);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9577312948567636820 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9577312948567636820(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9577312948567636820(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9577312948567636820(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9577312948567636820);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14687371925992442343 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14687371925992442343(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14687371925992442343(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14687371925992442343(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14687371925992442343);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17305282216412000083 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17305282216412000083(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17305282216412000083(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17305282216412000083(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17305282216412000083);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16010727832887419668 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16010727832887419668(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16010727832887419668(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16010727832887419668(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16010727832887419668);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6686716559063595659 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6686716559063595659(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6686716559063595659(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6686716559063595659(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6686716559063595659);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8244527653369781623 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8244527653369781623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8244527653369781623(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8244527653369781623(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8244527653369781623);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9457152556210480391 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9457152556210480391(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9457152556210480391(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9457152556210480391(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9457152556210480391);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3971148956832476653 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3971148956832476653(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3971148956832476653(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3971148956832476653(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3971148956832476653);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7232781853966123370 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7232781853966123370(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7232781853966123370(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7232781853966123370(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7232781853966123370);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3387671378151395917 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3387671378151395917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3387671378151395917(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3387671378151395917(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3387671378151395917);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13591931143566910615 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13591931143566910615(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13591931143566910615(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13591931143566910615(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13591931143566910615);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14880653165603991275 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14880653165603991275(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14880653165603991275(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14880653165603991275(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14880653165603991275);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8141155162710783698 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8141155162710783698(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8141155162710783698(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8141155162710783698(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8141155162710783698);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7934064227181089553 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7934064227181089553(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7934064227181089553(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7934064227181089553(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7934064227181089553);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4556043604922093103 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4556043604922093103(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4556043604922093103(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4556043604922093103(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4556043604922093103);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__381087267646375698 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__381087267646375698(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__381087267646375698(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__381087267646375698(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__381087267646375698);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7989315597836305063 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7989315597836305063(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7989315597836305063(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7989315597836305063(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7989315597836305063);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__602257310161160178 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__602257310161160178(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__602257310161160178(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__602257310161160178(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__602257310161160178);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4132359637430301005 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4132359637430301005(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4132359637430301005(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4132359637430301005(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4132359637430301005);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5813295984587453985 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5813295984587453985(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5813295984587453985(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5813295984587453985(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5813295984587453985);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8248481714390764980 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      96 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8248481714390764980(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8248481714390764980(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8248481714390764980(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8248481714390764980);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1325085525789708894 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1325085525789708894(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1325085525789708894(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1325085525789708894(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1325085525789708894);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6173416582744445368 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6173416582744445368(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6173416582744445368(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6173416582744445368(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6173416582744445368);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15654853141023081641 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15654853141023081641(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15654853141023081641(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15654853141023081641(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15654853141023081641);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16171132687512720293 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16171132687512720293(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16171132687512720293(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16171132687512720293(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16171132687512720293);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__2069152170234272711 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__2069152170234272711(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__2069152170234272711(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__2069152170234272711(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__2069152170234272711);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13671913707074293945 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13671913707074293945(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13671913707074293945(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13671913707074293945(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13671913707074293945);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15056641318874899927 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15056641318874899927(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15056641318874899927(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15056641318874899927(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15056641318874899927);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17502971137226720898 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17502971137226720898(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17502971137226720898(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17502971137226720898(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17502971137226720898);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15245572799006640542 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15245572799006640542(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15245572799006640542(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15245572799006640542(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15245572799006640542);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6551901671649110634 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      11 /* FilterHeight */, \
      11 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      4 /* StrideHeight */, \
      4 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6551901671649110634(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 11} /* FilterHeight */, 
      {"filter_width", 11} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 4} /* StrideHeight */, 
      {"stride_width", 4} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6551901671649110634(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6551901671649110634(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6551901671649110634);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__5381795484250700655 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5381795484250700655(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__5381795484250700655(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5381795484250700655(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__5381795484250700655);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3986517366254032839 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3986517366254032839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3986517366254032839(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3986517366254032839(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3986517366254032839);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5149838787838462532 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5149838787838462532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5149838787838462532(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5149838787838462532(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5149838787838462532);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13267185967054114658 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13267185967054114658(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13267185967054114658(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13267185967054114658(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13267185967054114658);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__14340328536803873980 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14340328536803873980(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14340328536803873980(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14340328536803873980(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14340328536803873980);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14226213591011230637 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14226213591011230637(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14226213591011230637(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14226213591011230637(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14226213591011230637);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__18407445671024594836 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18407445671024594836(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18407445671024594836(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18407445671024594836(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18407445671024594836);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2462413543087211797 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2462413543087211797(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2462413543087211797(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2462413543087211797(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2462413543087211797);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5352193882635741427 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5352193882635741427(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5352193882635741427(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5352193882635741427(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5352193882635741427);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15185572569827389942 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15185572569827389942(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15185572569827389942(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15185572569827389942(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15185572569827389942);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__111694601051109384 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      109 /* Input2 */, \
      109 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__111694601051109384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 109} /* Input2 */, 
      {"input[3]", 109} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__111694601051109384(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__111694601051109384(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__111694601051109384);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__8329506171894779533 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__8329506171894779533(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__8329506171894779533(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__8329506171894779533(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__8329506171894779533)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2837400038193796855 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2837400038193796855(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2837400038193796855(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2837400038193796855(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2837400038193796855);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4646270151761565249 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4646270151761565249(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4646270151761565249(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4646270151761565249(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4646270151761565249);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18415055035863758026 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18415055035863758026(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18415055035863758026(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18415055035863758026(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18415055035863758026);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2492211536809953271 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2492211536809953271(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2492211536809953271(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2492211536809953271(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2492211536809953271);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7980302411728810973 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7980302411728810973(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7980302411728810973(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7980302411728810973(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7980302411728810973);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17930685256344347291 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17930685256344347291(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17930685256344347291(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17930685256344347291(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17930685256344347291);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13030401819861922347 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13030401819861922347(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13030401819861922347(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13030401819861922347(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13030401819861922347);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12065982938116530409 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12065982938116530409(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12065982938116530409(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12065982938116530409(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12065982938116530409);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14577948107415594341 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14577948107415594341(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14577948107415594341(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14577948107415594341(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14577948107415594341);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16057789509882746007 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16057789509882746007(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16057789509882746007(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16057789509882746007(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16057789509882746007);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11809544919840419019 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11809544919840419019(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11809544919840419019(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11809544919840419019(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11809544919840419019);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__14721686372792215035 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14721686372792215035(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14721686372792215035(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14721686372792215035(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14721686372792215035);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1165315047780237803 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1165315047780237803(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1165315047780237803(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1165315047780237803(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1165315047780237803);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9873982959341716949 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      320 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9873982959341716949(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9873982959341716949(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9873982959341716949(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9873982959341716949);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1534289481647033338 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1534289481647033338(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1534289481647033338(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1534289481647033338(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1534289481647033338);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5146759419159561313 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5146759419159561313(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5146759419159561313(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5146759419159561313(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5146759419159561313);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16815452807015142872 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16815452807015142872(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16815452807015142872(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16815452807015142872(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16815452807015142872);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6190621761482074129 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6190621761482074129(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6190621761482074129(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6190621761482074129(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6190621761482074129);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8868141470313720913 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8868141470313720913(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8868141470313720913(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8868141470313720913(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8868141470313720913);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__591959943986211331 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__591959943986211331(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__591959943986211331(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__591959943986211331(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__591959943986211331);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7302095272644851392 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7302095272644851392(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7302095272644851392(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7302095272644851392(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7302095272644851392);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__14233255520025848385 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__14233255520025848385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__14233255520025848385(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__14233255520025848385(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__14233255520025848385)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8645661258652471890 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8645661258652471890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8645661258652471890(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8645661258652471890(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8645661258652471890);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15647394451997817769 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15647394451997817769(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15647394451997817769(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15647394451997817769(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15647394451997817769);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_SOFTMAX_FWD
namespace LAYER_CUDNN_SOFTMAX_FWD__14187650155642638240 {

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__14187650155642638240(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__14187650155642638240(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__14187650155642638240(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_INSTANCE)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_CHANNEL)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_FAST);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_ACCURATE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_LOG)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__14187650155642638240);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
}
#endif // ENABLE_LAYER_CUDNN_SOFTMAX_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3724975058425410825 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3724975058425410825(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3724975058425410825(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3724975058425410825(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3724975058425410825);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15186403383768561979 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15186403383768561979(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15186403383768561979(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15186403383768561979(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15186403383768561979);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__14435748861645387208 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14435748861645387208(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14435748861645387208(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14435748861645387208(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14435748861645387208);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2361998790346769200 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2361998790346769200(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2361998790346769200(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2361998790346769200(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2361998790346769200);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9752890234467883035 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9752890234467883035(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9752890234467883035(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9752890234467883035(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9752890234467883035);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__52223787688482943 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__52223787688482943(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__52223787688482943(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__52223787688482943(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__52223787688482943);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16189597779051153015 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16189597779051153015(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16189597779051153015(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16189597779051153015(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16189597779051153015);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2184469741284080635 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2184469741284080635(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2184469741284080635(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2184469741284080635(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2184469741284080635);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16237051568521462078 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16237051568521462078(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16237051568521462078(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16237051568521462078(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16237051568521462078);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15239378468976466465 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15239378468976466465(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15239378468976466465(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15239378468976466465(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15239378468976466465);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13519217862995630811 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13519217862995630811(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13519217862995630811(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13519217862995630811(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13519217862995630811);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17677213651847915360 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17677213651847915360(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17677213651847915360(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17677213651847915360(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17677213651847915360);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13872855199312296457 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13872855199312296457(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13872855199312296457(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13872855199312296457(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13872855199312296457);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4022562739369418415 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4022562739369418415(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4022562739369418415(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4022562739369418415(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4022562739369418415);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8996082633817787631 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8996082633817787631(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8996082633817787631(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8996082633817787631(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8996082633817787631);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3165886976029780943 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3165886976029780943(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3165886976029780943(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3165886976029780943(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3165886976029780943);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1110041260798771571 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      8 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1110041260798771571(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1110041260798771571(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1110041260798771571(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1110041260798771571);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3685282082610857421 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3685282082610857421(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3685282082610857421(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3685282082610857421(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3685282082610857421);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6450547532147671245 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6450547532147671245(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6450547532147671245(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6450547532147671245(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6450547532147671245);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6032457654269847307 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6032457654269847307(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6032457654269847307(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6032457654269847307(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6032457654269847307);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16047866410811852489 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16047866410811852489(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16047866410811852489(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16047866410811852489(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16047866410811852489);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12694762118511195970 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12694762118511195970(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12694762118511195970(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12694762118511195970(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12694762118511195970);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2945687916678152509 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2945687916678152509(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2945687916678152509(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2945687916678152509(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2945687916678152509);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11031486037564235755 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031486037564235755(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11031486037564235755(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11031486037564235755(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11031486037564235755);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8238470066576893709 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8238470066576893709(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8238470066576893709(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8238470066576893709(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8238470066576893709);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7277309953864058304 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7277309953864058304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7277309953864058304(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7277309953864058304(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7277309953864058304);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10580315402764265879 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10580315402764265879(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10580315402764265879(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10580315402764265879(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10580315402764265879);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5039464067789074783 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5039464067789074783(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5039464067789074783(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5039464067789074783(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5039464067789074783);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4972120396707882623 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4972120396707882623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4972120396707882623(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4972120396707882623(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4972120396707882623);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8429983628083150996 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8429983628083150996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8429983628083150996(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8429983628083150996(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8429983628083150996);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17150130891256436802 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17150130891256436802(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17150130891256436802(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17150130891256436802(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17150130891256436802);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3326670619946439078 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3326670619946439078(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3326670619946439078(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3326670619946439078(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3326670619946439078);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18371209813557010672 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      12 /* PadHeight */, \
      12 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      12 /* DilationWidth */, \
      12 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18371209813557010672(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 12} /* PadHeight */, 
      {"pad_width", 12} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 12} /* DilationWidth */, 
      {"dilation_width", 12} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18371209813557010672(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18371209813557010672(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18371209813557010672);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8718628461762171833 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8718628461762171833(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8718628461762171833(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8718628461762171833(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8718628461762171833);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12227444713802376746 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12227444713802376746(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12227444713802376746(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12227444713802376746(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12227444713802376746);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11623715628434164505 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11623715628434164505(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11623715628434164505(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11623715628434164505(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11623715628434164505);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14735979663004713248 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14735979663004713248(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14735979663004713248(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14735979663004713248(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14735979663004713248);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1193101703640888246 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1193101703640888246(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1193101703640888246(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1193101703640888246(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1193101703640888246);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__16227687146493963642 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16227687146493963642(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__16227687146493963642(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16227687146493963642(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__16227687146493963642);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9612427867185606561 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9612427867185606561(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9612427867185606561(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9612427867185606561(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9612427867185606561);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2574699966635929699 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2574699966635929699(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2574699966635929699(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2574699966635929699(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2574699966635929699);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6057859539534473586 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6057859539534473586(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6057859539534473586(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6057859539534473586(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6057859539534473586);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14310593639485676154 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14310593639485676154(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14310593639485676154(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14310593639485676154(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14310593639485676154);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__573977784486670803 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__573977784486670803(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__573977784486670803(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__573977784486670803(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__573977784486670803);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8643787200754411929 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8643787200754411929(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8643787200754411929(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8643787200754411929(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8643787200754411929);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6422664375192109975 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6422664375192109975(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6422664375192109975(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6422664375192109975(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6422664375192109975);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__1982744944911961421 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      3 /* StrideHeight */, \
      3 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1982744944911961421(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 3} /* StrideHeight */, 
      {"stride_width", 3} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__1982744944911961421(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1982744944911961421(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__1982744944911961421);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__13759470554931620562 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13759470554931620562(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13759470554931620562(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13759470554931620562(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13759470554931620562);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7918259395807610013 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7918259395807610013(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7918259395807610013(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7918259395807610013(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7918259395807610013);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15548568184157050184 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15548568184157050184(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15548568184157050184(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15548568184157050184(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15548568184157050184);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7083262967282093511 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7083262967282093511(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7083262967282093511(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7083262967282093511(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7083262967282093511);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16428275725314983920 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16428275725314983920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16428275725314983920(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16428275725314983920(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16428275725314983920);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13644590394779497151 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13644590394779497151(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13644590394779497151(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13644590394779497151(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13644590394779497151);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13701959827459882203 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      16 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13701959827459882203(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13701959827459882203(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13701959827459882203(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13701959827459882203);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7828870751363327440 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7828870751363327440(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7828870751363327440(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7828870751363327440(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7828870751363327440);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16034782842765852607 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16034782842765852607(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16034782842765852607(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16034782842765852607(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16034782842765852607);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__15521290055259577842 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15521290055259577842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15521290055259577842(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15521290055259577842(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15521290055259577842);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5296317942596270867 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5296317942596270867(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5296317942596270867(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5296317942596270867(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5296317942596270867);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14540919944857449591 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14540919944857449591(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14540919944857449591(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14540919944857449591(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14540919944857449591);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9379469959200313648 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9379469959200313648(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9379469959200313648(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9379469959200313648(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9379469959200313648);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11219825188059207384 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11219825188059207384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11219825188059207384(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11219825188059207384(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11219825188059207384);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__23641356039338941 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__23641356039338941(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__23641356039338941(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__23641356039338941(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__23641356039338941);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15212534539808942385 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15212534539808942385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15212534539808942385(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15212534539808942385(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15212534539808942385);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16091559348160683751 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      5 /* PadHeight */, \
      5 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      5 /* DilationWidth */, \
      5 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16091559348160683751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 5} /* PadHeight */, 
      {"pad_width", 5} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 5} /* DilationWidth */, 
      {"dilation_width", 5} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16091559348160683751(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16091559348160683751(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16091559348160683751);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1542258147635646020 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      111 /* Input2 */, \
      111 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1542258147635646020(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 111} /* Input2 */, 
      {"input[3]", 111} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1542258147635646020(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1542258147635646020(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1542258147635646020);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2899153152071520830 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2899153152071520830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2899153152071520830(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2899153152071520830(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2899153152071520830);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13502593805294398204 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13502593805294398204(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13502593805294398204(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13502593805294398204(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13502593805294398204);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__7568873665197585074 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7568873665197585074(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__7568873665197585074(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__7568873665197585074(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__7568873665197585074);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12474211842482464741 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12474211842482464741(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12474211842482464741(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12474211842482464741(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12474211842482464741);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11036517315591126228 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11036517315591126228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11036517315591126228(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11036517315591126228(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11036517315591126228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15058351983299159370 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15058351983299159370(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15058351983299159370(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15058351983299159370(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15058351983299159370);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13818220732576916443 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13818220732576916443(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13818220732576916443(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13818220732576916443(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13818220732576916443);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6215001038199591344 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6215001038199591344(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6215001038199591344(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6215001038199591344(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6215001038199591344);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5384736038218191023 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5384736038218191023(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5384736038218191023(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5384736038218191023(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5384736038218191023);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13669043597022133292 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13669043597022133292(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13669043597022133292(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13669043597022133292(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13669043597022133292);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12615732586145934046 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12615732586145934046(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12615732586145934046(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12615732586145934046(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12615732586145934046);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14747375001325423809 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14747375001325423809(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14747375001325423809(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14747375001325423809(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14747375001325423809);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2523969922405812350 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2523969922405812350(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2523969922405812350(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2523969922405812350(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2523969922405812350);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7244607918959169583 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7244607918959169583(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7244607918959169583(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7244607918959169583(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7244607918959169583);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10148436721567184693 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10148436721567184693(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10148436721567184693(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10148436721567184693(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10148436721567184693);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3792930394359139334 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3792930394359139334(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3792930394359139334(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3792930394359139334(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3792930394359139334);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4717817694501758675 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4717817694501758675(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4717817694501758675(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4717817694501758675(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4717817694501758675);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14068287207444834384 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14068287207444834384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14068287207444834384(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14068287207444834384(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14068287207444834384);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12974068049565033934 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12974068049565033934(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12974068049565033934(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12974068049565033934(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12974068049565033934);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17888875603203001082 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17888875603203001082(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17888875603203001082(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17888875603203001082(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17888875603203001082);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3686724583192622760 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3686724583192622760(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3686724583192622760(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3686724583192622760(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3686724583192622760);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12591060355056683750 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12591060355056683750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12591060355056683750(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12591060355056683750(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12591060355056683750);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__914850259376748153 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__914850259376748153(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__914850259376748153(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__914850259376748153(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__914850259376748153);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13052229087020040350 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13052229087020040350(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13052229087020040350(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13052229087020040350(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13052229087020040350);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10718131449478315853 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10718131449478315853(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10718131449478315853(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10718131449478315853(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10718131449478315853);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12720639289297483658 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12720639289297483658(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12720639289297483658(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12720639289297483658(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12720639289297483658);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14530315328420626116 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14530315328420626116(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14530315328420626116(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14530315328420626116(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14530315328420626116);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5313049615948649075 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5313049615948649075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5313049615948649075(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5313049615948649075(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5313049615948649075);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5219582616064157014 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5219582616064157014(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5219582616064157014(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5219582616064157014(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5219582616064157014);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16820969227090969801 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16820969227090969801(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16820969227090969801(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16820969227090969801(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16820969227090969801);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5021832865957522574 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5021832865957522574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5021832865957522574(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5021832865957522574(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5021832865957522574);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14280181955398462627 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14280181955398462627(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14280181955398462627(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14280181955398462627(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14280181955398462627);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8967393568859315564 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8967393568859315564(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8967393568859315564(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8967393568859315564(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8967393568859315564);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2408227733629786754 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2408227733629786754(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2408227733629786754(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2408227733629786754(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2408227733629786754);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16647805630597358785 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16647805630597358785(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16647805630597358785(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16647805630597358785(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16647805630597358785);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2222136022595082299 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2222136022595082299(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2222136022595082299(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2222136022595082299(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2222136022595082299);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13679639673023638410 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13679639673023638410(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13679639673023638410(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13679639673023638410(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13679639673023638410);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__9708505419366385636 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__9708505419366385636(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__9708505419366385636(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__9708505419366385636(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__9708505419366385636)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6488785360293754905 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      960 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6488785360293754905(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6488785360293754905(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6488785360293754905(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6488785360293754905);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1555929621392901101 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1555929621392901101(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1555929621392901101(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1555929621392901101(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1555929621392901101);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14746743156888511848 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      6 /* PadHeight */, \
      6 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      6 /* DilationWidth */, \
      6 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14746743156888511848(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 6} /* PadHeight */, 
      {"pad_width", 6} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 6} /* DilationWidth */, 
      {"dilation_width", 6} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14746743156888511848(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14746743156888511848(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14746743156888511848);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12906086185523084506 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12906086185523084506(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12906086185523084506(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12906086185523084506(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12906086185523084506);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__6834273180964491560 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6834273180964491560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6834273180964491560(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6834273180964491560(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6834273180964491560);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3940294008889642186 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3940294008889642186(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3940294008889642186(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3940294008889642186(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3940294008889642186);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2458891372967387125 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2458891372967387125(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2458891372967387125(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2458891372967387125(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2458891372967387125);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3438188063164113087 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3438188063164113087(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3438188063164113087(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3438188063164113087(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3438188063164113087);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3756321889670451996 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3756321889670451996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3756321889670451996(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3756321889670451996(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3756321889670451996);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12350546090614888206 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12350546090614888206(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12350546090614888206(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12350546090614888206(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12350546090614888206);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__8215067756076455818 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8215067756076455818(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__8215067756076455818(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8215067756076455818(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__8215067756076455818);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__6431601138506872814 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6431601138506872814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6431601138506872814(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6431601138506872814(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6431601138506872814);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14247581130032964168 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14247581130032964168(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14247581130032964168(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14247581130032964168(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14247581130032964168);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__10202782189584565967 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10202782189584565967(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10202782189584565967(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10202782189584565967(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10202782189584565967);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16520077813815738295 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16520077813815738295(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16520077813815738295(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16520077813815738295(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16520077813815738295);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__18380283381454870928 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18380283381454870928(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__18380283381454870928(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18380283381454870928(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__18380283381454870928);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6972859017849786859 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6972859017849786859(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6972859017849786859(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6972859017849786859(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6972859017849786859);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14515006269717951679 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14515006269717951679(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14515006269717951679(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14515006269717951679(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14515006269717951679);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1789334999582145847 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1789334999582145847(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1789334999582145847(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1789334999582145847(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1789334999582145847);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6165556501598549919 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6165556501598549919(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6165556501598549919(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6165556501598549919(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6165556501598549919);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__11557250819629118980 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11557250819629118980(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11557250819629118980(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11557250819629118980(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11557250819629118980);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3406049415245072553 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3406049415245072553(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3406049415245072553(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3406049415245072553(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3406049415245072553);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_SOFTMAX_FWD
namespace LAYER_CUDNN_SOFTMAX_FWD__8500292617054095521 {

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      8 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__8500292617054095521(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__8500292617054095521(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__8500292617054095521(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_INSTANCE)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_CHANNEL)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_FAST);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_ACCURATE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_LOG)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__8500292617054095521);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
}
#endif // ENABLE_LAYER_CUDNN_SOFTMAX_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3084351876862416505 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3084351876862416505(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3084351876862416505(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3084351876862416505(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3084351876862416505);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__908667132371803787 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__908667132371803787(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__908667132371803787(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__908667132371803787(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__908667132371803787);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3712764342670279401 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3712764342670279401(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3712764342670279401(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3712764342670279401(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3712764342670279401);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10657432943760462718 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10657432943760462718(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10657432943760462718(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10657432943760462718(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10657432943760462718);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9968820715202748015 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9968820715202748015(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9968820715202748015(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9968820715202748015(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9968820715202748015);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4546705688369710733 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4546705688369710733(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4546705688369710733(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4546705688369710733(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4546705688369710733);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17088397258645074215 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17088397258645074215(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17088397258645074215(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17088397258645074215(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17088397258645074215);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1114133697519785498 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1114133697519785498(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1114133697519785498(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1114133697519785498(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1114133697519785498);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6669063400062430227 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      800 /* Input2 */, \
      800 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6669063400062430227(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 800} /* Input2 */, 
      {"input[3]", 800} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6669063400062430227(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6669063400062430227(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6669063400062430227);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_SOFTMAX_FWD
namespace LAYER_CUDNN_SOFTMAX_FWD__13597052971789281607 {

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      19 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__13597052971789281607(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 19} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__13597052971789281607(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__13597052971789281607(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_INSTANCE)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_CHANNEL)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_FAST);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_ACCURATE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_LOG)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__13597052971789281607);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
}
#endif // ENABLE_LAYER_CUDNN_SOFTMAX_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__10818095207005908231 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10818095207005908231(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10818095207005908231(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10818095207005908231(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10818095207005908231);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__18278422918993393833 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18278422918993393833(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18278422918993393833(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__18278422918993393833(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__18278422918993393833);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15988199270513231628 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15988199270513231628(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15988199270513231628(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15988199270513231628(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15988199270513231628);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12650743081999984508 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12650743081999984508(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12650743081999984508(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12650743081999984508(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12650743081999984508);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8838781751936347992 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8838781751936347992(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8838781751936347992(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8838781751936347992(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8838781751936347992);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18257461080127169801 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18257461080127169801(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18257461080127169801(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18257461080127169801(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18257461080127169801);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__4610484160792007623 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4610484160792007623(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4610484160792007623(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4610484160792007623(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4610484160792007623);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17889512221495661062 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17889512221495661062(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17889512221495661062(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17889512221495661062(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17889512221495661062);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8748824854915822071 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8748824854915822071(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8748824854915822071(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8748824854915822071(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8748824854915822071);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7216094293720789912 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7216094293720789912(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7216094293720789912(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7216094293720789912(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7216094293720789912);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1133300748610517660 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1133300748610517660(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1133300748610517660(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1133300748610517660(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1133300748610517660);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4006692630664387207 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4006692630664387207(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4006692630664387207(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4006692630664387207(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4006692630664387207);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14746066742089024598 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14746066742089024598(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14746066742089024598(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14746066742089024598(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14746066742089024598);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18134801193599313089 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18134801193599313089(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18134801193599313089(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18134801193599313089(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18134801193599313089);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9233601495781143479 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9233601495781143479(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9233601495781143479(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9233601495781143479(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9233601495781143479);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15554666289150176486 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15554666289150176486(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15554666289150176486(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15554666289150176486(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15554666289150176486);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__7059716319619218776 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      208 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7059716319619218776(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 208} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7059716319619218776(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7059716319619218776(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7059716319619218776);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9648945597698420442 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9648945597698420442(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9648945597698420442(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9648945597698420442(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9648945597698420442);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2857745631286981712 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      18 /* PadHeight */, \
      18 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      18 /* DilationWidth */, \
      18 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2857745631286981712(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 18} /* PadHeight */, 
      {"pad_width", 18} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 18} /* DilationWidth */, 
      {"dilation_width", 18} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2857745631286981712(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2857745631286981712(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2857745631286981712);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6823671317724849844 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6823671317724849844(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6823671317724849844(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6823671317724849844(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6823671317724849844);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4648190217551941823 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4648190217551941823(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4648190217551941823(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4648190217551941823(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4648190217551941823);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1034588026934547243 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      544 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1034588026934547243(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1034588026934547243(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1034588026934547243(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1034588026934547243);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8297571634141727729 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8297571634141727729(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8297571634141727729(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8297571634141727729(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8297571634141727729);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__960838002080971033 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__960838002080971033(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__960838002080971033(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__960838002080971033(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__960838002080971033);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4333880348022893394 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4333880348022893394(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4333880348022893394(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4333880348022893394(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4333880348022893394);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__6473443639223627606 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6473443639223627606(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6473443639223627606(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6473443639223627606(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6473443639223627606);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__12444554535359479061 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12444554535359479061(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__12444554535359479061(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12444554535359479061(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__12444554535359479061)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8634621187108854777 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8634621187108854777(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8634621187108854777(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8634621187108854777(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8634621187108854777);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2797590667860634651 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2797590667860634651(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2797590667860634651(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2797590667860634651(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2797590667860634651);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13206541835836003983 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13206541835836003983(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13206541835836003983(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13206541835836003983(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13206541835836003983);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3651629432983394441 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3651629432983394441(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3651629432983394441(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3651629432983394441(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3651629432983394441);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4265681727510337615 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4265681727510337615(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4265681727510337615(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4265681727510337615(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4265681727510337615);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11181214175020609488 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11181214175020609488(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11181214175020609488(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11181214175020609488(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11181214175020609488);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__759914326251173488 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__759914326251173488(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__759914326251173488(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__759914326251173488(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__759914326251173488);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2827126172085976665 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2827126172085976665(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2827126172085976665(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2827126172085976665(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2827126172085976665);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16663102725372018339 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16663102725372018339(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16663102725372018339(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16663102725372018339(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16663102725372018339);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12424528218923690816 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      25 /* Input2 */, \
      25 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12424528218923690816(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 25} /* Input2 */, 
      {"input[3]", 25} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12424528218923690816(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12424528218923690816(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12424528218923690816);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6783278530329466122 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6783278530329466122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6783278530329466122(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6783278530329466122(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6783278530329466122);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__17619375045465429805 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17619375045465429805(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17619375045465429805(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17619375045465429805(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17619375045465429805);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12465600794165807773 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12465600794165807773(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12465600794165807773(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12465600794165807773(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12465600794165807773);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2740797455682743091 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2740797455682743091(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2740797455682743091(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2740797455682743091(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2740797455682743091);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6691433020158694876 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6691433020158694876(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6691433020158694876(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6691433020158694876(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6691433020158694876);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4007237897450042281 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4007237897450042281(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4007237897450042281(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4007237897450042281(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4007237897450042281);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12229792305167097478 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12229792305167097478(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12229792305167097478(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12229792305167097478(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12229792305167097478);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7562360568756780407 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7562360568756780407(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7562360568756780407(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7562360568756780407(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7562360568756780407);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14603693014952145773 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14603693014952145773(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14603693014952145773(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14603693014952145773(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14603693014952145773);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10536871922373149473 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10536871922373149473(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10536871922373149473(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10536871922373149473(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10536871922373149473);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4828607698675694393 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      3 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4828607698675694393(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4828607698675694393(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4828607698675694393(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4828607698675694393);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14259327654413281130 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14259327654413281130(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14259327654413281130(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14259327654413281130(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14259327654413281130);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8700263661632945034 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8700263661632945034(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8700263661632945034(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8700263661632945034(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8700263661632945034);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__17251107987348484754 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17251107987348484754(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17251107987348484754(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17251107987348484754(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17251107987348484754);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9503802833001444026 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      24 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9503802833001444026(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9503802833001444026(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9503802833001444026(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9503802833001444026);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__10734127099318901992 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10734127099318901992(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10734127099318901992(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10734127099318901992(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10734127099318901992);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14263831019593457275 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14263831019593457275(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14263831019593457275(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14263831019593457275(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14263831019593457275);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2124164833088588453 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2124164833088588453(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2124164833088588453(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2124164833088588453(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2124164833088588453);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9715077599448925830 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9715077599448925830(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9715077599448925830(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9715077599448925830(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9715077599448925830);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16284555761278325055 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16284555761278325055(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16284555761278325055(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16284555761278325055(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16284555761278325055);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5598375363668643157 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5598375363668643157(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5598375363668643157(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5598375363668643157(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5598375363668643157);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10022632152851108444 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10022632152851108444(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10022632152851108444(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10022632152851108444(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10022632152851108444);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4409428883504523733 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4409428883504523733(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4409428883504523733(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4409428883504523733(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4409428883504523733);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4832089379267381230 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4832089379267381230(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4832089379267381230(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4832089379267381230(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4832089379267381230);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1508671762290273807 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1508671762290273807(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1508671762290273807(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1508671762290273807(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1508671762290273807);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12010865927043240633 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12010865927043240633(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12010865927043240633(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12010865927043240633(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12010865927043240633);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16256264489076340519 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16256264489076340519(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16256264489076340519(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16256264489076340519(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16256264489076340519);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__18048158905373514585 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__18048158905373514585(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__18048158905373514585(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__18048158905373514585(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__18048158905373514585)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6486238417783462764 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6486238417783462764(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6486238417783462764(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6486238417783462764(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6486238417783462764);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12281959347864482037 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12281959347864482037(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12281959347864482037(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12281959347864482037(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12281959347864482037);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14674349336516983799 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      125 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14674349336516983799(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 125} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14674349336516983799(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14674349336516983799(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14674349336516983799);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3676793786416043881 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3676793786416043881(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3676793786416043881(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3676793786416043881(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3676793786416043881);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17684117603592552393 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17684117603592552393(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17684117603592552393(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17684117603592552393(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17684117603592552393);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9258930286705630179 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9258930286705630179(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9258930286705630179(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9258930286705630179(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9258930286705630179);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17026919867087613588 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17026919867087613588(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17026919867087613588(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17026919867087613588(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17026919867087613588);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4236813545579748226 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4236813545579748226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4236813545579748226(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4236813545579748226(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4236813545579748226);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11452075024834847756 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11452075024834847756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11452075024834847756(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11452075024834847756(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11452075024834847756);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7248623929783808270 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7248623929783808270(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7248623929783808270(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7248623929783808270(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7248623929783808270);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6049565943751530786 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6049565943751530786(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6049565943751530786(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6049565943751530786(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6049565943751530786);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1954566012347004536 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      208 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1954566012347004536(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 208} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1954566012347004536(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1954566012347004536(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1954566012347004536);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6069707469437629196 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6069707469437629196(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6069707469437629196(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6069707469437629196(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6069707469437629196);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5154402696139085552 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5154402696139085552(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5154402696139085552(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5154402696139085552(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5154402696139085552);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12468177010946544895 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12468177010946544895(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12468177010946544895(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12468177010946544895(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12468177010946544895);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9294842010163024501 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9294842010163024501(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9294842010163024501(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9294842010163024501(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9294842010163024501);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15879275359218280358 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15879275359218280358(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15879275359218280358(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15879275359218280358(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15879275359218280358);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15636489388137840673 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15636489388137840673(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15636489388137840673(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15636489388137840673(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15636489388137840673);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1487582472298713700 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1487582472298713700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1487582472298713700(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1487582472298713700(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1487582472298713700);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12315147312468526821 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12315147312468526821(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12315147312468526821(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12315147312468526821(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12315147312468526821);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8521822504429745749 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8521822504429745749(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8521822504429745749(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8521822504429745749(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8521822504429745749);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5730866989077438766 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5730866989077438766(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5730866989077438766(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5730866989077438766(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5730866989077438766);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11808211018047965336 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11808211018047965336(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11808211018047965336(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11808211018047965336(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11808211018047965336);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__1344592609883842756 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1344592609883842756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1344592609883842756(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1344592609883842756(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1344592609883842756);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5003516423658944989 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5003516423658944989(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5003516423658944989(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5003516423658944989(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5003516423658944989);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17436833547780997955 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17436833547780997955(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17436833547780997955(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17436833547780997955(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17436833547780997955);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12635031315166861956 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12635031315166861956(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12635031315166861956(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12635031315166861956(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12635031315166861956);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3027127743166960527 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3027127743166960527(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3027127743166960527(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3027127743166960527(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3027127743166960527);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10068432358227770105 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10068432358227770105(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10068432358227770105(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10068432358227770105(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10068432358227770105);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15909352926300161865 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15909352926300161865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15909352926300161865(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15909352926300161865(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15909352926300161865);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2838038037792287595 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2838038037792287595(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2838038037792287595(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2838038037792287595(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2838038037792287595);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13814811078792920795 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13814811078792920795(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13814811078792920795(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13814811078792920795(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13814811078792920795);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5843226960585132920 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5843226960585132920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5843226960585132920(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5843226960585132920(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5843226960585132920);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17498290317597582460 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17498290317597582460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17498290317597582460(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17498290317597582460(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17498290317597582460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15763508915701863320 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15763508915701863320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15763508915701863320(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15763508915701863320(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15763508915701863320);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2304251090791798043 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2304251090791798043(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2304251090791798043(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2304251090791798043(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2304251090791798043);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__114605383031093492 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      208 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__114605383031093492(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 208} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__114605383031093492(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__114605383031093492(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__114605383031093492);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__12869831023777779143 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12869831023777779143(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12869831023777779143(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12869831023777779143(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12869831023777779143);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5262483590892099340 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5262483590892099340(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5262483590892099340(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5262483590892099340(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5262483590892099340);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12413174571390015454 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12413174571390015454(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12413174571390015454(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12413174571390015454(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12413174571390015454);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__4375248671032379261 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4375248671032379261(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4375248671032379261(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4375248671032379261(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4375248671032379261);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6555086379940950942 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6555086379940950942(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6555086379940950942(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6555086379940950942(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6555086379940950942);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7001814890575336413 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7001814890575336413(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7001814890575336413(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7001814890575336413(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7001814890575336413);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3570656135359570289 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3570656135359570289(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3570656135359570289(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3570656135359570289(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3570656135359570289);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17903719095409183604 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17903719095409183604(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17903719095409183604(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17903719095409183604(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17903719095409183604);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4723897481503050258 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4723897481503050258(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4723897481503050258(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4723897481503050258(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4723897481503050258);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5957152629793531954 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5957152629793531954(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5957152629793531954(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5957152629793531954(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5957152629793531954);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6504749182202284931 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6504749182202284931(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6504749182202284931(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6504749182202284931(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6504749182202284931);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10412215032885525556 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10412215032885525556(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10412215032885525556(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10412215032885525556(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10412215032885525556);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__8030934738103883030 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8030934738103883030(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__8030934738103883030(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8030934738103883030(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__8030934738103883030);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16083075379233882272 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16083075379233882272(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16083075379233882272(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16083075379233882272(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16083075379233882272);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5340554487906921646 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5340554487906921646(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5340554487906921646(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5340554487906921646(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5340554487906921646);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__9117014991420576495 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9117014991420576495(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9117014991420576495(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9117014991420576495(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9117014991420576495);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9325872407758533953 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9325872407758533953(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9325872407758533953(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9325872407758533953(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9325872407758533953);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2565368023536361922 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2565368023536361922(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2565368023536361922(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2565368023536361922(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2565368023536361922);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15925492014981282420 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15925492014981282420(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15925492014981282420(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15925492014981282420(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15925492014981282420);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__3097879982925643077 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3097879982925643077(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3097879982925643077(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3097879982925643077(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3097879982925643077);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__15417294399166253268 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15417294399166253268(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15417294399166253268(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15417294399166253268(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15417294399166253268);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7990389023733593719 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7990389023733593719(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7990389023733593719(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7990389023733593719(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7990389023733593719);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__990605506810500869 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__990605506810500869(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__990605506810500869(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__990605506810500869(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__990605506810500869);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__238512757366442532 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__238512757366442532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__238512757366442532(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__238512757366442532(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__238512757366442532);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14916069354069299885 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14916069354069299885(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14916069354069299885(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14916069354069299885(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14916069354069299885);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7363549351973709642 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7363549351973709642(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7363549351973709642(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7363549351973709642(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7363549351973709642);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1016629175607899446 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1016629175607899446(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1016629175607899446(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1016629175607899446(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1016629175607899446);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15803010932320146739 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15803010932320146739(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15803010932320146739(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15803010932320146739(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15803010932320146739);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5621060589845698895 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5621060589845698895(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5621060589845698895(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5621060589845698895(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5621060589845698895);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__8449452341443488712 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8449452341443488712(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8449452341443488712(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8449452341443488712(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8449452341443488712);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8683653915108196878 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8683653915108196878(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8683653915108196878(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8683653915108196878(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8683653915108196878);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12605704175761247443 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12605704175761247443(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12605704175761247443(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12605704175761247443(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12605704175761247443);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15432349983326512559 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      9 /* PadHeight */, \
      9 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      9 /* DilationWidth */, \
      9 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15432349983326512559(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 9} /* PadHeight */, 
      {"pad_width", 9} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 9} /* DilationWidth */, 
      {"dilation_width", 9} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15432349983326512559(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15432349983326512559(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15432349983326512559);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6836059521560544984 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6836059521560544984(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6836059521560544984(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6836059521560544984(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6836059521560544984);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14854210746003732964 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14854210746003732964(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14854210746003732964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14854210746003732964(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14854210746003732964);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__2145699134529975342 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2145699134529975342(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2145699134529975342(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2145699134529975342(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2145699134529975342);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4490556987346914935 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4490556987346914935(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4490556987346914935(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4490556987346914935(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4490556987346914935);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14580635442107334439 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14580635442107334439(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14580635442107334439(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14580635442107334439(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14580635442107334439);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5620632359960874420 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5620632359960874420(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5620632359960874420(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5620632359960874420(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5620632359960874420);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6734582422958434694 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6734582422958434694(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6734582422958434694(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6734582422958434694(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6734582422958434694);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16927561188876496540 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16927561188876496540(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16927561188876496540(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16927561188876496540(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16927561188876496540);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__384966559750756026 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__384966559750756026(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__384966559750756026(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__384966559750756026(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__384966559750756026);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__5255818317527207655 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      4 /* Input2 */, \
      4 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5255818317527207655(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 4} /* Input2 */, 
      {"input[3]", 4} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__5255818317527207655(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5255818317527207655(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__5255818317527207655)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13660275852028966174 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13660275852028966174(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13660275852028966174(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13660275852028966174(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13660275852028966174);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12732263121457752951 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12732263121457752951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12732263121457752951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12732263121457752951(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12732263121457752951);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__14204697383349644394 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14204697383349644394(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14204697383349644394(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14204697383349644394(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14204697383349644394);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15309862681637846015 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15309862681637846015(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15309862681637846015(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15309862681637846015(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15309862681637846015);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7818327790552661918 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7818327790552661918(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7818327790552661918(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7818327790552661918(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7818327790552661918);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15307751066474493584 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15307751066474493584(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15307751066474493584(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15307751066474493584(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15307751066474493584);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__18427579088652692951 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18427579088652692951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18427579088652692951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18427579088652692951(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18427579088652692951);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11870364772504761369 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11870364772504761369(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11870364772504761369(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11870364772504761369(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11870364772504761369);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12089224516157111362 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12089224516157111362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12089224516157111362(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12089224516157111362(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12089224516157111362);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15814375177174416484 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15814375177174416484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15814375177174416484(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15814375177174416484(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15814375177174416484);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3019590341403624968 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3019590341403624968(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3019590341403624968(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3019590341403624968(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3019590341403624968);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__12817277817632848834 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12817277817632848834(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12817277817632848834(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12817277817632848834(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12817277817632848834);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5423766719861704526 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5423766719861704526(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5423766719861704526(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5423766719861704526(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5423766719861704526);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4470072855039606612 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4470072855039606612(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4470072855039606612(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4470072855039606612(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4470072855039606612);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__4915529266747223451 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4915529266747223451(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4915529266747223451(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4915529266747223451(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4915529266747223451);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__3507736250666407212 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3507736250666407212(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3507736250666407212(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3507736250666407212(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3507736250666407212);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9524581801025899483 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9524581801025899483(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9524581801025899483(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9524581801025899483(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9524581801025899483);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16545706083723398953 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16545706083723398953(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16545706083723398953(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16545706083723398953(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16545706083723398953);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__285220768880623287 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__285220768880623287(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__285220768880623287(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__285220768880623287(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__285220768880623287);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14326729297845881155 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14326729297845881155(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14326729297845881155(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14326729297845881155(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14326729297845881155);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__5351943047852315796 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5351943047852315796(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5351943047852315796(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5351943047852315796(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5351943047852315796);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8019053118553728595 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8019053118553728595(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8019053118553728595(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8019053118553728595(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8019053118553728595);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__6639847000331438717 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6639847000331438717(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6639847000331438717(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6639847000331438717(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6639847000331438717);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2978427362453565835 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2978427362453565835(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2978427362453565835(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2978427362453565835(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2978427362453565835);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16343746590260828930 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16343746590260828930(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16343746590260828930(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16343746590260828930(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16343746590260828930);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__250211895649964860 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      208 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__250211895649964860(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 208} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__250211895649964860(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__250211895649964860(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__250211895649964860);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__15287891913801402689 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__15287891913801402689(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__15287891913801402689(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__15287891913801402689(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__15287891913801402689)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14197051975683366577 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14197051975683366577(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14197051975683366577(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14197051975683366577(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14197051975683366577);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2602737036341327938 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2602737036341327938(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2602737036341327938(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2602737036341327938(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2602737036341327938);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11303291426826128596 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11303291426826128596(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11303291426826128596(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11303291426826128596(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11303291426826128596);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6485251749147745310 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6485251749147745310(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6485251749147745310(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6485251749147745310(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6485251749147745310);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7832034717382194271 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7832034717382194271(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7832034717382194271(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7832034717382194271(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7832034717382194271);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__11654343797645886428 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11654343797645886428(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11654343797645886428(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11654343797645886428(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11654343797645886428);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15585072505056495605 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15585072505056495605(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15585072505056495605(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15585072505056495605(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15585072505056495605);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__6011505783418020926 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6011505783418020926(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6011505783418020926(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6011505783418020926(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6011505783418020926);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15027650355333145423 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15027650355333145423(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15027650355333145423(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15027650355333145423(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15027650355333145423);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__1565190159406507594 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1565190159406507594(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1565190159406507594(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1565190159406507594(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1565190159406507594);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10447890849695639359 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10447890849695639359(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10447890849695639359(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10447890849695639359(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10447890849695639359);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14390106017409081590 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390106017409081590(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14390106017409081590(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14390106017409081590(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14390106017409081590);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__8839593797758077813 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8839593797758077813(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8839593797758077813(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8839593797758077813(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8839593797758077813);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__17983397124165458048 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17983397124165458048(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17983397124165458048(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17983397124165458048(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17983397124165458048);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__533220354579412363 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__533220354579412363(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__533220354579412363(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__533220354579412363(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__533220354579412363);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__10952060416275744361 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10952060416275744361(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10952060416275744361(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10952060416275744361(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10952060416275744361);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
namespace LAYER_CUDNN_DROPOUT_FWD__5537382924719046629 {

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5537382924719046629(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__5537382924719046629(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5537382924719046629(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__5537382924719046629)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
}
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15746245466401791519 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      9 /* PadHeight */, \
      9 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      9 /* DilationWidth */, \
      9 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15746245466401791519(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 9} /* PadHeight */, 
      {"pad_width", 9} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 9} /* DilationWidth */, 
      {"dilation_width", 9} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15746245466401791519(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15746245466401791519(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15746245466401791519);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14636978426006243531 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14636978426006243531(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14636978426006243531(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14636978426006243531(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14636978426006243531);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16172102362316884634 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16172102362316884634(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16172102362316884634(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16172102362316884634(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16172102362316884634);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__5164785364112063860 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5164785364112063860(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__5164785364112063860(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5164785364112063860(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__5164785364112063860);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__13532452845293194360 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13532452845293194360(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13532452845293194360(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13532452845293194360(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13532452845293194360);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__13933557767171476639 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13933557767171476639(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13933557767171476639(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13933557767171476639(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13933557767171476639);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9081830790021268687 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      25 /* Input2 */, \
      25 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9081830790021268687(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 25} /* Input2 */, 
      {"input[3]", 25} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9081830790021268687(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9081830790021268687(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9081830790021268687);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8965242889667010775 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8965242889667010775(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8965242889667010775(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8965242889667010775(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8965242889667010775);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7796388334076984680 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7796388334076984680(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7796388334076984680(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7796388334076984680(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7796388334076984680);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14181975055170451171 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14181975055170451171(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14181975055170451171(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14181975055170451171(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14181975055170451171);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14213872664689711255 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14213872664689711255(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14213872664689711255(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14213872664689711255(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14213872664689711255);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__16621634355948397919 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16621634355948397919(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16621634355948397919(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16621634355948397919(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16621634355948397919);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11202203050645120969 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11202203050645120969(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11202203050645120969(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11202203050645120969(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11202203050645120969);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15200300093025998377 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15200300093025998377(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15200300093025998377(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15200300093025998377(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15200300093025998377);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11649024218046444947 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11649024218046444947(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11649024218046444947(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11649024218046444947(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11649024218046444947);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__13108360870563450684 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13108360870563450684(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13108360870563450684(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13108360870563450684(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13108360870563450684);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4681721593369858052 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4681721593369858052(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4681721593369858052(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4681721593369858052(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4681721593369858052);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__10971821146484793597 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10971821146484793597(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10971821146484793597(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10971821146484793597(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10971821146484793597);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__14420205126483560977 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14420205126483560977(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14420205126483560977(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14420205126483560977(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14420205126483560977);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__11512105483808569324 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11512105483808569324(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11512105483808569324(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11512105483808569324(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11512105483808569324);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15493187894455365749 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15493187894455365749(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15493187894455365749(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15493187894455365749(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15493187894455365749);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__17128372555752070933 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17128372555752070933(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17128372555752070933(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17128372555752070933(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17128372555752070933);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__14434250467796648356 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14434250467796648356(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14434250467796648356(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14434250467796648356(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14434250467796648356);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10278717247706946013 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10278717247706946013(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10278717247706946013(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10278717247706946013(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10278717247706946013);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__5519145610061030412 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5519145610061030412(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__5519145610061030412(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5519145610061030412(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__5519145610061030412);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13598187726920221540 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13598187726920221540(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13598187726920221540(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13598187726920221540(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13598187726920221540);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15243046506630001033 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15243046506630001033(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15243046506630001033(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15243046506630001033(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15243046506630001033);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4863986124195955951 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4863986124195955951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4863986124195955951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4863986124195955951(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4863986124195955951);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__1753043122443293887 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1753043122443293887(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1753043122443293887(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1753043122443293887(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1753043122443293887);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__2098459214284558696 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      24 /* PadHeight */, \
      24 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      24 /* DilationWidth */, \
      24 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2098459214284558696(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 24} /* PadHeight */, 
      {"pad_width", 24} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 24} /* DilationWidth */, 
      {"dilation_width", 24} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2098459214284558696(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2098459214284558696(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2098459214284558696);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14071237675082495715 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      112 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14071237675082495715(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14071237675082495715(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14071237675082495715(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14071237675082495715);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15365825386583629399 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15365825386583629399(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15365825386583629399(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15365825386583629399(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15365825386583629399);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8055305311629470974 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8055305311629470974(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8055305311629470974(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8055305311629470974(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8055305311629470974);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__10248979953753916897 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      13 /* FilterHeight */, \
      13 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      13 /* StrideHeight */, \
      13 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10248979953753916897(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 13} /* FilterHeight */, 
      {"filter_width", 13} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 13} /* StrideHeight */, 
      {"stride_width", 13} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10248979953753916897(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10248979953753916897(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10248979953753916897);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14118583937227726282 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14118583937227726282(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14118583937227726282(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14118583937227726282(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14118583937227726282);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14330255055066258256 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14330255055066258256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14330255055066258256(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14330255055066258256(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14330255055066258256);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17939504645306688307 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      48 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17939504645306688307(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17939504645306688307(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17939504645306688307(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17939504645306688307);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__185402082359019852 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__185402082359019852(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__185402082359019852(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__185402082359019852(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__185402082359019852);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3494449469716050815 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3494449469716050815(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3494449469716050815(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3494449469716050815(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3494449469716050815);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__15152914316172412661 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15152914316172412661(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15152914316172412661(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15152914316172412661(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15152914316172412661);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7086577136029764794 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7086577136029764794(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7086577136029764794(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7086577136029764794(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7086577136029764794);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__1276926915647568104 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1276926915647568104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__1276926915647568104(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1276926915647568104(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__1276926915647568104);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__4775755019391116875 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4775755019391116875(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4775755019391116875(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4775755019391116875(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4775755019391116875);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9895830676175542748 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9895830676175542748(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9895830676175542748(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9895830676175542748(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9895830676175542748);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3982040743582781001 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3982040743582781001(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3982040743582781001(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3982040743582781001(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3982040743582781001);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__9297438689114771963 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9297438689114771963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9297438689114771963(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9297438689114771963(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9297438689114771963);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__981611549120045328 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__981611549120045328(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__981611549120045328(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__981611549120045328(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__981611549120045328);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__1644058051260114996 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1644058051260114996(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__1644058051260114996(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1644058051260114996(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__1644058051260114996);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__3923295290237875426 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3923295290237875426(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3923295290237875426(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3923295290237875426(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3923295290237875426);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
namespace LAYER_CUDNN_POOLING_FWD__8015183197456225918 {

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8015183197456225918(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__8015183197456225918(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8015183197456225918(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__8015183197456225918);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
}
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__16454533780932209974 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16454533780932209974(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16454533780932209974(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16454533780932209974(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16454533780932209974);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__3114880264698057716 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3114880264698057716(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3114880264698057716(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3114880264698057716(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3114880264698057716);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__7855059165091522626 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7855059165091522626(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7855059165091522626(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7855059165091522626(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7855059165091522626);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__7219701415158997291 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7219701415158997291(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7219701415158997291(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7219701415158997291(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7219701415158997291);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__5376619983552692807 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5376619983552692807(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5376619983552692807(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5376619983552692807(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5376619983552692807);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__10987352129783125247 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10987352129783125247(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10987352129783125247(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10987352129783125247(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10987352129783125247);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__15276189735910810967 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      5 /* PadHeight */, \
      5 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      5 /* DilationWidth */, \
      5 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15276189735910810967(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 5} /* PadHeight */, 
      {"pad_width", 5} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 5} /* DilationWidth */, 
      {"dilation_width", 5} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15276189735910810967(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15276189735910810967(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15276189735910810967);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__14191195462177800145 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14191195462177800145(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14191195462177800145(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14191195462177800145(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14191195462177800145);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__12220329695905413346 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12220329695905413346(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12220329695905413346(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12220329695905413346(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12220329695905413346);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__2245149244713081951 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2245149244713081951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2245149244713081951(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2245149244713081951(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2245149244713081951);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__9205971309873383367 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9205971309873383367(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9205971309873383367(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9205971309873383367(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9205971309873383367);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__5400579817651844852 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5400579817651844852(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5400579817651844852(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5400579817651844852(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5400579817651844852);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__12906699498097529863 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12906699498097529863(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12906699498097529863(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12906699498097529863(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12906699498097529863);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
namespace LAYER_CUDNN_BATCHNORM_FWD_INFERENCE__16213185831864804096 {

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16213185831864804096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16213185831864804096(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16213185831864804096(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16213185831864804096);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
}
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6695277625013028751 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6695277625013028751(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6695277625013028751(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6695277625013028751(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6695277625013028751);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
namespace LAYER_CUDNN_ACTIVATION_FWD__14920651464737974503 {

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14920651464737974503(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14920651464737974503(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14920651464737974503(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14920651464737974503);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
}
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__14953546815360698786 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14953546815360698786(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14953546815360698786(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14953546815360698786(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14953546815360698786);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__8752156446139139059 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8752156446139139059(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8752156446139139059(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8752156446139139059(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8752156446139139059);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__17314946311813941814 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17314946311813941814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17314946311813941814(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17314946311813941814(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17314946311813941814);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__13298328516018988038 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13298328516018988038(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13298328516018988038(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13298328516018988038(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13298328516018988038);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
namespace LAYER_CUDNN_CONV_FWD__6913504858844657167 {

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6913504858844657167(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6913504858844657167(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6913504858844657167(state);
}



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6913504858844657167);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
}
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

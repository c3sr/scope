
#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10815976983044789771(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10815976983044789771(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10815976983044789771(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10815976983044789771(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10815976983044789771(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10815976983044789771(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10815976983044789771(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10815976983044789771);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10815976983044789771);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10815976983044789771);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1526076194988172311(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1526076194988172311(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1526076194988172311(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1526076194988172311(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1526076194988172311(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1526076194988172311(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1526076194988172311(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1526076194988172311);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1526076194988172311);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1526076194988172311);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12167721417824008433(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__12167721417824008433(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12167721417824008433(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__12167721417824008433(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__12167721417824008433(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__12167721417824008433)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__12167721417824008433)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      109 /* Input2 */, \
      109 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17739051333128294912(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 109} /* Input2 */, 
      {"input[3]", 109} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__17739051333128294912(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17739051333128294912(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17739051333128294912(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17739051333128294912(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__17739051333128294912);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17739051333128294912);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16501447141592368020(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16501447141592368020(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16501447141592368020(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16501447141592368020(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16501447141592368020(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16501447141592368020);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16501447141592368020);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8391025895846745015(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8391025895846745015(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8391025895846745015(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8391025895846745015(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8391025895846745015(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8391025895846745015(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8391025895846745015(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8391025895846745015);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8391025895846745015);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8391025895846745015);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13452890497527797034(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13452890497527797034(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13452890497527797034(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13452890497527797034(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13452890497527797034(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13452890497527797034);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13452890497527797034);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__748963412443348644(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__748963412443348644(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__748963412443348644(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__748963412443348644(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__748963412443348644(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__748963412443348644(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__748963412443348644(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__748963412443348644);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__748963412443348644);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__748963412443348644);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5046804595380285631(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5046804595380285631(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5046804595380285631(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5046804595380285631(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5046804595380285631(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5046804595380285631(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5046804595380285631(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5046804595380285631);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5046804595380285631);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5046804595380285631);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18187442051482370133(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18187442051482370133(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18187442051482370133(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18187442051482370133(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18187442051482370133(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18187442051482370133);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18187442051482370133);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5479812510537388680(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5479812510537388680(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5479812510537388680(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5479812510537388680(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5479812510537388680(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5479812510537388680);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5479812510537388680);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13336827735552849900(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13336827735552849900(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13336827735552849900(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13336827735552849900(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13336827735552849900(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13336827735552849900(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13336827735552849900(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13336827735552849900);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13336827735552849900);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13336827735552849900);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5283391998996742362(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__5283391998996742362(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5283391998996742362(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__5283391998996742362(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5283391998996742362(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__5283391998996742362);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__5283391998996742362);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3613659456283945159(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3613659456283945159(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3613659456283945159(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3613659456283945159(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3613659456283945159(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3613659456283945159(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3613659456283945159(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3613659456283945159);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3613659456283945159);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3613659456283945159);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4218464646450672659(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4218464646450672659(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4218464646450672659(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4218464646450672659(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4218464646450672659(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4218464646450672659);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4218464646450672659);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18057369957820824848(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18057369957820824848(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18057369957820824848(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18057369957820824848(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18057369957820824848(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18057369957820824848(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18057369957820824848(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18057369957820824848);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18057369957820824848);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18057369957820824848);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17819926742398030435(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17819926742398030435(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17819926742398030435(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17819926742398030435(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17819926742398030435(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17819926742398030435(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17819926742398030435(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17819926742398030435);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17819926742398030435);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17819926742398030435);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10460150471496987151(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10460150471496987151(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10460150471496987151(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10460150471496987151(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10460150471496987151(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10460150471496987151(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10460150471496987151(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10460150471496987151);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10460150471496987151);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10460150471496987151);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3928190614282215906(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3928190614282215906(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3928190614282215906(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3928190614282215906(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3928190614282215906(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3928190614282215906);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3928190614282215906);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__212009591064529660(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__212009591064529660(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__212009591064529660(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__212009591064529660(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__212009591064529660(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__212009591064529660);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__212009591064529660);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6064733132942165888(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6064733132942165888(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6064733132942165888(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6064733132942165888(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6064733132942165888(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6064733132942165888(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6064733132942165888(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6064733132942165888);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6064733132942165888);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6064733132942165888);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12613139808080914047(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12613139808080914047(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12613139808080914047(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12613139808080914047(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12613139808080914047(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12613139808080914047(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12613139808080914047(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12613139808080914047);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12613139808080914047);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12613139808080914047);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13627406763406758748(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13627406763406758748(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13627406763406758748(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13627406763406758748(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13627406763406758748(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13627406763406758748(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13627406763406758748(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13627406763406758748);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13627406763406758748);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13627406763406758748);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14181944527127461424(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14181944527127461424(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14181944527127461424(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14181944527127461424(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14181944527127461424(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14181944527127461424);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14181944527127461424);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3425180217892165560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3425180217892165560(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3425180217892165560(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3425180217892165560(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3425180217892165560(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3425180217892165560);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3425180217892165560);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      111 /* Input2 */, \
      111 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8699958559738897465(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 111} /* Input2 */, 
      {"input[3]", 111} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__8699958559738897465(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8699958559738897465(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__8699958559738897465(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__8699958559738897465(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__8699958559738897465);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__8699958559738897465);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      208 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11179982850289320216(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 208} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11179982850289320216(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11179982850289320216(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11179982850289320216(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11179982850289320216(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11179982850289320216);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11179982850289320216);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__868886597652115037(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__868886597652115037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__868886597652115037(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__868886597652115037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__868886597652115037(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__868886597652115037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__868886597652115037(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__868886597652115037);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__868886597652115037);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__868886597652115037);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2491507842322748117(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2491507842322748117(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2491507842322748117(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2491507842322748117(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2491507842322748117(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2491507842322748117);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2491507842322748117);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14524077973685622827(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14524077973685622827(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14524077973685622827(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14524077973685622827(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14524077973685622827(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14524077973685622827(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14524077973685622827(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14524077973685622827);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14524077973685622827);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14524077973685622827);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3924696655275782273(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3924696655275782273(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3924696655275782273(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3924696655275782273(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3924696655275782273(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3924696655275782273);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3924696655275782273);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16536962244353608863(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16536962244353608863(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16536962244353608863(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16536962244353608863(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16536962244353608863(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16536962244353608863);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16536962244353608863);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5585553627707444520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5585553627707444520(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5585553627707444520(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5585553627707444520(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5585553627707444520(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5585553627707444520);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5585553627707444520);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5052084844290384176(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5052084844290384176(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5052084844290384176(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5052084844290384176(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5052084844290384176(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5052084844290384176(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5052084844290384176(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5052084844290384176);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5052084844290384176);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5052084844290384176);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10529769016690864489(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10529769016690864489(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10529769016690864489(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10529769016690864489(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10529769016690864489(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10529769016690864489(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10529769016690864489(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10529769016690864489);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10529769016690864489);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10529769016690864489);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16278626890822850842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16278626890822850842(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16278626890822850842(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16278626890822850842(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16278626890822850842(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16278626890822850842(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16278626890822850842(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16278626890822850842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16278626890822850842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16278626890822850842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11505820789338571520(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11505820789338571520(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11505820789338571520(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11505820789338571520(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11505820789338571520(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11505820789338571520(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11505820789338571520(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11505820789338571520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11505820789338571520);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11505820789338571520);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12601273786818485734(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__12601273786818485734(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12601273786818485734(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12601273786818485734(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12601273786818485734(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__12601273786818485734);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12601273786818485734);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2473513156032682079(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2473513156032682079(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2473513156032682079(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2473513156032682079(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2473513156032682079(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2473513156032682079);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2473513156032682079);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4591946692368597531(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4591946692368597531(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4591946692368597531(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4591946692368597531(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4591946692368597531(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4591946692368597531);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4591946692368597531);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5768607260508354083(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5768607260508354083(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5768607260508354083(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5768607260508354083(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5768607260508354083(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5768607260508354083);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5768607260508354083);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7158900322274878290(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7158900322274878290(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7158900322274878290(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7158900322274878290(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7158900322274878290(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7158900322274878290);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7158900322274878290);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5000808435320554139(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5000808435320554139(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5000808435320554139(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5000808435320554139(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5000808435320554139(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5000808435320554139);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5000808435320554139);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8431588640811743112(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8431588640811743112(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8431588640811743112(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8431588640811743112(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8431588640811743112(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8431588640811743112);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8431588640811743112);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14988149825620892131(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14988149825620892131(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14988149825620892131(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14988149825620892131(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14988149825620892131(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14988149825620892131(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14988149825620892131(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14988149825620892131);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14988149825620892131);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14988149825620892131);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029049841936521675(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16029049841936521675(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029049841936521675(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16029049841936521675(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029049841936521675(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16029049841936521675(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16029049841936521675(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16029049841936521675);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16029049841936521675);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16029049841936521675);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11410467705677237139(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11410467705677237139(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11410467705677237139(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11410467705677237139(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11410467705677237139(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11410467705677237139);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11410467705677237139);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      96 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6400767465845332187(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6400767465845332187(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6400767465845332187(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6400767465845332187(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6400767465845332187(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6400767465845332187(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6400767465845332187(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6400767465845332187);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6400767465845332187);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6400767465845332187);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13206908632265086933(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13206908632265086933(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13206908632265086933(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13206908632265086933(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13206908632265086933(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13206908632265086933);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13206908632265086933);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10099138632270857505(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10099138632270857505(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10099138632270857505(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10099138632270857505(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10099138632270857505(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10099138632270857505);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10099138632270857505);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10810072933000466351(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10810072933000466351(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10810072933000466351(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10810072933000466351(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10810072933000466351(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10810072933000466351);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10810072933000466351);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13342178576729139953(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13342178576729139953(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13342178576729139953(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13342178576729139953(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13342178576729139953(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13342178576729139953(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13342178576729139953(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13342178576729139953);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13342178576729139953);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13342178576729139953);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__187516384653618758(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__187516384653618758(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__187516384653618758(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__187516384653618758(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__187516384653618758(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__187516384653618758);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__187516384653618758);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12195845546254919411(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12195845546254919411(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12195845546254919411(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12195845546254919411(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12195845546254919411(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12195845546254919411(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12195845546254919411(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12195845546254919411);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12195845546254919411);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12195845546254919411);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2474458627239105842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2474458627239105842(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2474458627239105842(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2474458627239105842(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2474458627239105842(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2474458627239105842);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2474458627239105842);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8847755090713485385(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8847755090713485385(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8847755090713485385(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8847755090713485385(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8847755090713485385(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8847755090713485385(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8847755090713485385(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8847755090713485385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8847755090713485385);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8847755090713485385);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9589614956263522028(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9589614956263522028(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9589614956263522028(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9589614956263522028(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9589614956263522028(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9589614956263522028(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9589614956263522028(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9589614956263522028);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9589614956263522028);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9589614956263522028);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7180002810907689571(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7180002810907689571(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7180002810907689571(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7180002810907689571(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7180002810907689571(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7180002810907689571(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7180002810907689571(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7180002810907689571);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7180002810907689571);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7180002810907689571);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11740663292480802126(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11740663292480802126(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11740663292480802126(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11740663292480802126(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11740663292480802126(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11740663292480802126);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11740663292480802126);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17789058214015538293(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17789058214015538293(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17789058214015538293(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17789058214015538293(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17789058214015538293(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17789058214015538293);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17789058214015538293);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__146638020796686043(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__146638020796686043(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__146638020796686043(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__146638020796686043(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__146638020796686043(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__146638020796686043);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__146638020796686043);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6078779431213774988(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6078779431213774988(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6078779431213774988(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6078779431213774988(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6078779431213774988(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6078779431213774988);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6078779431213774988);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9115400738014989200(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9115400738014989200(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9115400738014989200(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9115400738014989200(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9115400738014989200(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9115400738014989200);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9115400738014989200);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17117827050974523869(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17117827050974523869(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17117827050974523869(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17117827050974523869(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17117827050974523869(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17117827050974523869);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17117827050974523869);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13562563597333318101(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13562563597333318101(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13562563597333318101(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13562563597333318101(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13562563597333318101(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13562563597333318101(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13562563597333318101(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13562563597333318101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13562563597333318101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13562563597333318101);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      320 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11793530346666283194(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11793530346666283194(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11793530346666283194(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11793530346666283194(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11793530346666283194(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11793530346666283194(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11793530346666283194(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11793530346666283194);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11793530346666283194);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11793530346666283194);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5786636869133874680(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5786636869133874680(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5786636869133874680(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5786636869133874680(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5786636869133874680(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5786636869133874680(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5786636869133874680(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5786636869133874680);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5786636869133874680);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5786636869133874680);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10811005499807458594(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10811005499807458594(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10811005499807458594(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10811005499807458594(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10811005499807458594(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10811005499807458594(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10811005499807458594(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10811005499807458594);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10811005499807458594);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10811005499807458594);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12098852990732506323(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__12098852990732506323(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12098852990732506323(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12098852990732506323(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12098852990732506323(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__12098852990732506323);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12098852990732506323);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5138370255701447154(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5138370255701447154(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5138370255701447154(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5138370255701447154(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5138370255701447154(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5138370255701447154(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5138370255701447154(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5138370255701447154);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5138370255701447154);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5138370255701447154);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2105089547714596840(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2105089547714596840(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2105089547714596840(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2105089547714596840(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2105089547714596840(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2105089547714596840(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2105089547714596840(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2105089547714596840);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2105089547714596840);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2105089547714596840);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7578502565670064512(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7578502565670064512(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7578502565670064512(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7578502565670064512(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7578502565670064512(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7578502565670064512(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7578502565670064512(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7578502565670064512);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7578502565670064512);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7578502565670064512);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4323230770192265240(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4323230770192265240(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4323230770192265240(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4323230770192265240(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4323230770192265240(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4323230770192265240);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4323230770192265240);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16140463849040196111(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16140463849040196111(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16140463849040196111(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16140463849040196111(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16140463849040196111(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16140463849040196111(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16140463849040196111(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16140463849040196111);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16140463849040196111);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16140463849040196111);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6251759826264027950(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6251759826264027950(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6251759826264027950(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6251759826264027950(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6251759826264027950(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6251759826264027950);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6251759826264027950);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14676717698504869490(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14676717698504869490(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14676717698504869490(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14676717698504869490(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14676717698504869490(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14676717698504869490);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14676717698504869490);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14713032870015447864(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14713032870015447864(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14713032870015447864(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14713032870015447864(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14713032870015447864(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14713032870015447864);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14713032870015447864);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__956429273765280612(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__956429273765280612(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__956429273765280612(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__956429273765280612(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__956429273765280612(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__956429273765280612);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__956429273765280612);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      11 /* FilterHeight */, \
      11 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      4 /* StrideHeight */, \
      4 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8088853784562335493(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 11} /* FilterHeight */, 
      {"filter_width", 11} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 4} /* StrideHeight */, 
      {"stride_width", 4} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8088853784562335493(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8088853784562335493(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8088853784562335493(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8088853784562335493(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8088853784562335493(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8088853784562335493(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8088853784562335493);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8088853784562335493);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8088853784562335493);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045002077308908932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11045002077308908932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045002077308908932(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11045002077308908932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045002077308908932(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11045002077308908932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11045002077308908932(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11045002077308908932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11045002077308908932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11045002077308908932);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8217646678157006037(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8217646678157006037(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8217646678157006037(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8217646678157006037(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8217646678157006037(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8217646678157006037);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8217646678157006037);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5161964240668923628(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__5161964240668923628(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5161964240668923628(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__5161964240668923628(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5161964240668923628(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__5161964240668923628);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__5161964240668923628);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15938132403477944585(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15938132403477944585(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15938132403477944585(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15938132403477944585(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15938132403477944585(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15938132403477944585);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15938132403477944585);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14285655036421638874(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14285655036421638874(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14285655036421638874(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14285655036421638874(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14285655036421638874(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14285655036421638874);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14285655036421638874);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8522501494854013664(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8522501494854013664(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8522501494854013664(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8522501494854013664(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8522501494854013664(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8522501494854013664(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8522501494854013664(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8522501494854013664);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8522501494854013664);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8522501494854013664);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6726101209495750461(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6726101209495750461(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6726101209495750461(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6726101209495750461(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6726101209495750461(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6726101209495750461(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6726101209495750461(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6726101209495750461);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6726101209495750461);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6726101209495750461);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5251382234019059880(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5251382234019059880(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5251382234019059880(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5251382234019059880(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5251382234019059880(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5251382234019059880(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5251382234019059880(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5251382234019059880);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5251382234019059880);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5251382234019059880);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__15007716831143070885(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__15007716831143070885(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__15007716831143070885(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__15007716831143070885(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__15007716831143070885(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__15007716831143070885)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__15007716831143070885)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15025882225283396304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15025882225283396304(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15025882225283396304(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15025882225283396304(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15025882225283396304(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15025882225283396304);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15025882225283396304);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      13 /* FilterHeight */, \
      13 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      13 /* StrideHeight */, \
      13 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10594902337483250805(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 13} /* FilterHeight */, 
      {"filter_width", 13} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 13} /* StrideHeight */, 
      {"stride_width", 13} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__10594902337483250805(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10594902337483250805(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10594902337483250805(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10594902337483250805(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__10594902337483250805);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10594902337483250805);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5000781136389634449(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5000781136389634449(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5000781136389634449(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5000781136389634449(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5000781136389634449(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5000781136389634449(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5000781136389634449(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5000781136389634449);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5000781136389634449);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5000781136389634449);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14483693791612159164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__14483693791612159164(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14483693791612159164(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14483693791612159164(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14483693791612159164(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__14483693791612159164);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14483693791612159164);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4171792268151234235(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4171792268151234235(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4171792268151234235(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4171792268151234235(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4171792268151234235(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4171792268151234235(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4171792268151234235(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4171792268151234235);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4171792268151234235);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4171792268151234235);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12936009885703088571(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12936009885703088571(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12936009885703088571(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12936009885703088571(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12936009885703088571(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12936009885703088571(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12936009885703088571(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12936009885703088571);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12936009885703088571);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12936009885703088571);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__431937515690610498(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__431937515690610498(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__431937515690610498(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__431937515690610498(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__431937515690610498(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__431937515690610498(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__431937515690610498(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__431937515690610498);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__431937515690610498);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__431937515690610498);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073756770377917059(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13073756770377917059(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073756770377917059(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13073756770377917059(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073756770377917059(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13073756770377917059(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13073756770377917059(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13073756770377917059);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13073756770377917059);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13073756770377917059);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4795179933268198405(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4795179933268198405(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4795179933268198405(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4795179933268198405(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4795179933268198405(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4795179933268198405);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4795179933268198405);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2282097860619404064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2282097860619404064(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2282097860619404064(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2282097860619404064(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2282097860619404064(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2282097860619404064);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2282097860619404064);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2261930468346135681(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2261930468346135681(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2261930468346135681(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2261930468346135681(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2261930468346135681(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2261930468346135681);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2261930468346135681);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1704571810670300453(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1704571810670300453(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1704571810670300453(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1704571810670300453(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1704571810670300453(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1704571810670300453);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1704571810670300453);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12032237318630340157(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12032237318630340157(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12032237318630340157(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12032237318630340157(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12032237318630340157(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12032237318630340157);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12032237318630340157);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12837232080084759370(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__12837232080084759370(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12837232080084759370(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12837232080084759370(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12837232080084759370(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__12837232080084759370);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12837232080084759370);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14780513298809048234(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14780513298809048234(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14780513298809048234(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14780513298809048234(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14780513298809048234(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14780513298809048234);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14780513298809048234);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16244987982778991164(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__16244987982778991164(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16244987982778991164(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__16244987982778991164(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16244987982778991164(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__16244987982778991164);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__16244987982778991164);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12694757873303030452(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12694757873303030452(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12694757873303030452(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12694757873303030452(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12694757873303030452(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12694757873303030452);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12694757873303030452);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1157796883211625169(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1157796883211625169(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1157796883211625169(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1157796883211625169(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1157796883211625169(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1157796883211625169);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1157796883211625169);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7247905046140210777(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7247905046140210777(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7247905046140210777(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7247905046140210777(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7247905046140210777(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7247905046140210777(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7247905046140210777(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7247905046140210777);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7247905046140210777);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7247905046140210777);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5678026154536948741(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5678026154536948741(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5678026154536948741(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5678026154536948741(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5678026154536948741(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5678026154536948741(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5678026154536948741(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5678026154536948741);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5678026154536948741);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5678026154536948741);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10932869071584989172(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10932869071584989172(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10932869071584989172(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10932869071584989172(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10932869071584989172(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10932869071584989172(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10932869071584989172(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10932869071584989172);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10932869071584989172);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10932869071584989172);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14000707319440240215(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14000707319440240215(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14000707319440240215(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14000707319440240215(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14000707319440240215(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14000707319440240215(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14000707319440240215(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14000707319440240215);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14000707319440240215);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14000707319440240215);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6726267724886932726(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6726267724886932726(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6726267724886932726(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6726267724886932726(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6726267724886932726(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6726267724886932726(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6726267724886932726(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6726267724886932726);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6726267724886932726);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6726267724886932726);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__763229438962398963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__763229438962398963(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__763229438962398963(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__763229438962398963(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__763229438962398963(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__763229438962398963);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__763229438962398963);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9275217762979818494(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9275217762979818494(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9275217762979818494(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9275217762979818494(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9275217762979818494(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9275217762979818494(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9275217762979818494(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9275217762979818494);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9275217762979818494);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9275217762979818494);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10292490042747455632(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10292490042747455632(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10292490042747455632(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10292490042747455632(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10292490042747455632(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10292490042747455632(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10292490042747455632(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10292490042747455632);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10292490042747455632);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10292490042747455632);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5079599551162472663(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5079599551162472663(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5079599551162472663(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5079599551162472663(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5079599551162472663(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5079599551162472663);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5079599551162472663);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15002806021068758116(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15002806021068758116(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15002806021068758116(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15002806021068758116(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15002806021068758116(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15002806021068758116);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15002806021068758116);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15193649645893557787(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15193649645893557787(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15193649645893557787(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15193649645893557787(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15193649645893557787(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15193649645893557787(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15193649645893557787(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15193649645893557787);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15193649645893557787);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15193649645893557787);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3143378556990070254(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3143378556990070254(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3143378556990070254(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3143378556990070254(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3143378556990070254(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3143378556990070254);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3143378556990070254);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      96 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6967368174598158785(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6967368174598158785(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6967368174598158785(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6967368174598158785(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6967368174598158785(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6967368174598158785(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6967368174598158785(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6967368174598158785);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6967368174598158785);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6967368174598158785);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3451308362975351421(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3451308362975351421(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3451308362975351421(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3451308362975351421(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3451308362975351421(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3451308362975351421(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3451308362975351421(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3451308362975351421);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3451308362975351421);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3451308362975351421);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3499621458178447653(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3499621458178447653(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3499621458178447653(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3499621458178447653(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3499621458178447653(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3499621458178447653);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3499621458178447653);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10036234530271664118(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10036234530271664118(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10036234530271664118(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10036234530271664118(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10036234530271664118(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10036234530271664118);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10036234530271664118);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14089360318332128525(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14089360318332128525(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14089360318332128525(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14089360318332128525(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14089360318332128525(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14089360318332128525);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14089360318332128525);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2645773156969586777(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2645773156969586777(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2645773156969586777(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2645773156969586777(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2645773156969586777(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2645773156969586777(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2645773156969586777(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2645773156969586777);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2645773156969586777);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2645773156969586777);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5078142141453631301(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5078142141453631301(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5078142141453631301(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5078142141453631301(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5078142141453631301(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5078142141453631301);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5078142141453631301);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9537519391268133308(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9537519391268133308(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9537519391268133308(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9537519391268133308(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9537519391268133308(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9537519391268133308(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9537519391268133308(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9537519391268133308);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9537519391268133308);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9537519391268133308);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16336354208971779168(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16336354208971779168(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16336354208971779168(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16336354208971779168(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16336354208971779168(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16336354208971779168);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16336354208971779168);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6922452264981680806(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6922452264981680806(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6922452264981680806(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6922452264981680806(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6922452264981680806(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6922452264981680806(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6922452264981680806(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6922452264981680806);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6922452264981680806);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6922452264981680806);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__203003208681466466(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__203003208681466466(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__203003208681466466(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__203003208681466466(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__203003208681466466(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__203003208681466466(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__203003208681466466(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__203003208681466466);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__203003208681466466);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__203003208681466466);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14296220451854662461(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14296220451854662461(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14296220451854662461(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14296220451854662461(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14296220451854662461(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14296220451854662461);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14296220451854662461);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10414935754128708560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10414935754128708560(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10414935754128708560(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10414935754128708560(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10414935754128708560(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10414935754128708560);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10414935754128708560);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7645167491719395316(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7645167491719395316(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7645167491719395316(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7645167491719395316(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7645167491719395316(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7645167491719395316(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7645167491719395316(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7645167491719395316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7645167491719395316);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7645167491719395316);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9606137383848118606(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9606137383848118606(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9606137383848118606(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9606137383848118606(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9606137383848118606(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9606137383848118606(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9606137383848118606(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9606137383848118606);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9606137383848118606);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9606137383848118606);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13739081449221971001(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13739081449221971001(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13739081449221971001(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13739081449221971001(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13739081449221971001(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13739081449221971001);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13739081449221971001);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12911122204820650572(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__12911122204820650572(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12911122204820650572(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12911122204820650572(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12911122204820650572(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__12911122204820650572);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12911122204820650572);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8942209838158982764(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8942209838158982764(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8942209838158982764(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8942209838158982764(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8942209838158982764(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8942209838158982764(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8942209838158982764(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8942209838158982764);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8942209838158982764);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8942209838158982764);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__196179758851327265(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__196179758851327265(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__196179758851327265(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__196179758851327265(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__196179758851327265(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__196179758851327265(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__196179758851327265(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__196179758851327265);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__196179758851327265);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__196179758851327265);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12939401273880403752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12939401273880403752(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12939401273880403752(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12939401273880403752(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12939401273880403752(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12939401273880403752(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12939401273880403752(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12939401273880403752);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12939401273880403752);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12939401273880403752);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2564286975914416446(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2564286975914416446(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2564286975914416446(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2564286975914416446(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2564286975914416446(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2564286975914416446);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2564286975914416446);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17953116658406466790(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17953116658406466790(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17953116658406466790(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17953116658406466790(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17953116658406466790(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17953116658406466790(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17953116658406466790(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17953116658406466790);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17953116658406466790);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17953116658406466790);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14775954713735603951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__14775954713735603951(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14775954713735603951(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14775954713735603951(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14775954713735603951(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__14775954713735603951);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14775954713735603951);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2067883177166444822(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2067883177166444822(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2067883177166444822(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2067883177166444822(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2067883177166444822(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2067883177166444822);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2067883177166444822);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4951447285616920190(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4951447285616920190(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4951447285616920190(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4951447285616920190(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4951447285616920190(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4951447285616920190(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4951447285616920190(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4951447285616920190);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4951447285616920190);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4951447285616920190);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13434935669841583760(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13434935669841583760(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13434935669841583760(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13434935669841583760(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13434935669841583760(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13434935669841583760(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13434935669841583760(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13434935669841583760);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13434935669841583760);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13434935669841583760);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12330601793814287098(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12330601793814287098(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12330601793814287098(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12330601793814287098(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12330601793814287098(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12330601793814287098);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12330601793814287098);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3398909505926771860(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3398909505926771860(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3398909505926771860(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3398909505926771860(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3398909505926771860(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3398909505926771860);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3398909505926771860);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18023570715464185376(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18023570715464185376(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18023570715464185376(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18023570715464185376(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18023570715464185376(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18023570715464185376(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18023570715464185376(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18023570715464185376);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18023570715464185376);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18023570715464185376);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1687255204434338645(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1687255204434338645(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1687255204434338645(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1687255204434338645(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1687255204434338645(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1687255204434338645);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1687255204434338645);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9868079343209390608(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9868079343209390608(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9868079343209390608(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9868079343209390608(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9868079343209390608(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9868079343209390608);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9868079343209390608);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6615445093641311738(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6615445093641311738(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6615445093641311738(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6615445093641311738(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6615445093641311738(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6615445093641311738);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6615445093641311738);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__605336469353451040(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__605336469353451040(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__605336469353451040(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__605336469353451040(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__605336469353451040(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__605336469353451040);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__605336469353451040);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17875239618748458137(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17875239618748458137(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17875239618748458137(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17875239618748458137(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17875239618748458137(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17875239618748458137(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17875239618748458137(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17875239618748458137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17875239618748458137);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17875239618748458137);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15050155952360157135(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15050155952360157135(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15050155952360157135(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15050155952360157135(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15050155952360157135(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15050155952360157135);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15050155952360157135);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3636824657610664024(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3636824657610664024(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3636824657610664024(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3636824657610664024(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3636824657610664024(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3636824657610664024(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3636824657610664024(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3636824657610664024);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3636824657610664024);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3636824657610664024);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2125528659808848832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2125528659808848832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2125528659808848832(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2125528659808848832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2125528659808848832(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2125528659808848832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2125528659808848832(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2125528659808848832);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2125528659808848832);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2125528659808848832);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5784881107234391039(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5784881107234391039(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5784881107234391039(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5784881107234391039(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5784881107234391039(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5784881107234391039);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5784881107234391039);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1755501856470839208(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1755501856470839208(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1755501856470839208(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1755501856470839208(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1755501856470839208(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1755501856470839208);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1755501856470839208);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13032888352281637364(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13032888352281637364(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13032888352281637364(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13032888352281637364(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13032888352281637364(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13032888352281637364);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13032888352281637364);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12535030195168611700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12535030195168611700(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12535030195168611700(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12535030195168611700(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12535030195168611700(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12535030195168611700(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12535030195168611700(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12535030195168611700);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12535030195168611700);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12535030195168611700);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10141362813043809324(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10141362813043809324(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10141362813043809324(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10141362813043809324(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10141362813043809324(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10141362813043809324);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10141362813043809324);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3943394671765491342(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3943394671765491342(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3943394671765491342(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3943394671765491342(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3943394671765491342(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3943394671765491342(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3943394671765491342(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3943394671765491342);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3943394671765491342);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3943394671765491342);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4780224930908629036(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4780224930908629036(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4780224930908629036(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4780224930908629036(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4780224930908629036(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4780224930908629036(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4780224930908629036(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4780224930908629036);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4780224930908629036);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4780224930908629036);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4643874613456953975(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4643874613456953975(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4643874613456953975(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4643874613456953975(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4643874613456953975(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4643874613456953975);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4643874613456953975);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6943596775039659105(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__6943596775039659105(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6943596775039659105(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6943596775039659105(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6943596775039659105(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__6943596775039659105);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6943596775039659105);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12804041709797828844(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12804041709797828844(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12804041709797828844(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12804041709797828844(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12804041709797828844(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12804041709797828844(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12804041709797828844(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12804041709797828844);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12804041709797828844);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12804041709797828844);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5507333605881161328(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__5507333605881161328(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5507333605881161328(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__5507333605881161328(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5507333605881161328(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__5507333605881161328);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__5507333605881161328);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11995930380626371162(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11995930380626371162(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11995930380626371162(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11995930380626371162(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11995930380626371162(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11995930380626371162(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11995930380626371162(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11995930380626371162);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11995930380626371162);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11995930380626371162);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13282675397875566003(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13282675397875566003(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13282675397875566003(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13282675397875566003(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13282675397875566003(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13282675397875566003);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13282675397875566003);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1924451277408661477(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1924451277408661477(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1924451277408661477(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1924451277408661477(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1924451277408661477(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1924451277408661477);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1924451277408661477);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10520044039649488355(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10520044039649488355(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10520044039649488355(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10520044039649488355(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10520044039649488355(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10520044039649488355);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10520044039649488355);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10664983003851095783(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10664983003851095783(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10664983003851095783(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10664983003851095783(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10664983003851095783(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10664983003851095783);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10664983003851095783);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1256425355898429004(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1256425355898429004(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1256425355898429004(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1256425355898429004(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1256425355898429004(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1256425355898429004(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1256425355898429004(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1256425355898429004);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1256425355898429004);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1256425355898429004);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2746726302535164694(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2746726302535164694(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2746726302535164694(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2746726302535164694(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2746726302535164694(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2746726302535164694(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2746726302535164694(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2746726302535164694);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2746726302535164694);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2746726302535164694);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12288126360362903256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12288126360362903256(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12288126360362903256(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12288126360362903256(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12288126360362903256(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12288126360362903256(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12288126360362903256(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12288126360362903256);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12288126360362903256);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12288126360362903256);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13051997596057960526(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13051997596057960526(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13051997596057960526(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13051997596057960526(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13051997596057960526(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13051997596057960526);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13051997596057960526);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12741086912339104705(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12741086912339104705(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12741086912339104705(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12741086912339104705(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12741086912339104705(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12741086912339104705);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12741086912339104705);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4552346479222252091(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4552346479222252091(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4552346479222252091(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4552346479222252091(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4552346479222252091(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4552346479222252091);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4552346479222252091);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__367751051239306704(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__367751051239306704(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__367751051239306704(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__367751051239306704(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__367751051239306704(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__367751051239306704(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__367751051239306704(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__367751051239306704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__367751051239306704);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__367751051239306704);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5698643873604482704(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5698643873604482704(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5698643873604482704(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5698643873604482704(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5698643873604482704(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5698643873604482704);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5698643873604482704);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11137205046061780963(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11137205046061780963(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11137205046061780963(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11137205046061780963(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11137205046061780963(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11137205046061780963(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11137205046061780963(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11137205046061780963);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11137205046061780963);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11137205046061780963);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7466221231019192882(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7466221231019192882(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7466221231019192882(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7466221231019192882(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7466221231019192882(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7466221231019192882);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7466221231019192882);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4426954466137392924(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4426954466137392924(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4426954466137392924(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4426954466137392924(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4426954466137392924(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4426954466137392924(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4426954466137392924(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4426954466137392924);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4426954466137392924);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4426954466137392924);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3483573232498814044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3483573232498814044(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3483573232498814044(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3483573232498814044(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3483573232498814044(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3483573232498814044);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3483573232498814044);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__776905583418386017(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__776905583418386017(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__776905583418386017(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__776905583418386017(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__776905583418386017(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__776905583418386017);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__776905583418386017);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11077476028821147096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11077476028821147096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11077476028821147096(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11077476028821147096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11077476028821147096(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11077476028821147096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11077476028821147096(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11077476028821147096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11077476028821147096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11077476028821147096);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14508349231888301737(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14508349231888301737(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14508349231888301737(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14508349231888301737(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14508349231888301737(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14508349231888301737);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14508349231888301737);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15060505997677874499(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__15060505997677874499(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15060505997677874499(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15060505997677874499(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15060505997677874499(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__15060505997677874499);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15060505997677874499);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10720790428457472123(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10720790428457472123(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10720790428457472123(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10720790428457472123(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10720790428457472123(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10720790428457472123(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10720790428457472123(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10720790428457472123);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10720790428457472123);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10720790428457472123);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15210745605079163067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15210745605079163067(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15210745605079163067(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15210745605079163067(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15210745605079163067(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15210745605079163067);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15210745605079163067);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9591628838612958087(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9591628838612958087(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9591628838612958087(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9591628838612958087(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9591628838612958087(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9591628838612958087);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9591628838612958087);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18415789202055138104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18415789202055138104(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18415789202055138104(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18415789202055138104(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18415789202055138104(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18415789202055138104(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18415789202055138104(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18415789202055138104);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18415789202055138104);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18415789202055138104);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7853484266379740450(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7853484266379740450(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7853484266379740450(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7853484266379740450(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7853484266379740450(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7853484266379740450);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7853484266379740450);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8627628207626040630(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8627628207626040630(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8627628207626040630(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8627628207626040630(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8627628207626040630(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8627628207626040630);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8627628207626040630);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12422962783724608794(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12422962783724608794(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12422962783724608794(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12422962783724608794(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12422962783724608794(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12422962783724608794(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12422962783724608794(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12422962783724608794);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12422962783724608794);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12422962783724608794);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1758817446524199742(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1758817446524199742(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1758817446524199742(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1758817446524199742(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1758817446524199742(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1758817446524199742);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1758817446524199742);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11770882687277300877(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11770882687277300877(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11770882687277300877(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11770882687277300877(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11770882687277300877(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11770882687277300877);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11770882687277300877);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4189150158755866329(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4189150158755866329(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4189150158755866329(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4189150158755866329(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4189150158755866329(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4189150158755866329(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4189150158755866329(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4189150158755866329);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4189150158755866329);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4189150158755866329);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13696033290079660578(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13696033290079660578(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13696033290079660578(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13696033290079660578(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13696033290079660578(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13696033290079660578(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13696033290079660578(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13696033290079660578);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13696033290079660578);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13696033290079660578);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__413091796872647771(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__413091796872647771(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__413091796872647771(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__413091796872647771(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__413091796872647771(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__413091796872647771(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__413091796872647771(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__413091796872647771);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__413091796872647771);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__413091796872647771);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7074725500018564449(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7074725500018564449(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7074725500018564449(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7074725500018564449(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7074725500018564449(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7074725500018564449);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7074725500018564449);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13049748800346937505(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13049748800346937505(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13049748800346937505(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13049748800346937505(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13049748800346937505(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13049748800346937505);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13049748800346937505);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8469539835640869750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8469539835640869750(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8469539835640869750(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8469539835640869750(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8469539835640869750(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8469539835640869750(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8469539835640869750(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8469539835640869750);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8469539835640869750);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8469539835640869750);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16107867830088824038(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16107867830088824038(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16107867830088824038(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16107867830088824038(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16107867830088824038(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16107867830088824038);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16107867830088824038);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14554122420320205197(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14554122420320205197(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14554122420320205197(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14554122420320205197(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14554122420320205197(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14554122420320205197);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14554122420320205197);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12841448640101263661(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12841448640101263661(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12841448640101263661(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12841448640101263661(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12841448640101263661(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12841448640101263661);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12841448640101263661);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17430044110932738140(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17430044110932738140(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17430044110932738140(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17430044110932738140(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17430044110932738140(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17430044110932738140(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17430044110932738140(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17430044110932738140);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17430044110932738140);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17430044110932738140);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15805871117312897156(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15805871117312897156(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15805871117312897156(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15805871117312897156(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15805871117312897156(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15805871117312897156);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15805871117312897156);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3142857211718964574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__3142857211718964574(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3142857211718964574(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3142857211718964574(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3142857211718964574(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__3142857211718964574);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3142857211718964574);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7280382801953482468(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7280382801953482468(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7280382801953482468(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7280382801953482468(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7280382801953482468(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7280382801953482468(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7280382801953482468(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7280382801953482468);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7280382801953482468);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7280382801953482468);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17604962792888188228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17604962792888188228(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17604962792888188228(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17604962792888188228(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17604962792888188228(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17604962792888188228(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17604962792888188228(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17604962792888188228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17604962792888188228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17604962792888188228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8006630551429593569(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8006630551429593569(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8006630551429593569(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8006630551429593569(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8006630551429593569(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8006630551429593569(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8006630551429593569(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8006630551429593569);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8006630551429593569);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8006630551429593569);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5520554894797632037(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5520554894797632037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5520554894797632037(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5520554894797632037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5520554894797632037(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5520554894797632037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5520554894797632037(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5520554894797632037);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5520554894797632037);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5520554894797632037);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      25 /* Input2 */, \
      25 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6615187519924437320(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 25} /* Input2 */, 
      {"input[3]", 25} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__6615187519924437320(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6615187519924437320(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6615187519924437320(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6615187519924437320(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__6615187519924437320);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6615187519924437320);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_SOFTMAX_FWD
#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9508884679017556109(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__9508884679017556109(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9508884679017556109(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__9508884679017556109(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__9508884679017556109(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_INSTANCE)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_CHANNEL)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_FAST);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_ACCURATE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_LOG)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__9508884679017556109);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__9508884679017556109);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
#endif // ENABLE_LAYER_CUDNN_SOFTMAX_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3127878204417879361(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__3127878204417879361(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3127878204417879361(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3127878204417879361(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3127878204417879361(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__3127878204417879361);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3127878204417879361);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6931586357805143848(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6931586357805143848(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6931586357805143848(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6931586357805143848(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6931586357805143848(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6931586357805143848(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6931586357805143848(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6931586357805143848);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6931586357805143848);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6931586357805143848);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13990411788332197131(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13990411788332197131(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13990411788332197131(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13990411788332197131(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13990411788332197131(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13990411788332197131);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13990411788332197131);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3877226312301302951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3877226312301302951(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3877226312301302951(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3877226312301302951(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3877226312301302951(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3877226312301302951);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3877226312301302951);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2184470228659868936(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2184470228659868936(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2184470228659868936(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2184470228659868936(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2184470228659868936(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2184470228659868936);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2184470228659868936);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1972883367709284544(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1972883367709284544(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1972883367709284544(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1972883367709284544(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1972883367709284544(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1972883367709284544);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1972883367709284544);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3033842841253778452(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3033842841253778452(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3033842841253778452(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3033842841253778452(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3033842841253778452(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3033842841253778452);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3033842841253778452);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4761995853137840126(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4761995853137840126(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4761995853137840126(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4761995853137840126(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4761995853137840126(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4761995853137840126);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4761995853137840126);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15585671821416453101(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15585671821416453101(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15585671821416453101(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15585671821416453101(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15585671821416453101(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15585671821416453101(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15585671821416453101(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15585671821416453101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15585671821416453101);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15585671821416453101);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16075416250355869351(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16075416250355869351(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16075416250355869351(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16075416250355869351(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16075416250355869351(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16075416250355869351(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16075416250355869351(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16075416250355869351);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16075416250355869351);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16075416250355869351);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16044729412100052592(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16044729412100052592(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16044729412100052592(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16044729412100052592(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16044729412100052592(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16044729412100052592);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16044729412100052592);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13334112021634779108(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13334112021634779108(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13334112021634779108(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13334112021634779108(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13334112021634779108(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13334112021634779108(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13334112021634779108(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13334112021634779108);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13334112021634779108);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13334112021634779108);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7406526817589036267(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7406526817589036267(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7406526817589036267(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7406526817589036267(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7406526817589036267(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7406526817589036267(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7406526817589036267(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7406526817589036267);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7406526817589036267);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7406526817589036267);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__663826650911241389(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__663826650911241389(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__663826650911241389(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__663826650911241389(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__663826650911241389(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__663826650911241389(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__663826650911241389(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__663826650911241389);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__663826650911241389);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__663826650911241389);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5169346068191122960(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5169346068191122960(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5169346068191122960(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5169346068191122960(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5169346068191122960(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5169346068191122960);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5169346068191122960);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9841207209399870304(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9841207209399870304(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9841207209399870304(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9841207209399870304(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9841207209399870304(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9841207209399870304(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9841207209399870304(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9841207209399870304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9841207209399870304);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9841207209399870304);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      109 /* Input2 */, \
      109 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17931335574211380832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 109} /* Input2 */, 
      {"input[3]", 109} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17931335574211380832(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17931335574211380832(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17931335574211380832(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17931335574211380832(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17931335574211380832);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17931335574211380832);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15404201480166000588(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15404201480166000588(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15404201480166000588(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15404201480166000588(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15404201480166000588(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15404201480166000588);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15404201480166000588);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18242629205493587064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__18242629205493587064(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18242629205493587064(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__18242629205493587064(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__18242629205493587064(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__18242629205493587064);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__18242629205493587064);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4598103031539503992(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4598103031539503992(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4598103031539503992(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4598103031539503992(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4598103031539503992(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4598103031539503992(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4598103031539503992(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4598103031539503992);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4598103031539503992);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4598103031539503992);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16128135446575071500(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16128135446575071500(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16128135446575071500(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16128135446575071500(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16128135446575071500(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16128135446575071500(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16128135446575071500(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16128135446575071500);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16128135446575071500);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16128135446575071500);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5137882866068375902(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5137882866068375902(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5137882866068375902(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5137882866068375902(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5137882866068375902(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5137882866068375902);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5137882866068375902);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8212661007483345825(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8212661007483345825(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8212661007483345825(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8212661007483345825(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8212661007483345825(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8212661007483345825(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8212661007483345825(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8212661007483345825);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8212661007483345825);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8212661007483345825);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14446633481389215341(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14446633481389215341(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14446633481389215341(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14446633481389215341(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14446633481389215341(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14446633481389215341(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14446633481389215341(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14446633481389215341);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14446633481389215341);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14446633481389215341);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16122548460772616121(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16122548460772616121(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16122548460772616121(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16122548460772616121(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16122548460772616121(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16122548460772616121);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16122548460772616121);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12211831076706646816(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__12211831076706646816(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12211831076706646816(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12211831076706646816(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12211831076706646816(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__12211831076706646816);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12211831076706646816);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15055598125788339075(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15055598125788339075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15055598125788339075(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15055598125788339075(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15055598125788339075(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15055598125788339075);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15055598125788339075);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6538659167388813892(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6538659167388813892(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6538659167388813892(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6538659167388813892(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6538659167388813892(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6538659167388813892);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6538659167388813892);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17142802449860017624(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17142802449860017624(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17142802449860017624(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17142802449860017624(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17142802449860017624(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17142802449860017624);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17142802449860017624);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14476008769891548852(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14476008769891548852(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14476008769891548852(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14476008769891548852(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14476008769891548852(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14476008769891548852);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14476008769891548852);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2751802189631425508(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2751802189631425508(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2751802189631425508(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2751802189631425508(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2751802189631425508(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2751802189631425508(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2751802189631425508(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2751802189631425508);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2751802189631425508);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2751802189631425508);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1792503040508058795(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1792503040508058795(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1792503040508058795(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1792503040508058795(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1792503040508058795(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1792503040508058795(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1792503040508058795(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1792503040508058795);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1792503040508058795);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1792503040508058795);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14807659138554604608(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14807659138554604608(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14807659138554604608(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14807659138554604608(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14807659138554604608(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14807659138554604608);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14807659138554604608);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15154707988443179841(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15154707988443179841(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15154707988443179841(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15154707988443179841(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15154707988443179841(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15154707988443179841);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15154707988443179841);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10597509618099620194(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__10597509618099620194(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10597509618099620194(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10597509618099620194(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10597509618099620194(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__10597509618099620194);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10597509618099620194);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13031567229670364774(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13031567229670364774(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13031567229670364774(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13031567229670364774(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13031567229670364774(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13031567229670364774);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13031567229670364774);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16252532445762493860(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16252532445762493860(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16252532445762493860(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16252532445762493860(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16252532445762493860(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16252532445762493860(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16252532445762493860(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16252532445762493860);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16252532445762493860);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16252532445762493860);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3249907742777092360(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3249907742777092360(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3249907742777092360(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3249907742777092360(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3249907742777092360(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3249907742777092360);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3249907742777092360);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      768 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15056833884971800302(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 768} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15056833884971800302(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15056833884971800302(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15056833884971800302(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15056833884971800302(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15056833884971800302);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15056833884971800302);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8765589722943066340(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8765589722943066340(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8765589722943066340(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8765589722943066340(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8765589722943066340(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8765589722943066340(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8765589722943066340(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8765589722943066340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8765589722943066340);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8765589722943066340);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11243832188556223116(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11243832188556223116(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11243832188556223116(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11243832188556223116(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11243832188556223116(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11243832188556223116);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11243832188556223116);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6721945344277081325(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__6721945344277081325(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6721945344277081325(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6721945344277081325(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6721945344277081325(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__6721945344277081325);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6721945344277081325);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14510398859888497898(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14510398859888497898(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14510398859888497898(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14510398859888497898(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14510398859888497898(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14510398859888497898(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14510398859888497898(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14510398859888497898);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14510398859888497898);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14510398859888497898);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7106078290322021565(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7106078290322021565(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7106078290322021565(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7106078290322021565(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7106078290322021565(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7106078290322021565);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7106078290322021565);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5848468794619489264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5848468794619489264(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5848468794619489264(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5848468794619489264(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5848468794619489264(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5848468794619489264);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5848468794619489264);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8118961437510535537(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8118961437510535537(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8118961437510535537(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8118961437510535537(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8118961437510535537(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8118961437510535537(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8118961437510535537(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8118961437510535537);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8118961437510535537);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8118961437510535537);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5510391693189785346(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__5510391693189785346(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5510391693189785346(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__5510391693189785346(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__5510391693189785346(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__5510391693189785346);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__5510391693189785346);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14286795786586121712(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__14286795786586121712(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14286795786586121712(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14286795786586121712(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14286795786586121712(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__14286795786586121712);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14286795786586121712);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14311761468688669122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14311761468688669122(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14311761468688669122(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14311761468688669122(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14311761468688669122(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14311761468688669122);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14311761468688669122);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3638958615899604578(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__3638958615899604578(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3638958615899604578(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3638958615899604578(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3638958615899604578(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__3638958615899604578);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3638958615899604578);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12663239932055948979(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12663239932055948979(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12663239932055948979(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12663239932055948979(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12663239932055948979(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12663239932055948979);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12663239932055948979);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6789544230408840001(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6789544230408840001(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6789544230408840001(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6789544230408840001(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6789544230408840001(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6789544230408840001);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6789544230408840001);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9445338405099391977(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9445338405099391977(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9445338405099391977(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9445338405099391977(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9445338405099391977(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9445338405099391977(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9445338405099391977(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9445338405099391977);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9445338405099391977);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9445338405099391977);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15196761235144535678(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15196761235144535678(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15196761235144535678(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15196761235144535678(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15196761235144535678(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15196761235144535678);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15196761235144535678);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4645815894248544949(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4645815894248544949(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4645815894248544949(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4645815894248544949(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4645815894248544949(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4645815894248544949(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4645815894248544949(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4645815894248544949);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4645815894248544949);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4645815894248544949);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10392879909911925260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10392879909911925260(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10392879909911925260(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10392879909911925260(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10392879909911925260(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10392879909911925260);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10392879909911925260);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13669154746902837281(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13669154746902837281(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13669154746902837281(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13669154746902837281(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13669154746902837281(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13669154746902837281);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13669154746902837281);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3681085883924400183(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3681085883924400183(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3681085883924400183(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3681085883924400183(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3681085883924400183(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3681085883924400183);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3681085883924400183);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15955344350486166641(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__15955344350486166641(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15955344350486166641(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15955344350486166641(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15955344350486166641(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__15955344350486166641);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15955344350486166641);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13542127565897969037(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13542127565897969037(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13542127565897969037(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13542127565897969037(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13542127565897969037(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13542127565897969037);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13542127565897969037);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1542679310919720550(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__1542679310919720550(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1542679310919720550(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__1542679310919720550(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1542679310919720550(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__1542679310919720550);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__1542679310919720550);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11987615444638264574(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11987615444638264574(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11987615444638264574(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11987615444638264574(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11987615444638264574(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11987615444638264574);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11987615444638264574);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__2693686925996479283(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__2693686925996479283(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__2693686925996479283(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__2693686925996479283(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__2693686925996479283(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__2693686925996479283);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__2693686925996479283);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2436024503276770456(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2436024503276770456(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2436024503276770456(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2436024503276770456(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2436024503276770456(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2436024503276770456);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2436024503276770456);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5239694270057495041(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__5239694270057495041(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5239694270057495041(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__5239694270057495041(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5239694270057495041(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__5239694270057495041)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__5239694270057495041)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447671242764405232(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7447671242764405232(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447671242764405232(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7447671242764405232(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7447671242764405232(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7447671242764405232);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7447671242764405232);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10422604032787996507(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__10422604032787996507(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10422604032787996507(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10422604032787996507(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10422604032787996507(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__10422604032787996507);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10422604032787996507);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16165378561394504272(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16165378561394504272(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16165378561394504272(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16165378561394504272(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16165378561394504272(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16165378561394504272(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16165378561394504272(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16165378561394504272);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16165378561394504272);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16165378561394504272);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1180023412133695213(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1180023412133695213(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1180023412133695213(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1180023412133695213(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1180023412133695213(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1180023412133695213(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1180023412133695213(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1180023412133695213);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1180023412133695213);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1180023412133695213);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2427542665408654858(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2427542665408654858(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2427542665408654858(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2427542665408654858(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2427542665408654858(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2427542665408654858);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2427542665408654858);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7491455168248196910(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7491455168248196910(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7491455168248196910(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7491455168248196910(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7491455168248196910(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7491455168248196910);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7491455168248196910);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1555492256466983744(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1555492256466983744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1555492256466983744(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1555492256466983744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1555492256466983744(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1555492256466983744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1555492256466983744(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1555492256466983744);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1555492256466983744);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1555492256466983744);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9357134419723915517(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9357134419723915517(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9357134419723915517(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9357134419723915517(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9357134419723915517(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9357134419723915517(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9357134419723915517(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9357134419723915517);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9357134419723915517);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9357134419723915517);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15603494436036777766(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15603494436036777766(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15603494436036777766(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15603494436036777766(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15603494436036777766(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15603494436036777766);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15603494436036777766);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9353859960165863441(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9353859960165863441(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9353859960165863441(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9353859960165863441(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9353859960165863441(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9353859960165863441);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9353859960165863441);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10162996553824167865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10162996553824167865(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10162996553824167865(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10162996553824167865(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10162996553824167865(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10162996553824167865);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10162996553824167865);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9640054253862025016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__9640054253862025016(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9640054253862025016(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__9640054253862025016(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9640054253862025016(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__9640054253862025016);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__9640054253862025016);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      24 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12198048109055535061(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12198048109055535061(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12198048109055535061(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12198048109055535061(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12198048109055535061(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12198048109055535061(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12198048109055535061(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12198048109055535061);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12198048109055535061);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12198048109055535061);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__326180294386599113(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__326180294386599113(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__326180294386599113(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__326180294386599113(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__326180294386599113(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__326180294386599113(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__326180294386599113(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__326180294386599113);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__326180294386599113);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__326180294386599113);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4343458668643624506(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4343458668643624506(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4343458668643624506(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4343458668643624506(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4343458668643624506(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4343458668643624506);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4343458668643624506);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9465020866946547625(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9465020866946547625(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9465020866946547625(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9465020866946547625(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9465020866946547625(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9465020866946547625(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9465020866946547625(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9465020866946547625);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9465020866946547625);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9465020866946547625);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6790320813440802426(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__6790320813440802426(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6790320813440802426(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6790320813440802426(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6790320813440802426(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__6790320813440802426);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6790320813440802426);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_SOFTMAX_FWD
#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      8 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__1817558133404490377(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__1817558133404490377(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__1817558133404490377(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__1817558133404490377(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__1817558133404490377(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_INSTANCE)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_CHANNEL)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_FAST);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_ACCURATE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_LOG)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__1817558133404490377);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__1817558133404490377);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
#endif // ENABLE_LAYER_CUDNN_SOFTMAX_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__165870124837147730(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__165870124837147730(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__165870124837147730(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__165870124837147730(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__165870124837147730(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__165870124837147730(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__165870124837147730(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__165870124837147730);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__165870124837147730);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__165870124837147730);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14900654629146656045(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14900654629146656045(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14900654629146656045(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14900654629146656045(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14900654629146656045(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14900654629146656045(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14900654629146656045(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14900654629146656045);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14900654629146656045);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14900654629146656045);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5400584918565374722(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5400584918565374722(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5400584918565374722(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5400584918565374722(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5400584918565374722(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5400584918565374722);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5400584918565374722);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9168223327639446256(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9168223327639446256(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9168223327639446256(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9168223327639446256(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9168223327639446256(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9168223327639446256(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9168223327639446256(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9168223327639446256);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9168223327639446256);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9168223327639446256);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__13972752370126172069(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__13972752370126172069(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__13972752370126172069(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__13972752370126172069(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__13972752370126172069(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__13972752370126172069)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__13972752370126172069)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14936398444154254017(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14936398444154254017(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14936398444154254017(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14936398444154254017(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14936398444154254017(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14936398444154254017(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14936398444154254017(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14936398444154254017);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14936398444154254017);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14936398444154254017);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12815659184766349990(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12815659184766349990(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12815659184766349990(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12815659184766349990(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12815659184766349990(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12815659184766349990(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12815659184766349990(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12815659184766349990);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12815659184766349990);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12815659184766349990);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3181637741262569401(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3181637741262569401(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3181637741262569401(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3181637741262569401(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3181637741262569401(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3181637741262569401(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3181637741262569401(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3181637741262569401);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3181637741262569401);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3181637741262569401);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16142830836991190530(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16142830836991190530(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16142830836991190530(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16142830836991190530(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16142830836991190530(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16142830836991190530(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16142830836991190530(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16142830836991190530);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16142830836991190530);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16142830836991190530);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2278111690616095735(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2278111690616095735(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2278111690616095735(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2278111690616095735(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2278111690616095735(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2278111690616095735);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2278111690616095735);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11379305046732735456(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11379305046732735456(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11379305046732735456(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11379305046732735456(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11379305046732735456(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11379305046732735456(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11379305046732735456(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11379305046732735456);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11379305046732735456);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11379305046732735456);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1487461001133025851(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1487461001133025851(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1487461001133025851(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1487461001133025851(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1487461001133025851(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1487461001133025851(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1487461001133025851(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1487461001133025851);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1487461001133025851);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1487461001133025851);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17901577878734235856(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17901577878734235856(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17901577878734235856(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17901577878734235856(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17901577878734235856(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17901577878734235856);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17901577878734235856);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5973619715981863870(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5973619715981863870(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5973619715981863870(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5973619715981863870(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5973619715981863870(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5973619715981863870);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5973619715981863870);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2872407203348563508(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2872407203348563508(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2872407203348563508(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2872407203348563508(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2872407203348563508(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2872407203348563508);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2872407203348563508);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10692068327600410525(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10692068327600410525(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10692068327600410525(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10692068327600410525(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10692068327600410525(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10692068327600410525);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10692068327600410525);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16360005952632204203(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16360005952632204203(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16360005952632204203(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16360005952632204203(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16360005952632204203(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16360005952632204203(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16360005952632204203(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16360005952632204203);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16360005952632204203);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16360005952632204203);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13351669560892507665(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13351669560892507665(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13351669560892507665(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13351669560892507665(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13351669560892507665(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13351669560892507665(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13351669560892507665(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13351669560892507665);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13351669560892507665);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13351669560892507665);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12886323434863471814(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12886323434863471814(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12886323434863471814(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12886323434863471814(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12886323434863471814(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12886323434863471814);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12886323434863471814);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11105440913517461016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11105440913517461016(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11105440913517461016(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11105440913517461016(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11105440913517461016(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11105440913517461016(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11105440913517461016(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11105440913517461016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11105440913517461016);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11105440913517461016);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11597960833206881024(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11597960833206881024(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11597960833206881024(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11597960833206881024(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11597960833206881024(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11597960833206881024(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11597960833206881024(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11597960833206881024);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11597960833206881024);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11597960833206881024);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6709215661897728002(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6709215661897728002(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6709215661897728002(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6709215661897728002(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6709215661897728002(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6709215661897728002);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6709215661897728002);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9301665391591202432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9301665391591202432(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9301665391591202432(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9301665391591202432(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9301665391591202432(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9301665391591202432);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9301665391591202432);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10314189509668379553(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10314189509668379553(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10314189509668379553(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10314189509668379553(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10314189509668379553(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10314189509668379553(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10314189509668379553(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10314189509668379553);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10314189509668379553);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10314189509668379553);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10667499030895477747(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10667499030895477747(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10667499030895477747(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10667499030895477747(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10667499030895477747(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10667499030895477747(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10667499030895477747(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10667499030895477747);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10667499030895477747);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10667499030895477747);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4609154450826567616(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__4609154450826567616(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4609154450826567616(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4609154450826567616(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4609154450826567616(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__4609154450826567616);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4609154450826567616);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4109848315519568067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4109848315519568067(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4109848315519568067(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4109848315519568067(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4109848315519568067(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4109848315519568067);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4109848315519568067);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9086691779399340701(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9086691779399340701(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9086691779399340701(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9086691779399340701(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9086691779399340701(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9086691779399340701);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9086691779399340701);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7396757981339398230(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7396757981339398230(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7396757981339398230(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7396757981339398230(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7396757981339398230(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7396757981339398230);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7396757981339398230);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9679031174148857842(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9679031174148857842(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9679031174148857842(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9679031174148857842(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9679031174148857842(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9679031174148857842(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9679031174148857842(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9679031174148857842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9679031174148857842);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9679031174148857842);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      448 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12438519106570300912(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 448} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12438519106570300912(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12438519106570300912(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12438519106570300912(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12438519106570300912(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12438519106570300912);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12438519106570300912);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17883997089170667586(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17883997089170667586(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17883997089170667586(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17883997089170667586(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17883997089170667586(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17883997089170667586);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17883997089170667586);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__396891571442930038(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__396891571442930038(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__396891571442930038(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__396891571442930038(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__396891571442930038(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__396891571442930038);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__396891571442930038);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17210956699018984656(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17210956699018984656(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17210956699018984656(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17210956699018984656(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17210956699018984656(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17210956699018984656(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17210956699018984656(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17210956699018984656);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17210956699018984656);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17210956699018984656);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4782401332905289752(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4782401332905289752(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4782401332905289752(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4782401332905289752(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4782401332905289752(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4782401332905289752(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4782401332905289752(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4782401332905289752);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4782401332905289752);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4782401332905289752);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5813066616511532213(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5813066616511532213(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5813066616511532213(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5813066616511532213(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5813066616511532213(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5813066616511532213);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5813066616511532213);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3763503517845018483(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3763503517845018483(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3763503517845018483(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3763503517845018483(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3763503517845018483(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3763503517845018483);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3763503517845018483);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14087568680559902744(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14087568680559902744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14087568680559902744(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14087568680559902744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14087568680559902744(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14087568680559902744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14087568680559902744(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14087568680559902744);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14087568680559902744);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14087568680559902744);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16401283400557289620(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16401283400557289620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16401283400557289620(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16401283400557289620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16401283400557289620(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16401283400557289620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16401283400557289620(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16401283400557289620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16401283400557289620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16401283400557289620);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2459416183922048097(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2459416183922048097(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2459416183922048097(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2459416183922048097(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2459416183922048097(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2459416183922048097);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2459416183922048097);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6749939474278784169(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6749939474278784169(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6749939474278784169(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6749939474278784169(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6749939474278784169(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6749939474278784169);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6749939474278784169);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14130314969156383122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14130314969156383122(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14130314969156383122(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14130314969156383122(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14130314969156383122(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14130314969156383122);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14130314969156383122);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3170411373773671854(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__3170411373773671854(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3170411373773671854(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3170411373773671854(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3170411373773671854(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__3170411373773671854);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3170411373773671854);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4600548175483724529(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4600548175483724529(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4600548175483724529(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4600548175483724529(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4600548175483724529(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4600548175483724529);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4600548175483724529);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10327024159906117090(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10327024159906117090(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10327024159906117090(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10327024159906117090(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10327024159906117090(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10327024159906117090);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10327024159906117090);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11607508592314380182(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11607508592314380182(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11607508592314380182(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11607508592314380182(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11607508592314380182(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11607508592314380182(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11607508592314380182(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11607508592314380182);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11607508592314380182);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11607508592314380182);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9170991041434008958(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9170991041434008958(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9170991041434008958(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9170991041434008958(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9170991041434008958(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9170991041434008958(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9170991041434008958(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9170991041434008958);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9170991041434008958);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9170991041434008958);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1512593456016497432(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1512593456016497432(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1512593456016497432(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1512593456016497432(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1512593456016497432(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1512593456016497432(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1512593456016497432(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1512593456016497432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1512593456016497432);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1512593456016497432);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__626834655664993176(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__626834655664993176(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__626834655664993176(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__626834655664993176(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__626834655664993176(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__626834655664993176);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__626834655664993176);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8457147090145436512(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8457147090145436512(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8457147090145436512(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8457147090145436512(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8457147090145436512(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8457147090145436512(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8457147090145436512(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8457147090145436512);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8457147090145436512);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8457147090145436512);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13994397718090733460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13994397718090733460(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13994397718090733460(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13994397718090733460(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13994397718090733460(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13994397718090733460);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13994397718090733460);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8796412144305172358(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8796412144305172358(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8796412144305172358(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8796412144305172358(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8796412144305172358(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8796412144305172358);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8796412144305172358);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9850519749071970615(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9850519749071970615(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9850519749071970615(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9850519749071970615(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9850519749071970615(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9850519749071970615);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9850519749071970615);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1131805356948161417(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1131805356948161417(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1131805356948161417(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1131805356948161417(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1131805356948161417(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1131805356948161417);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1131805356948161417);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__630405900720949453(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__630405900720949453(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__630405900720949453(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__630405900720949453(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__630405900720949453(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__630405900720949453(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__630405900720949453(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__630405900720949453);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__630405900720949453);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__630405900720949453);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6155735259251858881(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6155735259251858881(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6155735259251858881(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6155735259251858881(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6155735259251858881(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6155735259251858881);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6155735259251858881);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5813055457069612177(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5813055457069612177(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5813055457069612177(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5813055457069612177(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5813055457069612177(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5813055457069612177(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5813055457069612177(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5813055457069612177);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5813055457069612177);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5813055457069612177);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14698058332228047953(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14698058332228047953(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14698058332228047953(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14698058332228047953(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14698058332228047953(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14698058332228047953(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14698058332228047953(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14698058332228047953);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14698058332228047953);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14698058332228047953);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12419540829646296533(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12419540829646296533(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12419540829646296533(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12419540829646296533(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12419540829646296533(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12419540829646296533);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12419540829646296533);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      800 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7251404211828850974(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 800} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7251404211828850974(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7251404211828850974(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7251404211828850974(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7251404211828850974(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7251404211828850974);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7251404211828850974);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2150049606453090433(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2150049606453090433(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2150049606453090433(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2150049606453090433(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2150049606453090433(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2150049606453090433);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2150049606453090433);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15912519067695912008(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__15912519067695912008(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15912519067695912008(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15912519067695912008(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15912519067695912008(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__15912519067695912008);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15912519067695912008);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4197967278698491909(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4197967278698491909(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4197967278698491909(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4197967278698491909(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4197967278698491909(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4197967278698491909);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4197967278698491909);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4988800599205177295(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__4988800599205177295(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4988800599205177295(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4988800599205177295(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4988800599205177295(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__4988800599205177295);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4988800599205177295);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2324459155775731493(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2324459155775731493(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2324459155775731493(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2324459155775731493(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2324459155775731493(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2324459155775731493);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2324459155775731493);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17816525163179711824(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17816525163179711824(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17816525163179711824(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17816525163179711824(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17816525163179711824(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17816525163179711824);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17816525163179711824);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12099331537877934217(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12099331537877934217(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12099331537877934217(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12099331537877934217(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12099331537877934217(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12099331537877934217);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12099331537877934217);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17502442449222374042(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17502442449222374042(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17502442449222374042(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17502442449222374042(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17502442449222374042(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17502442449222374042(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17502442449222374042(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17502442449222374042);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17502442449222374042);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17502442449222374042);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6405659971526687269(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6405659971526687269(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6405659971526687269(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6405659971526687269(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6405659971526687269(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6405659971526687269(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6405659971526687269(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6405659971526687269);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6405659971526687269);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6405659971526687269);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      960 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8115677386954501494(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8115677386954501494(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8115677386954501494(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8115677386954501494(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8115677386954501494(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8115677386954501494(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8115677386954501494(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8115677386954501494);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8115677386954501494);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8115677386954501494);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15561043287475777638(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15561043287475777638(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15561043287475777638(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15561043287475777638(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15561043287475777638(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15561043287475777638(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15561043287475777638(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15561043287475777638);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15561043287475777638);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15561043287475777638);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      8 /* Input2 */, \
      8 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9632794189073636839(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 8} /* Input2 */, 
      {"input[3]", 8} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9632794189073636839(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9632794189073636839(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9632794189073636839(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9632794189073636839(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9632794189073636839);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9632794189073636839);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9794619547008971607(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9794619547008971607(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9794619547008971607(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9794619547008971607(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9794619547008971607(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9794619547008971607);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9794619547008971607);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6259707175505807363(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6259707175505807363(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6259707175505807363(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6259707175505807363(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6259707175505807363(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6259707175505807363(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6259707175505807363(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6259707175505807363);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6259707175505807363);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6259707175505807363);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8336469522777952305(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8336469522777952305(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8336469522777952305(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8336469522777952305(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8336469522777952305(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8336469522777952305);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8336469522777952305);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      288 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5257092925832174037(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 288} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5257092925832174037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5257092925832174037(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5257092925832174037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5257092925832174037(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5257092925832174037(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5257092925832174037(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5257092925832174037);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5257092925832174037);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5257092925832174037);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__546344794801535039(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__546344794801535039(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__546344794801535039(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__546344794801535039(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__546344794801535039(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__546344794801535039);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__546344794801535039);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__408143055156360148(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__408143055156360148(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__408143055156360148(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__408143055156360148(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__408143055156360148(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__408143055156360148);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__408143055156360148);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      800 /* Input2 */, \
      800 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8512123967790659964(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 800} /* Input2 */, 
      {"input[3]", 800} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8512123967790659964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8512123967790659964(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8512123967790659964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8512123967790659964(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8512123967790659964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8512123967790659964(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8512123967790659964);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8512123967790659964);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8512123967790659964);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7928741598971794318(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7928741598971794318(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7928741598971794318(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7928741598971794318(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7928741598971794318(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7928741598971794318);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7928741598971794318);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9482392965862756365(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9482392965862756365(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9482392965862756365(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9482392965862756365(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9482392965862756365(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9482392965862756365(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9482392965862756365(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9482392965862756365);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9482392965862756365);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9482392965862756365);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7254844495243824160(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7254844495243824160(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7254844495243824160(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7254844495243824160(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7254844495243824160(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7254844495243824160(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7254844495243824160(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7254844495243824160);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7254844495243824160);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7254844495243824160);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12685536767843751804(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__12685536767843751804(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12685536767843751804(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12685536767843751804(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12685536767843751804(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__12685536767843751804);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12685536767843751804);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      640 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15617760328074484389(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 640} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15617760328074484389(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15617760328074484389(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15617760328074484389(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15617760328074484389(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15617760328074484389);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15617760328074484389);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1963737390258567368(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1963737390258567368(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1963737390258567368(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1963737390258567368(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1963737390258567368(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1963737390258567368(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1963737390258567368(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1963737390258567368);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1963737390258567368);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1963737390258567368);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7560001783655772964(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7560001783655772964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7560001783655772964(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7560001783655772964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7560001783655772964(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7560001783655772964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7560001783655772964(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7560001783655772964);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7560001783655772964);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7560001783655772964);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__908310758918631264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__908310758918631264(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__908310758918631264(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__908310758918631264(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__908310758918631264(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__908310758918631264);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__908310758918631264);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16592692405317499543(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16592692405317499543(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16592692405317499543(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16592692405317499543(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16592692405317499543(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16592692405317499543);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16592692405317499543);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      125 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16215672852111879832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 125} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16215672852111879832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16215672852111879832(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16215672852111879832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16215672852111879832(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16215672852111879832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16215672852111879832(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16215672852111879832);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16215672852111879832);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16215672852111879832);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7661949568662209899(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7661949568662209899(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7661949568662209899(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7661949568662209899(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7661949568662209899(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7661949568662209899(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7661949568662209899(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7661949568662209899);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7661949568662209899);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7661949568662209899);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9094758725469947668(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9094758725469947668(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9094758725469947668(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9094758725469947668(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9094758725469947668(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9094758725469947668);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9094758725469947668);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__911172647281507932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__911172647281507932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__911172647281507932(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__911172647281507932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__911172647281507932(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__911172647281507932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__911172647281507932(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__911172647281507932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__911172647281507932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__911172647281507932);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8017431724469134384(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8017431724469134384(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8017431724469134384(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8017431724469134384(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8017431724469134384(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8017431724469134384(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8017431724469134384(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8017431724469134384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8017431724469134384);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8017431724469134384);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6162832716379137732(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6162832716379137732(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6162832716379137732(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6162832716379137732(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6162832716379137732(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6162832716379137732);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6162832716379137732);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7430106819168770106(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7430106819168770106(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7430106819168770106(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7430106819168770106(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7430106819168770106(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7430106819168770106(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7430106819168770106(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7430106819168770106);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7430106819168770106);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7430106819168770106);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14206094556989198446(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14206094556989198446(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14206094556989198446(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14206094556989198446(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14206094556989198446(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14206094556989198446(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14206094556989198446(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14206094556989198446);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14206094556989198446);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14206094556989198446);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14343101831499595556(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__14343101831499595556(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14343101831499595556(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14343101831499595556(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14343101831499595556(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__14343101831499595556);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14343101831499595556);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__592921826325664408(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__592921826325664408(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__592921826325664408(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__592921826325664408(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__592921826325664408(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__592921826325664408(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__592921826325664408(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__592921826325664408);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__592921826325664408);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__592921826325664408);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18411151964730267089(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18411151964730267089(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18411151964730267089(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18411151964730267089(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18411151964730267089(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18411151964730267089(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18411151964730267089(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18411151964730267089);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18411151964730267089);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18411151964730267089);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5889465342197672782(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5889465342197672782(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5889465342197672782(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5889465342197672782(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5889465342197672782(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5889465342197672782);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5889465342197672782);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088561338493429767(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5088561338493429767(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088561338493429767(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5088561338493429767(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088561338493429767(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5088561338493429767(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5088561338493429767(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5088561338493429767);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5088561338493429767);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5088561338493429767);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7683703752641314932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7683703752641314932(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7683703752641314932(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7683703752641314932(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__7683703752641314932(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__7683703752641314932);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__7683703752641314932);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14463363175362636933(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__14463363175362636933(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14463363175362636933(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14463363175362636933(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14463363175362636933(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__14463363175362636933);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14463363175362636933);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3218430921900452068(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3218430921900452068(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3218430921900452068(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3218430921900452068(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3218430921900452068(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3218430921900452068);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3218430921900452068);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3153219025103422098(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3153219025103422098(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3153219025103422098(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3153219025103422098(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3153219025103422098(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3153219025103422098);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3153219025103422098);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17503646579432937027(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17503646579432937027(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17503646579432937027(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17503646579432937027(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17503646579432937027(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17503646579432937027);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17503646579432937027);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15358132961021714853(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15358132961021714853(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15358132961021714853(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15358132961021714853(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15358132961021714853(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15358132961021714853(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15358132961021714853(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15358132961021714853);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15358132961021714853);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15358132961021714853);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12631355143882128309(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12631355143882128309(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12631355143882128309(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12631355143882128309(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12631355143882128309(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12631355143882128309(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12631355143882128309(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12631355143882128309);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12631355143882128309);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12631355143882128309);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2221530038269451328(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2221530038269451328(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2221530038269451328(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2221530038269451328(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2221530038269451328(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2221530038269451328(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2221530038269451328(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2221530038269451328);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2221530038269451328);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2221530038269451328);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10128634487427238309(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10128634487427238309(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10128634487427238309(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10128634487427238309(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10128634487427238309(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10128634487427238309);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10128634487427238309);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8060433840608488626(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8060433840608488626(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8060433840608488626(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8060433840608488626(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8060433840608488626(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8060433840608488626(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8060433840608488626(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8060433840608488626);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8060433840608488626);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8060433840608488626);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14378384711003923528(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14378384711003923528(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14378384711003923528(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14378384711003923528(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14378384711003923528(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14378384711003923528(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14378384711003923528(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14378384711003923528);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14378384711003923528);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14378384711003923528);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17586137934118905027(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__17586137934118905027(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17586137934118905027(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17586137934118905027(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17586137934118905027(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__17586137934118905027);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17586137934118905027);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9862507435931162204(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9862507435931162204(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9862507435931162204(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9862507435931162204(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9862507435931162204(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9862507435931162204);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9862507435931162204);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      9 /* PadHeight */, \
      9 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      9 /* DilationWidth */, \
      9 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17305504473669192560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 9} /* PadHeight */, 
      {"pad_width", 9} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 9} /* DilationWidth */, 
      {"dilation_width", 9} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17305504473669192560(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17305504473669192560(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17305504473669192560(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17305504473669192560(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17305504473669192560(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17305504473669192560(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17305504473669192560);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17305504473669192560);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17305504473669192560);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5834280277806930944(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5834280277806930944(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5834280277806930944(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5834280277806930944(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5834280277806930944(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5834280277806930944);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5834280277806930944);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14113278573398675878(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14113278573398675878(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14113278573398675878(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14113278573398675878(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14113278573398675878(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14113278573398675878(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14113278573398675878(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14113278573398675878);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14113278573398675878);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14113278573398675878);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9200016476934711560(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9200016476934711560(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9200016476934711560(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9200016476934711560(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9200016476934711560(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9200016476934711560);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9200016476934711560);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10017628162295537916(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10017628162295537916(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10017628162295537916(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10017628162295537916(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10017628162295537916(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10017628162295537916(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10017628162295537916(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10017628162295537916);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10017628162295537916);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10017628162295537916);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7288028555508626497(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7288028555508626497(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7288028555508626497(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7288028555508626497(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7288028555508626497(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7288028555508626497(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7288028555508626497(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7288028555508626497);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7288028555508626497);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7288028555508626497);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9255717487998662453(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9255717487998662453(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9255717487998662453(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9255717487998662453(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9255717487998662453(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9255717487998662453);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9255717487998662453);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14650837811847922768(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14650837811847922768(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14650837811847922768(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14650837811847922768(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14650837811847922768(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14650837811847922768(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14650837811847922768(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14650837811847922768);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14650837811847922768);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14650837811847922768);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11820032104681917618(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11820032104681917618(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11820032104681917618(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11820032104681917618(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11820032104681917618(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11820032104681917618(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11820032104681917618(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11820032104681917618);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11820032104681917618);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11820032104681917618);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5194186497288641435(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5194186497288641435(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5194186497288641435(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5194186497288641435(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5194186497288641435(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5194186497288641435);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5194186497288641435);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7445075578534275588(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7445075578534275588(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7445075578534275588(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7445075578534275588(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7445075578534275588(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7445075578534275588);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7445075578534275588);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13939605056698363599(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13939605056698363599(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13939605056698363599(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13939605056698363599(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13939605056698363599(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13939605056698363599(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13939605056698363599(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13939605056698363599);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13939605056698363599);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13939605056698363599);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732120110785402212(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5732120110785402212(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732120110785402212(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5732120110785402212(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732120110785402212(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5732120110785402212(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5732120110785402212(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5732120110785402212);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5732120110785402212);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5732120110785402212);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11012013634169880350(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__11012013634169880350(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11012013634169880350(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__11012013634169880350(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11012013634169880350(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__11012013634169880350);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__11012013634169880350);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17922049942033281942(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17922049942033281942(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17922049942033281942(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17922049942033281942(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17922049942033281942(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17922049942033281942(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17922049942033281942(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17922049942033281942);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17922049942033281942);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17922049942033281942);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11394198750636374400(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11394198750636374400(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11394198750636374400(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11394198750636374400(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11394198750636374400(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11394198750636374400);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11394198750636374400);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11190451349084466861(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11190451349084466861(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11190451349084466861(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11190451349084466861(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11190451349084466861(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11190451349084466861(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11190451349084466861(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11190451349084466861);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11190451349084466861);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11190451349084466861);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11898127480368102276(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11898127480368102276(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11898127480368102276(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11898127480368102276(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11898127480368102276(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11898127480368102276);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11898127480368102276);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10298495613547510130(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10298495613547510130(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10298495613547510130(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10298495613547510130(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10298495613547510130(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10298495613547510130);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10298495613547510130);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7937653275641439735(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7937653275641439735(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7937653275641439735(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7937653275641439735(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7937653275641439735(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7937653275641439735(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7937653275641439735(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7937653275641439735);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7937653275641439735);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7937653275641439735);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16559001440478473700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16559001440478473700(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16559001440478473700(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16559001440478473700(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16559001440478473700(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16559001440478473700);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16559001440478473700);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      288 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11458429312969254407(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 288} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11458429312969254407(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11458429312969254407(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11458429312969254407(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11458429312969254407(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11458429312969254407(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11458429312969254407(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11458429312969254407);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11458429312969254407);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11458429312969254407);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2161325529191989928(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2161325529191989928(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2161325529191989928(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2161325529191989928(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2161325529191989928(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2161325529191989928(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2161325529191989928(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2161325529191989928);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2161325529191989928);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2161325529191989928);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11290451428504100159(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11290451428504100159(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11290451428504100159(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11290451428504100159(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11290451428504100159(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11290451428504100159);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11290451428504100159);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3496679056930046823(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3496679056930046823(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3496679056930046823(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3496679056930046823(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3496679056930046823(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3496679056930046823);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3496679056930046823);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13957475082879010438(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13957475082879010438(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13957475082879010438(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13957475082879010438(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13957475082879010438(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13957475082879010438);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13957475082879010438);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__790056007111086867(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__790056007111086867(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__790056007111086867(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__790056007111086867(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__790056007111086867(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__790056007111086867);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__790056007111086867);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9173576122128450722(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9173576122128450722(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9173576122128450722(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9173576122128450722(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9173576122128450722(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9173576122128450722);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9173576122128450722);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      18 /* PadHeight */, \
      18 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      18 /* DilationWidth */, \
      18 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__938058176187556159(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 18} /* PadHeight */, 
      {"pad_width", 18} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 18} /* DilationWidth */, 
      {"dilation_width", 18} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__938058176187556159(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__938058176187556159(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__938058176187556159(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__938058176187556159(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__938058176187556159(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__938058176187556159(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__938058176187556159);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__938058176187556159);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__938058176187556159);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11837807102538943882(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11837807102538943882(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11837807102538943882(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11837807102538943882(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11837807102538943882(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11837807102538943882);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11837807102538943882);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4604510974261010069(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4604510974261010069(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4604510974261010069(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4604510974261010069(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4604510974261010069(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4604510974261010069(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4604510974261010069(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4604510974261010069);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4604510974261010069);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4604510974261010069);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16649314740962438056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16649314740962438056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16649314740962438056(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16649314740962438056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16649314740962438056(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16649314740962438056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16649314740962438056(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16649314740962438056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16649314740962438056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16649314740962438056);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9058587308365073763(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9058587308365073763(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9058587308365073763(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9058587308365073763(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__9058587308365073763(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__9058587308365073763);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__9058587308365073763);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3255422801048812119(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3255422801048812119(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3255422801048812119(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3255422801048812119(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3255422801048812119(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3255422801048812119);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3255422801048812119);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18320017018928554449(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18320017018928554449(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18320017018928554449(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18320017018928554449(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18320017018928554449(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18320017018928554449);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18320017018928554449);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12189557064334730594(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12189557064334730594(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12189557064334730594(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12189557064334730594(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12189557064334730594(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12189557064334730594);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12189557064334730594);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4486577586171738976(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4486577586171738976(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4486577586171738976(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4486577586171738976(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4486577586171738976(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4486577586171738976(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4486577586171738976(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4486577586171738976);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4486577586171738976);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4486577586171738976);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3299457803427586276(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3299457803427586276(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3299457803427586276(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3299457803427586276(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3299457803427586276(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3299457803427586276(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3299457803427586276(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3299457803427586276);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3299457803427586276);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3299457803427586276);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3672801068391793104(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3672801068391793104(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3672801068391793104(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3672801068391793104(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3672801068391793104(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3672801068391793104(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3672801068391793104(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3672801068391793104);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3672801068391793104);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3672801068391793104);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15697455897089340478(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15697455897089340478(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15697455897089340478(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15697455897089340478(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15697455897089340478(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15697455897089340478);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15697455897089340478);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8828032312705036277(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8828032312705036277(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8828032312705036277(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8828032312705036277(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8828032312705036277(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8828032312705036277(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8828032312705036277(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8828032312705036277);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8828032312705036277);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8828032312705036277);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12017270587531758941(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12017270587531758941(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12017270587531758941(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12017270587531758941(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12017270587531758941(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12017270587531758941);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12017270587531758941);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10508078542293447762(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10508078542293447762(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10508078542293447762(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10508078542293447762(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__10508078542293447762(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__10508078542293447762);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__10508078542293447762);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4850242659239023903(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4850242659239023903(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4850242659239023903(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4850242659239023903(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4850242659239023903(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4850242659239023903);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4850242659239023903);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15978237049050409655(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15978237049050409655(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15978237049050409655(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15978237049050409655(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15978237049050409655(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15978237049050409655);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15978237049050409655);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5635181249345578081(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5635181249345578081(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5635181249345578081(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5635181249345578081(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5635181249345578081(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5635181249345578081(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5635181249345578081(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5635181249345578081);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5635181249345578081);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5635181249345578081);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4590232753430413293(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4590232753430413293(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4590232753430413293(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4590232753430413293(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4590232753430413293(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4590232753430413293);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4590232753430413293);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10506546039286267795(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10506546039286267795(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10506546039286267795(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10506546039286267795(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10506546039286267795(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10506546039286267795(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10506546039286267795(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10506546039286267795);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10506546039286267795);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10506546039286267795);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5314873157364043149(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5314873157364043149(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5314873157364043149(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5314873157364043149(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5314873157364043149(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5314873157364043149);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5314873157364043149);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17034860938447286346(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17034860938447286346(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17034860938447286346(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17034860938447286346(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17034860938447286346(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17034860938447286346);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17034860938447286346);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17545955617348399653(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17545955617348399653(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17545955617348399653(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17545955617348399653(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17545955617348399653(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17545955617348399653(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17545955617348399653(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17545955617348399653);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17545955617348399653);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17545955617348399653);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17805154662556217975(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__17805154662556217975(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17805154662556217975(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17805154662556217975(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17805154662556217975(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__17805154662556217975);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17805154662556217975);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13425858503959786064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13425858503959786064(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13425858503959786064(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13425858503959786064(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13425858503959786064(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13425858503959786064(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13425858503959786064(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13425858503959786064);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13425858503959786064);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13425858503959786064);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2313636893561575964(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2313636893561575964(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2313636893561575964(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2313636893561575964(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2313636893561575964(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2313636893561575964);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2313636893561575964);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12886642406935314832(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12886642406935314832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12886642406935314832(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12886642406935314832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12886642406935314832(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12886642406935314832(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12886642406935314832(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12886642406935314832);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12886642406935314832);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12886642406935314832);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10805553274349085877(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10805553274349085877(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10805553274349085877(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10805553274349085877(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10805553274349085877(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10805553274349085877);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10805553274349085877);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14133576793546485587(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14133576793546485587(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14133576793546485587(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14133576793546485587(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14133576793546485587(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14133576793546485587);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14133576793546485587);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1269690311440289056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1269690311440289056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1269690311440289056(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1269690311440289056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1269690311440289056(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1269690311440289056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1269690311440289056(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1269690311440289056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1269690311440289056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1269690311440289056);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12988829915299246435(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12988829915299246435(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12988829915299246435(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12988829915299246435(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12988829915299246435(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12988829915299246435(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12988829915299246435(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12988829915299246435);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12988829915299246435);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12988829915299246435);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      5 /* PadHeight */, \
      5 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      5 /* DilationWidth */, \
      5 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17725277022713564040(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 5} /* PadHeight */, 
      {"pad_width", 5} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 5} /* DilationWidth */, 
      {"dilation_width", 5} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17725277022713564040(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17725277022713564040(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17725277022713564040(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17725277022713564040(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17725277022713564040(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17725277022713564040(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17725277022713564040);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17725277022713564040);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17725277022713564040);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11433266066606512314(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__11433266066606512314(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11433266066606512314(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__11433266066606512314(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__11433266066606512314(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__11433266066606512314);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__11433266066606512314);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15773696740255260505(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15773696740255260505(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15773696740255260505(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15773696740255260505(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15773696740255260505(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15773696740255260505(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15773696740255260505(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15773696740255260505);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15773696740255260505);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15773696740255260505);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1955092907428590622(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1955092907428590622(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1955092907428590622(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1955092907428590622(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1955092907428590622(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1955092907428590622(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1955092907428590622(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1955092907428590622);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1955092907428590622);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1955092907428590622);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17992339981773552734(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17992339981773552734(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17992339981773552734(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17992339981773552734(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17992339981773552734(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17992339981773552734(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17992339981773552734(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17992339981773552734);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17992339981773552734);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17992339981773552734);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13292570833598561528(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13292570833598561528(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13292570833598561528(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13292570833598561528(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13292570833598561528(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13292570833598561528(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13292570833598561528(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13292570833598561528);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13292570833598561528);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13292570833598561528);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1022538056749232776(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__1022538056749232776(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1022538056749232776(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__1022538056749232776(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1022538056749232776(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__1022538056749232776);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__1022538056749232776);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14564286214345587460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14564286214345587460(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14564286214345587460(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14564286214345587460(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14564286214345587460(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14564286214345587460(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14564286214345587460(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14564286214345587460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14564286214345587460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14564286214345587460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      144 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__994973349008408068(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__994973349008408068(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__994973349008408068(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__994973349008408068(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__994973349008408068(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__994973349008408068(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__994973349008408068(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__994973349008408068);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__994973349008408068);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__994973349008408068);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      160 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__731056388344985006(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__731056388344985006(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__731056388344985006(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__731056388344985006(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__731056388344985006(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__731056388344985006(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__731056388344985006(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__731056388344985006);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__731056388344985006);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__731056388344985006);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3419585052366247461(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3419585052366247461(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3419585052366247461(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3419585052366247461(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3419585052366247461(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3419585052366247461(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3419585052366247461(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3419585052366247461);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3419585052366247461);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3419585052366247461);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3036660784521435402(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3036660784521435402(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3036660784521435402(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3036660784521435402(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3036660784521435402(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3036660784521435402);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3036660784521435402);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14556028811298444056(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14556028811298444056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14556028811298444056(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14556028811298444056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14556028811298444056(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14556028811298444056(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14556028811298444056(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14556028811298444056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14556028811298444056);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14556028811298444056);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16570501054123614239(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16570501054123614239(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16570501054123614239(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16570501054123614239(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16570501054123614239(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16570501054123614239);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16570501054123614239);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11458051364243223521(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11458051364243223521(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11458051364243223521(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11458051364243223521(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11458051364243223521(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11458051364243223521);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11458051364243223521);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10021229608274130067(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__10021229608274130067(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10021229608274130067(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10021229608274130067(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10021229608274130067(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__10021229608274130067);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10021229608274130067);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__979425390678469430(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__979425390678469430(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__979425390678469430(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__979425390678469430(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__979425390678469430(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__979425390678469430(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__979425390678469430(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__979425390678469430);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__979425390678469430);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__979425390678469430);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10158557818596074302(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10158557818596074302(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10158557818596074302(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10158557818596074302(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10158557818596074302(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10158557818596074302(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10158557818596074302(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10158557818596074302);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10158557818596074302);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10158557818596074302);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      8 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2723497824417715228(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 8} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2723497824417715228(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2723497824417715228(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2723497824417715228(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2723497824417715228(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2723497824417715228(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2723497824417715228(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2723497824417715228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2723497824417715228);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2723497824417715228);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16939742965356048847(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__16939742965356048847(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16939742965356048847(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__16939742965356048847(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16939742965356048847(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__16939742965356048847);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__16939742965356048847);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5908057268551627324(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5908057268551627324(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5908057268551627324(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5908057268551627324(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__5908057268551627324(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__5908057268551627324);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__5908057268551627324);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3268289187971638460(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3268289187971638460(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3268289187971638460(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3268289187971638460(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3268289187971638460(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3268289187971638460(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3268289187971638460(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3268289187971638460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3268289187971638460);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3268289187971638460);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      26 /* Input2 */, \
      26 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10236795942157377910(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 26} /* Input2 */, 
      {"input[3]", 26} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10236795942157377910(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10236795942157377910(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10236795942157377910(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10236795942157377910(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10236795942157377910(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10236795942157377910(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10236795942157377910);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10236795942157377910);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10236795942157377910);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1471660426711566284(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1471660426711566284(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1471660426711566284(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1471660426711566284(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1471660426711566284(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1471660426711566284);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1471660426711566284);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13108391397600790511(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13108391397600790511(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13108391397600790511(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13108391397600790511(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13108391397600790511(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13108391397600790511);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13108391397600790511);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17489225495279521174(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17489225495279521174(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17489225495279521174(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17489225495279521174(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17489225495279521174(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17489225495279521174);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17489225495279521174);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4060287425292820453(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4060287425292820453(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4060287425292820453(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4060287425292820453(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4060287425292820453(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4060287425292820453);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4060287425292820453);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12792406019832412863(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12792406019832412863(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12792406019832412863(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12792406019832412863(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12792406019832412863(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12792406019832412863(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12792406019832412863(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12792406019832412863);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12792406019832412863);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12792406019832412863);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3881794679892487917(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3881794679892487917(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3881794679892487917(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3881794679892487917(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3881794679892487917(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3881794679892487917);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3881794679892487917);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      208 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3570339779809190679(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 208} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3570339779809190679(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3570339779809190679(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3570339779809190679(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3570339779809190679(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3570339779809190679(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3570339779809190679(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3570339779809190679);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3570339779809190679);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3570339779809190679);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1839023408493552583(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1839023408493552583(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1839023408493552583(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1839023408493552583(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1839023408493552583(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1839023408493552583(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1839023408493552583(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1839023408493552583);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1839023408493552583);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1839023408493552583);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13643228790299929264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13643228790299929264(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13643228790299929264(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13643228790299929264(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13643228790299929264(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13643228790299929264);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13643228790299929264);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10915513544165146341(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10915513544165146341(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10915513544165146341(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10915513544165146341(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10915513544165146341(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10915513544165146341(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10915513544165146341(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10915513544165146341);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10915513544165146341);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10915513544165146341);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6596981985080257491(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6596981985080257491(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6596981985080257491(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6596981985080257491(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6596981985080257491(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6596981985080257491);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6596981985080257491);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7770231357171823548(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7770231357171823548(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7770231357171823548(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7770231357171823548(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7770231357171823548(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7770231357171823548(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7770231357171823548(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7770231357171823548);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7770231357171823548);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7770231357171823548);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9776112599328260496(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9776112599328260496(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9776112599328260496(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9776112599328260496(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9776112599328260496(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9776112599328260496(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9776112599328260496(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9776112599328260496);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9776112599328260496);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9776112599328260496);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16691602733988558595(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__16691602733988558595(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16691602733988558595(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__16691602733988558595(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16691602733988558595(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__16691602733988558595);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__16691602733988558595);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5153581499472400127(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5153581499472400127(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5153581499472400127(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5153581499472400127(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5153581499472400127(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5153581499472400127);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5153581499472400127);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_SOFTMAX_FWD
#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      19 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__15022943773665246063(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 19} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__15022943773665246063(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__15022943773665246063(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__15022943773665246063(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__15022943773665246063(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_INSTANCE)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_CHANNEL)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_FAST);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_ACCURATE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_LOG)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__15022943773665246063);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__15022943773665246063);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
#endif // ENABLE_LAYER_CUDNN_SOFTMAX_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      1280 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4119526213311030226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 1280} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4119526213311030226(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4119526213311030226(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4119526213311030226(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4119526213311030226(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4119526213311030226(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4119526213311030226(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4119526213311030226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4119526213311030226);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4119526213311030226);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1666114835447481932(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1666114835447481932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1666114835447481932(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1666114835447481932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1666114835447481932(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1666114835447481932(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1666114835447481932(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1666114835447481932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1666114835447481932);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1666114835447481932);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      17 /* PadHeight */, \
      17 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      17 /* DilationWidth */, \
      17 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523060657704426440(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 17} /* PadHeight */, 
      {"pad_width", 17} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 17} /* DilationWidth */, 
      {"dilation_width", 17} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6523060657704426440(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523060657704426440(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6523060657704426440(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523060657704426440(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6523060657704426440(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6523060657704426440(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6523060657704426440);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6523060657704426440);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6523060657704426440);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16751649531800307863(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16751649531800307863(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16751649531800307863(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16751649531800307863(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16751649531800307863(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16751649531800307863(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16751649531800307863(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16751649531800307863);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16751649531800307863);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16751649531800307863);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17086528738770204057(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17086528738770204057(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17086528738770204057(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17086528738770204057(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17086528738770204057(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17086528738770204057(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17086528738770204057(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17086528738770204057);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17086528738770204057);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17086528738770204057);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6829390636745641283(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6829390636745641283(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6829390636745641283(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6829390636745641283(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6829390636745641283(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6829390636745641283(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6829390636745641283(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6829390636745641283);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6829390636745641283);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6829390636745641283);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__237524303834764135(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__237524303834764135(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__237524303834764135(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__237524303834764135(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__237524303834764135(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__237524303834764135(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__237524303834764135(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__237524303834764135);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__237524303834764135);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__237524303834764135);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3393788085832049670(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3393788085832049670(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3393788085832049670(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3393788085832049670(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3393788085832049670(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3393788085832049670);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3393788085832049670);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17614945975771169272(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17614945975771169272(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17614945975771169272(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17614945975771169272(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17614945975771169272(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17614945975771169272(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17614945975771169272(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17614945975771169272);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17614945975771169272);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17614945975771169272);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3823418177947489451(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3823418177947489451(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3823418177947489451(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3823418177947489451(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3823418177947489451(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3823418177947489451);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3823418177947489451);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      3 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      16 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7615252083335261782(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7615252083335261782(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7615252083335261782(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7615252083335261782(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7615252083335261782(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7615252083335261782(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7615252083335261782(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7615252083335261782);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7615252083335261782);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7615252083335261782);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18364527594097864336(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18364527594097864336(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18364527594097864336(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18364527594097864336(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18364527594097864336(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18364527594097864336(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18364527594097864336(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18364527594097864336);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18364527594097864336);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18364527594097864336);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2484813410029674878(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2484813410029674878(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2484813410029674878(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2484813410029674878(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__2484813410029674878(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__2484813410029674878);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__2484813410029674878);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10645155741700963122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10645155741700963122(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10645155741700963122(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10645155741700963122(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10645155741700963122(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10645155741700963122(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10645155741700963122(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10645155741700963122);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10645155741700963122);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10645155741700963122);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14997972831097667845(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14997972831097667845(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14997972831097667845(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14997972831097667845(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14997972831097667845(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14997972831097667845);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14997972831097667845);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12761149528321049527(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12761149528321049527(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12761149528321049527(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12761149528321049527(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12761149528321049527(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12761149528321049527(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12761149528321049527(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12761149528321049527);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12761149528321049527);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12761149528321049527);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1627285924629120882(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__1627285924629120882(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1627285924629120882(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__1627285924629120882(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__1627285924629120882(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__1627285924629120882);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__1627285924629120882);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5766242901127952951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5766242901127952951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5766242901127952951(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5766242901127952951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5766242901127952951(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5766242901127952951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5766242901127952951(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5766242901127952951);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5766242901127952951);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5766242901127952951);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12940639729362388883(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__12940639729362388883(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12940639729362388883(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12940639729362388883(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12940639729362388883(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__12940639729362388883);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12940639729362388883);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14544649644587821865(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14544649644587821865(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14544649644587821865(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14544649644587821865(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14544649644587821865(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14544649644587821865);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14544649644587821865);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11785602299271456065(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11785602299271456065(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11785602299271456065(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11785602299271456065(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11785602299271456065(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11785602299271456065);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11785602299271456065);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      1024 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11654168654301284147(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11654168654301284147(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11654168654301284147(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11654168654301284147(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11654168654301284147(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11654168654301284147(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11654168654301284147(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11654168654301284147);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11654168654301284147);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11654168654301284147);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14050970103210369773(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14050970103210369773(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14050970103210369773(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14050970103210369773(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14050970103210369773(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14050970103210369773);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14050970103210369773);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6796273787282576891(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6796273787282576891(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6796273787282576891(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6796273787282576891(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6796273787282576891(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6796273787282576891(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6796273787282576891(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6796273787282576891);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6796273787282576891);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6796273787282576891);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14610450251079802869(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14610450251079802869(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14610450251079802869(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14610450251079802869(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14610450251079802869(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14610450251079802869(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14610450251079802869(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14610450251079802869);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14610450251079802869);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14610450251079802869);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6036643915692123288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6036643915692123288(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6036643915692123288(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6036643915692123288(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6036643915692123288(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6036643915692123288(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6036643915692123288(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6036643915692123288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6036643915692123288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6036643915692123288);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2624218438314703497(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2624218438314703497(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2624218438314703497(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2624218438314703497(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2624218438314703497(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2624218438314703497);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2624218438314703497);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16873349802391711590(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16873349802391711590(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16873349802391711590(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16873349802391711590(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16873349802391711590(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16873349802391711590(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16873349802391711590(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16873349802391711590);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16873349802391711590);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16873349802391711590);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12571620407567533406(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__12571620407567533406(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12571620407567533406(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__12571620407567533406(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__12571620407567533406(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__12571620407567533406);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__12571620407567533406);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11229272385562586641(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11229272385562586641(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11229272385562586641(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11229272385562586641(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__11229272385562586641(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__11229272385562586641);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__11229272385562586641);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12642167177657921016(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12642167177657921016(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12642167177657921016(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12642167177657921016(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12642167177657921016(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12642167177657921016);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12642167177657921016);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11014428606977596729(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11014428606977596729(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11014428606977596729(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11014428606977596729(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11014428606977596729(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11014428606977596729(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11014428606977596729(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11014428606977596729);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11014428606977596729);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11014428606977596729);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13856893148723592161(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13856893148723592161(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13856893148723592161(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13856893148723592161(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13856893148723592161(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13856893148723592161(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13856893148723592161(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13856893148723592161);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13856893148723592161);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13856893148723592161);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13031034084429862219(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13031034084429862219(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13031034084429862219(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13031034084429862219(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13031034084429862219(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13031034084429862219(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13031034084429862219(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13031034084429862219);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13031034084429862219);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13031034084429862219);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5056453596798986993(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5056453596798986993(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5056453596798986993(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5056453596798986993(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5056453596798986993(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5056453596798986993(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5056453596798986993(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5056453596798986993);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5056453596798986993);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5056453596798986993);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17250701466394860042(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17250701466394860042(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17250701466394860042(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17250701466394860042(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17250701466394860042(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17250701466394860042);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17250701466394860042);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16580453746018620128(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__16580453746018620128(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16580453746018620128(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__16580453746018620128(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__16580453746018620128(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__16580453746018620128);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__16580453746018620128);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1623951481645628989(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1623951481645628989(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1623951481645628989(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1623951481645628989(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1623951481645628989(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1623951481645628989(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1623951481645628989(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1623951481645628989);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1623951481645628989);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1623951481645628989);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4430031683507009778(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4430031683507009778(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4430031683507009778(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4430031683507009778(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4430031683507009778(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4430031683507009778(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4430031683507009778(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4430031683507009778);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4430031683507009778);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4430031683507009778);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6857763245751865886(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6857763245751865886(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6857763245751865886(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6857763245751865886(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6857763245751865886(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6857763245751865886(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6857763245751865886(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6857763245751865886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6857763245751865886);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6857763245751865886);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14311892133563802428(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14311892133563802428(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14311892133563802428(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14311892133563802428(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14311892133563802428(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14311892133563802428(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14311892133563802428(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14311892133563802428);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14311892133563802428);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14311892133563802428);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2566862948728109289(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2566862948728109289(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2566862948728109289(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2566862948728109289(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2566862948728109289(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2566862948728109289);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2566862948728109289);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16512955920695172301(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16512955920695172301(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16512955920695172301(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16512955920695172301(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16512955920695172301(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16512955920695172301(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16512955920695172301(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16512955920695172301);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16512955920695172301);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16512955920695172301);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5664945990372193348(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5664945990372193348(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5664945990372193348(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5664945990372193348(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5664945990372193348(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5664945990372193348(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5664945990372193348(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5664945990372193348);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5664945990372193348);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5664945990372193348);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16609846453787706288(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16609846453787706288(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16609846453787706288(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16609846453787706288(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16609846453787706288(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16609846453787706288(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16609846453787706288(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16609846453787706288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16609846453787706288);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16609846453787706288);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6229858239253748096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6229858239253748096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6229858239253748096(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6229858239253748096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6229858239253748096(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6229858239253748096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6229858239253748096(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6229858239253748096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6229858239253748096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6229858239253748096);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__408978809476819166(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__408978809476819166(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__408978809476819166(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__408978809476819166(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__408978809476819166(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__408978809476819166(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__408978809476819166(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__408978809476819166);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__408978809476819166);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__408978809476819166);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6761365020711150409(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6761365020711150409(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6761365020711150409(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6761365020711150409(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__6761365020711150409(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__6761365020711150409);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__6761365020711150409);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12602636866043727692(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12602636866043727692(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12602636866043727692(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12602636866043727692(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12602636866043727692(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12602636866043727692);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12602636866043727692);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      24 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17896801429501330246(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 24} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17896801429501330246(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17896801429501330246(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17896801429501330246(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17896801429501330246(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17896801429501330246(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17896801429501330246(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17896801429501330246);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17896801429501330246);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17896801429501330246);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16467596251837246603(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16467596251837246603(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16467596251837246603(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16467596251837246603(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16467596251837246603(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16467596251837246603(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16467596251837246603(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16467596251837246603);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16467596251837246603);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16467596251837246603);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17805425187765630225(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__17805425187765630225(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17805425187765630225(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__17805425187765630225(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__17805425187765630225(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__17805425187765630225)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__17805425187765630225)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14185738044793648807(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14185738044793648807(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14185738044793648807(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14185738044793648807(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14185738044793648807(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14185738044793648807);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14185738044793648807);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6923733124647667136(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6923733124647667136(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6923733124647667136(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6923733124647667136(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6923733124647667136(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6923733124647667136(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6923733124647667136(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6923733124647667136);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6923733124647667136);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6923733124647667136);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16469287917064914875(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16469287917064914875(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16469287917064914875(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16469287917064914875(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16469287917064914875(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16469287917064914875);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16469287917064914875);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7900471234737995088(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7900471234737995088(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7900471234737995088(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7900471234737995088(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__7900471234737995088(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__7900471234737995088);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__7900471234737995088);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1280 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      1000 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18279752721557054746(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1280} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"filter_count", 1000} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18279752721557054746(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18279752721557054746(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18279752721557054746(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18279752721557054746(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18279752721557054746(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18279752721557054746(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18279752721557054746);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18279752721557054746);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18279752721557054746);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8114483575601041439(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8114483575601041439(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8114483575601041439(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8114483575601041439(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8114483575601041439(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8114483575601041439(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8114483575601041439(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8114483575601041439);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8114483575601041439);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8114483575601041439);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      144 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13244064044259755008(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 144} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13244064044259755008(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13244064044259755008(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13244064044259755008(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13244064044259755008(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13244064044259755008(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13244064044259755008(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13244064044259755008);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13244064044259755008);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13244064044259755008);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6983580577773534620(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6983580577773534620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6983580577773534620(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6983580577773534620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6983580577773534620(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6983580577773534620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6983580577773534620(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6983580577773534620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6983580577773534620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6983580577773534620);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17316246609014617260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__17316246609014617260(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17316246609014617260(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17316246609014617260(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17316246609014617260(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__17316246609014617260);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17316246609014617260);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15073861124557395855(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15073861124557395855(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15073861124557395855(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15073861124557395855(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15073861124557395855(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15073861124557395855);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15073861124557395855);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__9427182452500447744(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__9427182452500447744(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__9427182452500447744(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__9427182452500447744(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__9427182452500447744(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__9427182452500447744)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__9427182452500447744)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3843327891814482036(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3843327891814482036(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3843327891814482036(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3843327891814482036(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3843327891814482036(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3843327891814482036(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3843327891814482036(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3843327891814482036);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3843327891814482036);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3843327891814482036);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10462301150454468532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10462301150454468532(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10462301150454468532(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10462301150454468532(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10462301150454468532(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10462301150454468532(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10462301150454468532(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10462301150454468532);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10462301150454468532);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10462301150454468532);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15817940536137544930(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15817940536137544930(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15817940536137544930(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15817940536137544930(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15817940536137544930(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15817940536137544930(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15817940536137544930(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15817940536137544930);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15817940536137544930);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15817940536137544930);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17863739237017892746(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17863739237017892746(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17863739237017892746(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17863739237017892746(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17863739237017892746(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17863739237017892746);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17863739237017892746);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13988605591200157142(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13988605591200157142(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13988605591200157142(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13988605591200157142(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13988605591200157142(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13988605591200157142);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13988605591200157142);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17507384687778856114(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__17507384687778856114(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17507384687778856114(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17507384687778856114(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17507384687778856114(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__17507384687778856114);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17507384687778856114);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      208 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18107084319258330292(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 208} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18107084319258330292(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18107084319258330292(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18107084319258330292(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__18107084319258330292(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__18107084319258330292);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__18107084319258330292);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1063403217712418532(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1063403217712418532(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1063403217712418532(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1063403217712418532(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1063403217712418532(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1063403217712418532);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1063403217712418532);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3228023337401294231(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3228023337401294231(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3228023337401294231(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3228023337401294231(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3228023337401294231(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3228023337401294231(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3228023337401294231(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3228023337401294231);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3228023337401294231);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3228023337401294231);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__18309505941160812221(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__18309505941160812221(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__18309505941160812221(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__18309505941160812221(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__18309505941160812221(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__18309505941160812221)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__18309505941160812221)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      112 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17041906478097660364(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17041906478097660364(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17041906478097660364(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17041906478097660364(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17041906478097660364(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17041906478097660364(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17041906478097660364(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17041906478097660364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17041906478097660364);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17041906478097660364);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      12 /* PadHeight */, \
      12 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      12 /* DilationWidth */, \
      12 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15303165081504445855(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 12} /* PadHeight */, 
      {"pad_width", 12} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 12} /* DilationWidth */, 
      {"dilation_width", 12} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15303165081504445855(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15303165081504445855(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15303165081504445855(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15303165081504445855(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15303165081504445855(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15303165081504445855(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15303165081504445855);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15303165081504445855);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15303165081504445855);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      544 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15616611509473891620(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15616611509473891620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15616611509473891620(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15616611509473891620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15616611509473891620(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15616611509473891620(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15616611509473891620(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15616611509473891620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15616611509473891620);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15616611509473891620);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      24 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12207563124933204811(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 24} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12207563124933204811(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12207563124933204811(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12207563124933204811(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12207563124933204811(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12207563124933204811);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12207563124933204811);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14332603672566692859(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14332603672566692859(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14332603672566692859(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14332603672566692859(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14332603672566692859(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14332603672566692859(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14332603672566692859(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14332603672566692859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14332603672566692859);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14332603672566692859);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14642816601292186184(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14642816601292186184(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14642816601292186184(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14642816601292186184(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14642816601292186184(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14642816601292186184(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14642816601292186184(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14642816601292186184);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14642816601292186184);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14642816601292186184);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      48 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15157297887395591772(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15157297887395591772(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15157297887395591772(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15157297887395591772(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15157297887395591772(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15157297887395591772(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15157297887395591772(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15157297887395591772);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15157297887395591772);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15157297887395591772);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11532718246957595492(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11532718246957595492(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11532718246957595492(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11532718246957595492(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11532718246957595492(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11532718246957595492(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11532718246957595492(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11532718246957595492);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11532718246957595492);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11532718246957595492);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__951410723722206396(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__951410723722206396(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__951410723722206396(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__951410723722206396(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__951410723722206396(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__951410723722206396(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__951410723722206396(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__951410723722206396);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__951410723722206396);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__951410723722206396);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4922423437224682264(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4922423437224682264(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4922423437224682264(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4922423437224682264(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4922423437224682264(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4922423437224682264(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4922423437224682264(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4922423437224682264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4922423437224682264);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4922423437224682264);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10080144161047977590(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10080144161047977590(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10080144161047977590(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10080144161047977590(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10080144161047977590(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10080144161047977590(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10080144161047977590(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10080144161047977590);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10080144161047977590);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10080144161047977590);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      1024 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3018318179970473803(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 1024} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3018318179970473803(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3018318179970473803(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3018318179970473803(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3018318179970473803(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3018318179970473803(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3018318179970473803(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3018318179970473803);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3018318179970473803);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3018318179970473803);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4593610868966760473(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__4593610868966760473(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4593610868966760473(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4593610868966760473(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4593610868966760473(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__4593610868966760473);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4593610868966760473);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16939057039349510617(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16939057039349510617(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16939057039349510617(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16939057039349510617(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16939057039349510617(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16939057039349510617);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16939057039349510617);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      320 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15411078191407896720(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 320} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15411078191407896720(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15411078191407896720(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15411078191407896720(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15411078191407896720(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15411078191407896720(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15411078191407896720(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15411078191407896720);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15411078191407896720);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15411078191407896720);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12113216689161995509(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12113216689161995509(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12113216689161995509(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12113216689161995509(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12113216689161995509(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12113216689161995509);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12113216689161995509);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3039241712659323287(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3039241712659323287(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3039241712659323287(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3039241712659323287(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3039241712659323287(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3039241712659323287);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3039241712659323287);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__281512972518719813(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__281512972518719813(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__281512972518719813(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__281512972518719813(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__281512972518719813(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__281512972518719813(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__281512972518719813(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__281512972518719813);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__281512972518719813);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__281512972518719813);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4288431021331722463(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4288431021331722463(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4288431021331722463(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4288431021331722463(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4288431021331722463(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4288431021331722463);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4288431021331722463);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      136 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__947282319851499964(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__947282319851499964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__947282319851499964(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__947282319851499964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__947282319851499964(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__947282319851499964(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__947282319851499964(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__947282319851499964);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__947282319851499964);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__947282319851499964);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13166373242542520482(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__13166373242542520482(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13166373242542520482(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13166373242542520482(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13166373242542520482(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__13166373242542520482);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13166373242542520482);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8782282715593464135(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8782282715593464135(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8782282715593464135(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8782282715593464135(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8782282715593464135(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8782282715593464135);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8782282715593464135);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9894277516818708900(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9894277516818708900(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9894277516818708900(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9894277516818708900(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9894277516818708900(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9894277516818708900(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9894277516818708900(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9894277516818708900);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9894277516818708900);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9894277516818708900);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4287365764906837400(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4287365764906837400(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4287365764906837400(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4287365764906837400(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4287365764906837400(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4287365764906837400);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4287365764906837400);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13429683363279219967(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13429683363279219967(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13429683363279219967(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13429683363279219967(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__13429683363279219967(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__13429683363279219967);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__13429683363279219967);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6696850552211465018(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6696850552211465018(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6696850552211465018(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6696850552211465018(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6696850552211465018(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6696850552211465018(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6696850552211465018(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6696850552211465018);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6696850552211465018);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6696850552211465018);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      400 /* Input2 */, \
      400 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5361803806652187780(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 400} /* Input2 */, 
      {"input[3]", 400} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5361803806652187780(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5361803806652187780(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5361803806652187780(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5361803806652187780(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5361803806652187780(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5361803806652187780(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5361803806652187780);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5361803806652187780);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5361803806652187780);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      2048 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__612531744772755578(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 2048} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__612531744772755578(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__612531744772755578(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__612531744772755578(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__612531744772755578(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__612531744772755578(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__612531744772755578(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__612531744772755578);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__612531744772755578);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__612531744772755578);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      896 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4789119373443046768(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 896} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4789119373443046768(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4789119373443046768(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4789119373443046768(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4789119373443046768(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4789119373443046768);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4789119373443046768);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14631648710319210579(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14631648710319210579(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14631648710319210579(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14631648710319210579(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14631648710319210579(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14631648710319210579);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14631648710319210579);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      5 /* PadHeight */, \
      5 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      5 /* DilationWidth */, \
      5 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17965857074775142456(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 5} /* PadHeight */, 
      {"pad_width", 5} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 5} /* DilationWidth */, 
      {"dilation_width", 5} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17965857074775142456(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17965857074775142456(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17965857074775142456(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17965857074775142456(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17965857074775142456(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17965857074775142456(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17965857074775142456);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17965857074775142456);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17965857074775142456);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17837619785330935129(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17837619785330935129(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17837619785330935129(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17837619785330935129(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17837619785330935129(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17837619785330935129);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17837619785330935129);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      32 /* Input2 */, \
      32 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10765935143918565044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 32} /* Input2 */, 
      {"input[3]", 32} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10765935143918565044(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10765935143918565044(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10765935143918565044(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10765935143918565044(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10765935143918565044(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10765935143918565044(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10765935143918565044);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10765935143918565044);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10765935143918565044);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11430488060963729569(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11430488060963729569(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11430488060963729569(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11430488060963729569(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11430488060963729569(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11430488060963729569(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11430488060963729569(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11430488060963729569);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11430488060963729569);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11430488060963729569);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14118814854340673719(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14118814854340673719(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14118814854340673719(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14118814854340673719(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14118814854340673719(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14118814854340673719(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14118814854340673719(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14118814854340673719);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14118814854340673719);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14118814854340673719);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2979457388395293656(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2979457388395293656(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2979457388395293656(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2979457388395293656(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2979457388395293656(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2979457388395293656(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2979457388395293656(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2979457388395293656);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2979457388395293656);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2979457388395293656);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11541453483629853840(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11541453483629853840(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11541453483629853840(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11541453483629853840(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11541453483629853840(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11541453483629853840(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11541453483629853840(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11541453483629853840);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11541453483629853840);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11541453483629853840);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      480 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17227946426435893031(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 480} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17227946426435893031(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17227946426435893031(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17227946426435893031(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17227946426435893031(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17227946426435893031(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17227946426435893031(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17227946426435893031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17227946426435893031);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17227946426435893031);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__627085425587636890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__627085425587636890(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__627085425587636890(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__627085425587636890(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__627085425587636890(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__627085425587636890(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__627085425587636890(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__627085425587636890);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__627085425587636890);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__627085425587636890);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      6 /* PadHeight */, \
      6 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      6 /* DilationWidth */, \
      6 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16576222694463549447(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 6} /* PadHeight */, 
      {"pad_width", 6} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 6} /* DilationWidth */, 
      {"dilation_width", 6} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16576222694463549447(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16576222694463549447(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16576222694463549447(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16576222694463549447(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16576222694463549447(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16576222694463549447(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16576222694463549447);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16576222694463549447);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16576222694463549447);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      576 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18364667798343787519(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 576} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18364667798343787519(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18364667798343787519(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18364667798343787519(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18364667798343787519(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18364667798343787519(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18364667798343787519(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18364667798343787519);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18364667798343787519);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18364667798343787519);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6313181393575699195(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__6313181393575699195(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6313181393575699195(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6313181393575699195(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6313181393575699195(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__6313181393575699195);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6313181393575699195);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      3 /* StrideHeight */, \
      3 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17016064576558409541(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 3} /* StrideHeight */, 
      {"stride_width", 3} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__17016064576558409541(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17016064576558409541(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__17016064576558409541(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__17016064576558409541(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__17016064576558409541);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__17016064576558409541);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      25 /* Input2 */, \
      25 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9572195681745401999(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 25} /* Input2 */, 
      {"input[3]", 25} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9572195681745401999(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9572195681745401999(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9572195681745401999(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9572195681745401999(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9572195681745401999);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9572195681745401999);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9076939605630974803(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9076939605630974803(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9076939605630974803(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9076939605630974803(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9076939605630974803(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9076939605630974803);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9076939605630974803);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5146579765658828908(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5146579765658828908(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5146579765658828908(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5146579765658828908(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__5146579765658828908(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__5146579765658828908);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__5146579765658828908);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15017769591194545369(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15017769591194545369(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15017769591194545369(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15017769591194545369(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15017769591194545369(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15017769591194545369(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15017769591194545369(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15017769591194545369);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15017769591194545369);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15017769591194545369);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__8644597880501980009(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__8644597880501980009(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__8644597880501980009(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__8644597880501980009(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__8644597880501980009(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__8644597880501980009)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__8644597880501980009)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14300212033608517196(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14300212033608517196(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14300212033608517196(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14300212033608517196(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14300212033608517196(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14300212033608517196);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14300212033608517196);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13503209240263328572(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13503209240263328572(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13503209240263328572(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13503209240263328572(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13503209240263328572(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13503209240263328572);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13503209240263328572);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1811172756197784496(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1811172756197784496(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1811172756197784496(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1811172756197784496(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1811172756197784496(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1811172756197784496);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1811172756197784496);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5036431901618189116(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5036431901618189116(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5036431901618189116(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5036431901618189116(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5036431901618189116(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5036431901618189116(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5036431901618189116(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5036431901618189116);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5036431901618189116);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5036431901618189116);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3782401121649317066(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3782401121649317066(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3782401121649317066(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3782401121649317066(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3782401121649317066(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3782401121649317066(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3782401121649317066(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3782401121649317066);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3782401121649317066);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3782401121649317066);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      64 /* Input1 */, \
      104 /* Input2 */, \
      104 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8788046657438615035(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 104} /* Input2 */, 
      {"input[3]", 104} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8788046657438615035(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8788046657438615035(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8788046657438615035(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8788046657438615035(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8788046657438615035);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8788046657438615035);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      4096 /* Input0 */, \
      1024 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12924817072988374867(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 4096} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12924817072988374867(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12924817072988374867(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12924817072988374867(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12924817072988374867(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12924817072988374867);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12924817072988374867);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      111 /* Input2 */, \
      111 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17273928145671320068(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 111} /* Input2 */, 
      {"input[3]", 111} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17273928145671320068(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17273928145671320068(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17273928145671320068(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__17273928145671320068(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__17273928145671320068);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__17273928145671320068);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12308956224584830604(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12308956224584830604(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12308956224584830604(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12308956224584830604(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12308956224584830604(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12308956224584830604(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12308956224584830604(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12308956224584830604);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12308956224584830604);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12308956224584830604);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      224 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2080168479047082381(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2080168479047082381(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2080168479047082381(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2080168479047082381(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2080168479047082381(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2080168479047082381(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2080168479047082381(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2080168479047082381);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2080168479047082381);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2080168479047082381);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5631297194981885248(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5631297194981885248(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5631297194981885248(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5631297194981885248(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5631297194981885248(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5631297194981885248(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5631297194981885248(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5631297194981885248);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5631297194981885248);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5631297194981885248);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8534559420911665331(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8534559420911665331(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8534559420911665331(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8534559420911665331(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8534559420911665331(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8534559420911665331(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8534559420911665331(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8534559420911665331);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8534559420911665331);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8534559420911665331);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      208 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3014183682979956307(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 208} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3014183682979956307(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3014183682979956307(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3014183682979956307(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3014183682979956307(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3014183682979956307(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3014183682979956307(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3014183682979956307);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3014183682979956307);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3014183682979956307);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14517646230829843040(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14517646230829843040(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14517646230829843040(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14517646230829843040(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14517646230829843040(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14517646230829843040);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14517646230829843040);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5659003783530997495(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5659003783530997495(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5659003783530997495(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5659003783530997495(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5659003783530997495(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5659003783530997495(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5659003783530997495(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5659003783530997495);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5659003783530997495);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5659003783530997495);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16056719334589976260(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16056719334589976260(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16056719334589976260(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16056719334589976260(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16056719334589976260(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16056719334589976260(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16056719334589976260(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16056719334589976260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16056719334589976260);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16056719334589976260);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      16 /* Input1 */, \
      416 /* Input2 */, \
      416 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8234566555305914535(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 416} /* Input2 */, 
      {"input[3]", 416} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8234566555305914535(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8234566555305914535(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8234566555305914535(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__8234566555305914535(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__8234566555305914535);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__8234566555305914535);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7055294991886111265(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7055294991886111265(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7055294991886111265(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7055294991886111265(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7055294991886111265(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7055294991886111265(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7055294991886111265(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7055294991886111265);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7055294991886111265);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7055294991886111265);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15062192094608535982(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15062192094608535982(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15062192094608535982(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15062192094608535982(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15062192094608535982(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15062192094608535982(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15062192094608535982(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15062192094608535982);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15062192094608535982);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15062192094608535982);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      9 /* PadHeight */, \
      9 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      9 /* DilationWidth */, \
      9 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18196476509704237248(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 9} /* PadHeight */, 
      {"pad_width", 9} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 9} /* DilationWidth */, 
      {"dilation_width", 9} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18196476509704237248(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18196476509704237248(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18196476509704237248(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18196476509704237248(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18196476509704237248(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18196476509704237248(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18196476509704237248);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18196476509704237248);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18196476509704237248);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9941842197466462129(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9941842197466462129(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9941842197466462129(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9941842197466462129(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9941842197466462129(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9941842197466462129);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9941842197466462129);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      16 /* Input2 */, \
      16 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4171170052993831952(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 16} /* Input2 */, 
      {"input[3]", 16} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4171170052993831952(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4171170052993831952(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4171170052993831952(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4171170052993831952(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4171170052993831952);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4171170052993831952);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10309197568783680466(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10309197568783680466(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10309197568783680466(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10309197568783680466(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10309197568783680466(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10309197568783680466);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10309197568783680466);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9838831770619550541(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9838831770619550541(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9838831770619550541(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9838831770619550541(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__9838831770619550541(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__9838831770619550541);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__9838831770619550541);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16442313780842052484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16442313780842052484(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16442313780842052484(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16442313780842052484(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16442313780842052484(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16442313780842052484(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16442313780842052484(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16442313780842052484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16442313780842052484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16442313780842052484);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_SOFTMAX_FWD
#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1000 /* Input1 */, \
      1 /* Input2 */, \
      1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__12117762467682622856(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1000} /* Input1 */, 
      {"input[2]", 1} /* Input2 */, 
      {"input[3]", 1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__12117762467682622856(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<__half, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__12117762467682622856(state);
}

template <cudnnSoftmaxAlgorithm_t softmax_algorithm, cudnnSoftmaxMode_t softmax_mode>
static void LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__12117762467682622856(benchmark::State& state) {
  LAYER_CUDNN_SOFTMAX_FWD_Impl<float, softmax_algorithm, softmax_mode>(state);
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_ADD_COUNTERS__12117762467682622856(state);
}

#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, SOFTMAX_MODE) \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_INSTANCE)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \
 BENCHMARK_TEMPLATE(b, SOFTMAX_MODE, CUDNN_SOFTMAX_MODE_CHANNEL)\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES()\
  ->BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS()\
  ->UseManualTime(); \


#define BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(b)                                                                                             \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_FAST);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_ACCURATE);                                                                    \
  BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0(b, CUDNN_SOFTMAX_LOG)

BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT16__12117762467682622856);
BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD(LAYER_CUDNN_SOFTMAX_FWD_FLOAT32__12117762467682622856);

#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD0
#undef BENCHMARK_LAYER_CUDNN_SOFTMAX_FWD
#endif // ENABLE_LAYER_CUDNN_SOFTMAX_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      2 /* DilationWidth */, \
      2 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2002750516371113744(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 2} /* DilationWidth */, 
      {"dilation_width", 2} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2002750516371113744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2002750516371113744(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2002750516371113744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2002750516371113744(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2002750516371113744(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2002750516371113744(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2002750516371113744);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2002750516371113744);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2002750516371113744);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      4 /* Input2 */, \
      4 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5521400680158284547(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 4} /* Input2 */, 
      {"input[3]", 4} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__5521400680158284547(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5521400680158284547(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__5521400680158284547(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__5521400680158284547(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__5521400680158284547)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__5521400680158284547)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      3 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      64 /* FilterCount */, \
      7 /* FilterHeight */, \
      7 /* FilterWidth */, \
      3 /* PadHeight */, \
      3 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8927643059858945524(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 3} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 7} /* FilterHeight */, 
      {"filter_width", 7} /* FilterWidth */, 
      {"pad_height", 3} /* PadHeight */, 
      {"pad_width", 3} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8927643059858945524(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8927643059858945524(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8927643059858945524(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8927643059858945524(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8927643059858945524(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8927643059858945524(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8927643059858945524);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8927643059858945524);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8927643059858945524);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14932445488197911566(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14932445488197911566(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14932445488197911566(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14932445488197911566(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14932445488197911566(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14932445488197911566(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14932445488197911566(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14932445488197911566);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14932445488197911566);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14932445488197911566);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      192 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15169223140970839511(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15169223140970839511(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15169223140970839511(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15169223140970839511(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15169223140970839511(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15169223140970839511(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15169223140970839511(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15169223140970839511);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15169223140970839511);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15169223140970839511);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      32 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16246841764346109064(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16246841764346109064(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16246841764346109064(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16246841764346109064(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16246841764346109064(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16246841764346109064(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16246841764346109064(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16246841764346109064);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16246841764346109064);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16246841764346109064);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13490221687994240516(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__13490221687994240516(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13490221687994240516(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13490221687994240516(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13490221687994240516(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__13490221687994240516);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13490221687994240516);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2251880870619149161(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2251880870619149161(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2251880870619149161(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2251880870619149161(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2251880870619149161(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2251880870619149161);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2251880870619149161);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      32 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12272748779205199642(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12272748779205199642(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12272748779205199642(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12272748779205199642(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12272748779205199642(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12272748779205199642(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12272748779205199642(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12272748779205199642);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12272748779205199642);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12272748779205199642);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      352 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10506923894606583951(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 352} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10506923894606583951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10506923894606583951(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10506923894606583951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10506923894606583951(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10506923894606583951(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10506923894606583951(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10506923894606583951);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10506923894606583951);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10506923894606583951);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3504363756532891316(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__3504363756532891316(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3504363756532891316(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__3504363756532891316(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__3504363756532891316(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__3504363756532891316);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__3504363756532891316);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17549067783432105670(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17549067783432105670(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17549067783432105670(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17549067783432105670(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17549067783432105670(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17549067783432105670(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17549067783432105670(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17549067783432105670);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17549067783432105670);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17549067783432105670);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3134401165547321999(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3134401165547321999(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3134401165547321999(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3134401165547321999(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__3134401165547321999(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__3134401165547321999);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__3134401165547321999);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15665008551255952419(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15665008551255952419(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15665008551255952419(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15665008551255952419(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__15665008551255952419(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__15665008551255952419);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__15665008551255952419);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6042002869220234908(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6042002869220234908(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6042002869220234908(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6042002869220234908(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6042002869220234908(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6042002869220234908(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6042002869220234908(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6042002869220234908);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6042002869220234908);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6042002869220234908);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      32 /* Input1 */, \
      208 /* Input2 */, \
      208 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13852374489503685025(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 208} /* Input2 */, 
      {"input[3]", 208} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__13852374489503685025(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13852374489503685025(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13852374489503685025(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13852374489503685025(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__13852374489503685025);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13852374489503685025);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14573288559788325413(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14573288559788325413(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14573288559788325413(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14573288559788325413(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14573288559788325413(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14573288559788325413);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14573288559788325413);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      736 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11720903927213635730(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 736} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11720903927213635730(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11720903927213635730(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11720903927213635730(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__11720903927213635730(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__11720903927213635730);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__11720903927213635730);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3512832415282647161(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3512832415282647161(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3512832415282647161(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3512832415282647161(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3512832415282647161(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3512832415282647161(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3512832415282647161(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3512832415282647161);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3512832415282647161);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3512832415282647161);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      384 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4638136375802509216(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4638136375802509216(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4638136375802509216(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4638136375802509216(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4638136375802509216(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4638136375802509216(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4638136375802509216(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4638136375802509216);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4638136375802509216);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4638136375802509216);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      704 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17318033958679266381(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 704} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17318033958679266381(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17318033958679266381(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17318033958679266381(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17318033958679266381(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17318033958679266381);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17318033958679266381);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      8 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      16 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10703725560442169780(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 8} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10703725560442169780(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10703725560442169780(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10703725560442169780(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10703725560442169780(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10703725560442169780(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10703725560442169780(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10703725560442169780);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10703725560442169780);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10703725560442169780);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8708263895191627823(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8708263895191627823(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8708263895191627823(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8708263895191627823(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8708263895191627823(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8708263895191627823);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8708263895191627823);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13300848637909227086(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__13300848637909227086(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13300848637909227086(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__13300848637909227086(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13300848637909227086(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13300848637909227086(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__13300848637909227086(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__13300848637909227086);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__13300848637909227086);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__13300848637909227086);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13127908445823492122(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__13127908445823492122(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13127908445823492122(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13127908445823492122(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13127908445823492122(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__13127908445823492122);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13127908445823492122);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      544 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      544 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2663877145709364804(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 544} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 544} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2663877145709364804(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2663877145709364804(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2663877145709364804(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2663877145709364804(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2663877145709364804(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2663877145709364804(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2663877145709364804);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2663877145709364804);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2663877145709364804);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      352 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1718165260640453166(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 352} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1718165260640453166(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1718165260640453166(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1718165260640453166(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1718165260640453166(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1718165260640453166);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1718165260640453166);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16051011826187142146(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16051011826187142146(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16051011826187142146(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16051011826187142146(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16051011826187142146(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16051011826187142146);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16051011826187142146);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      128 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15354899777649703096(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15354899777649703096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15354899777649703096(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15354899777649703096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15354899777649703096(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15354899777649703096(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15354899777649703096(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15354899777649703096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15354899777649703096);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15354899777649703096);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17171815075545298440(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17171815075545298440(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17171815075545298440(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17171815075545298440(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17171815075545298440(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17171815075545298440(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17171815075545298440(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17171815075545298440);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17171815075545298440);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17171815075545298440);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1311441683107255882(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1311441683107255882(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1311441683107255882(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1311441683107255882(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1311441683107255882(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1311441683107255882);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1311441683107255882);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      304 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      24 /* PadHeight */, \
      24 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      24 /* DilationWidth */, \
      24 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4013657227814765575(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 304} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 24} /* PadHeight */, 
      {"pad_width", 24} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 24} /* DilationWidth */, 
      {"dilation_width", 24} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__4013657227814765575(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4013657227814765575(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__4013657227814765575(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4013657227814765575(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4013657227814765575(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__4013657227814765575(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__4013657227814765575);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__4013657227814765575);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__4013657227814765575);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      48 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9721040632572183217(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 48} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9721040632572183217(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9721040632572183217(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9721040632572183217(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9721040632572183217(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9721040632572183217(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9721040632572183217(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9721040632572183217);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9721040632572183217);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9721040632572183217);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7710152886681931639(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__7710152886681931639(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7710152886681931639(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__7710152886681931639(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7710152886681931639(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7710152886681931639(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__7710152886681931639(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__7710152886681931639);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__7710152886681931639);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__7710152886681931639);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1 /* Input1 */, \
      64 /* Input2 */, \
      64 /* Input3 */, \
      64 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11419210112456107844(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1} /* Input1 */, 
      {"input[2]", 64} /* Input2 */, 
      {"input[3]", 64} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__11419210112456107844(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11419210112456107844(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__11419210112456107844(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11419210112456107844(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11419210112456107844(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__11419210112456107844(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__11419210112456107844);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__11419210112456107844);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__11419210112456107844);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5913769220421283557(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__5913769220421283557(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5913769220421283557(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__5913769220421283557(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5913769220421283557(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5913769220421283557(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__5913769220421283557(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__5913769220421283557);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__5913769220421283557);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__5913769220421283557);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      512 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4607700037478902700(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__4607700037478902700(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4607700037478902700(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4607700037478902700(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4607700037478902700(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__4607700037478902700);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4607700037478902700);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      1024 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      160 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14851415959358289349(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 160} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14851415959358289349(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14851415959358289349(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14851415959358289349(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14851415959358289349(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14851415959358289349(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14851415959358289349(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14851415959358289349);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14851415959358289349);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14851415959358289349);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      128 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__577711926782026493(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__577711926782026493(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__577711926782026493(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__577711926782026493(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__577711926782026493(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__577711926782026493(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__577711926782026493(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__577711926782026493);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__577711926782026493);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__577711926782026493);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      128 /* Input1 */, \
      52 /* Input2 */, \
      52 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10996297362485034102(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 52} /* Input2 */, 
      {"input[3]", 52} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__10996297362485034102(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10996297362485034102(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__10996297362485034102(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__10996297362485034102(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__10996297362485034102);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__10996297362485034102);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      54 /* Input2 */, \
      54 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14915714600271802204(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 54} /* Input2 */, 
      {"input[3]", 54} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14915714600271802204(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14915714600271802204(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14915714600271802204(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14915714600271802204(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14915714600271802204);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14915714600271802204);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2375480748660778480(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2375480748660778480(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2375480748660778480(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2375480748660778480(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__2375480748660778480(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__2375480748660778480);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__2375480748660778480);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12951174302232021636(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12951174302232021636(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12951174302232021636(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12951174302232021636(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12951174302232021636(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12951174302232021636(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12951174302232021636(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12951174302232021636);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12951174302232021636);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12951174302232021636);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      384 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16545107006093604350(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 384} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16545107006093604350(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16545107006093604350(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16545107006093604350(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__16545107006093604350(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__16545107006093604350);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__16545107006093604350);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13099672516083093213(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13099672516083093213(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13099672516083093213(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13099672516083093213(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__13099672516083093213(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__13099672516083093213);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__13099672516083093213);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      224 /* Input2 */, \
      224 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12991032508717350293(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 224} /* Input2 */, 
      {"input[3]", 224} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12991032508717350293(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12991032508717350293(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12991032508717350293(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__12991032508717350293(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__12991032508717350293);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__12991032508717350293);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      672 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9132650600292318237(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 672} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__9132650600292318237(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9132650600292318237(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__9132650600292318237(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9132650600292318237(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9132650600292318237(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__9132650600292318237(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__9132650600292318237);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__9132650600292318237);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__9132650600292318237);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      112 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      112 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16855691850028491660(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 112} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 112} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16855691850028491660(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16855691850028491660(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16855691850028491660(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16855691850028491660(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16855691850028491660(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16855691850028491660(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16855691850028491660);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16855691850028491660);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16855691850028491660);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      1024 /* Input1 */, \
      12 /* Input2 */, \
      12 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15675092414502165837(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 1024} /* Input1 */, 
      {"input[2]", 12} /* Input2 */, 
      {"input[3]", 12} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15675092414502165837(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15675092414502165837(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15675092414502165837(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15675092414502165837(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15675092414502165837);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15675092414502165837);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      55 /* Input2 */, \
      55 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4323784059472996112(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 55} /* Input2 */, 
      {"input[3]", 55} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4323784059472996112(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4323784059472996112(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4323784059472996112(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__4323784059472996112(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__4323784059472996112);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__4323784059472996112);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__322112415648864964(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__322112415648864964(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__322112415648864964(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__322112415648864964(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__322112415648864964(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__322112415648864964);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__322112415648864964);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      64 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8720623216904218838(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 64} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8720623216904218838(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8720623216904218838(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8720623216904218838(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8720623216904218838(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8720623216904218838(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8720623216904218838(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8720623216904218838);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8720623216904218838);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8720623216904218838);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13248405381812419657(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__13248405381812419657(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13248405381812419657(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13248405381812419657(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13248405381812419657(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__13248405381812419657);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13248405381812419657);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1450902940694306647(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1450902940694306647(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1450902940694306647(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1450902940694306647(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__1450902940694306647(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__1450902940694306647);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__1450902940694306647);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8326457839956940536(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8326457839956940536(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8326457839956940536(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8326457839956940536(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8326457839956940536(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8326457839956940536(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8326457839956940536(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8326457839956940536);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8326457839956940536);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8326457839956940536);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      320 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6850139857686440716(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 320} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__6850139857686440716(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6850139857686440716(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__6850139857686440716(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__6850139857686440716(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__6850139857686440716);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__6850139857686440716);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18262414117481331081(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__18262414117481331081(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18262414117481331081(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__18262414117481331081(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18262414117481331081(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18262414117481331081(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__18262414117481331081(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__18262414117481331081);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__18262414117481331081);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__18262414117481331081);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1668095710755628911(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1668095710755628911(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1668095710755628911(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1668095710755628911(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1668095710755628911(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1668095710755628911);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1668095710755628911);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      256 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16456454258524609544(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__16456454258524609544(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16456454258524609544(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__16456454258524609544(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16456454258524609544(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16456454258524609544(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__16456454258524609544(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__16456454258524609544);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__16456454258524609544);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__16456454258524609544);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15629077799437787809(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__15629077799437787809(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15629077799437787809(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__15629077799437787809(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__15629077799437787809(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__15629077799437787809);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__15629077799437787809);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10225466221313296915(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10225466221313296915(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10225466221313296915(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10225466221313296915(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__10225466221313296915(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__10225466221313296915);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__10225466221313296915);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      48 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1167741318239490780(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 48} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1167741318239490780(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1167741318239490780(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1167741318239490780(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__1167741318239490780(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__1167741318239490780);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__1167741318239490780);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      608 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17310993163028160759(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 608} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17310993163028160759(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17310993163028160759(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17310993163028160759(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__17310993163028160759(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__17310993163028160759);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__17310993163028160759);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2815647000957796970(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2815647000957796970(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2815647000957796970(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2815647000957796970(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2815647000957796970(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2815647000957796970(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2815647000957796970(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2815647000957796970);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2815647000957796970);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2815647000957796970);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4318102496801181683(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__4318102496801181683(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4318102496801181683(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__4318102496801181683(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__4318102496801181683(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__4318102496801181683);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__4318102496801181683);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      224 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10801179933841102750(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 224} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10801179933841102750(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10801179933841102750(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10801179933841102750(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10801179933841102750(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10801179933841102750(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10801179933841102750(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10801179933841102750);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10801179933841102750);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10801179933841102750);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      864 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3925709713088526358(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 864} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3925709713088526358(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3925709713088526358(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3925709713088526358(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3925709713088526358(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3925709713088526358);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3925709713088526358);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6001385846236988578(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6001385846236988578(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6001385846236988578(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6001385846236988578(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__6001385846236988578(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__6001385846236988578);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__6001385846236988578);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      512 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      2 /* FilterHeight */, \
      2 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14950508429995433825(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 512} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_height", 2} /* FilterHeight */, 
      {"filter_width", 2} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__14950508429995433825(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14950508429995433825(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__14950508429995433825(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__14950508429995433825(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_INCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_AVERAGE_COUNT_EXCLUDE_PADDING)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__14950508429995433825);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__14950508429995433825);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2166042765476534633(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2166042765476534633(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2166042765476534633(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2166042765476534633(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2166042765476534633(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2166042765476534633(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2166042765476534633(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2166042765476534633);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2166042765476534633);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2166042765476534633);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      160 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14833618622216612202(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 160} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14833618622216612202(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14833618622216612202(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14833618622216612202(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14833618622216612202(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14833618622216612202);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14833618622216612202);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      112 /* Input2 */, \
      112 /* Input3 */, \
      16 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1701540730002341050(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 112} /* Input2 */, 
      {"input[3]", 112} /* Input3 */, 
      {"filter_count", 16} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1701540730002341050(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1701540730002341050(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1701540730002341050(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1701540730002341050(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1701540730002341050(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1701540730002341050(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1701540730002341050);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1701540730002341050);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1701540730002341050);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__185609824673335195(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__185609824673335195(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__185609824673335195(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__185609824673335195(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__185609824673335195(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__185609824673335195);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__185609824673335195);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      928 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15534974391322752225(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 928} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15534974391322752225(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15534974391322752225(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15534974391322752225(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15534974391322752225(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15534974391322752225(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15534974391322752225(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15534974391322752225);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15534974391322752225);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15534974391322752225);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12667027359005571790(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__12667027359005571790(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12667027359005571790(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__12667027359005571790(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12667027359005571790(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12667027359005571790(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__12667027359005571790(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__12667027359005571790);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__12667027359005571790);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__12667027359005571790);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      64 /* Input1 */, \
      199 /* Input2 */, \
      199 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14493886462319427182(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 64} /* Input1 */, 
      {"input[2]", 199} /* Input2 */, 
      {"input[3]", 199} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14493886462319427182(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14493886462319427182(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14493886462319427182(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__14493886462319427182(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__14493886462319427182);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__14493886462319427182);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      992 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3026011791631121973(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 992} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3026011791631121973(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3026011791631121973(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3026011791631121973(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3026011791631121973(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3026011791631121973);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3026011791631121973);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      192 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17288676387010416120(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 192} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17288676387010416120(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17288676387010416120(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17288676387010416120(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17288676387010416120(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17288676387010416120(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17288676387010416120(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17288676387010416120);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17288676387010416120);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17288676387010416120);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      16 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      32 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3098366328021200594(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 16} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3098366328021200594(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3098366328021200594(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3098366328021200594(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3098366328021200594(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3098366328021200594(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3098366328021200594(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3098366328021200594);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3098366328021200594);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3098366328021200594);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      0 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      512 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__900561121906293108(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 0} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__900561121906293108(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__900561121906293108(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__900561121906293108(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__900561121906293108(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__900561121906293108(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__900561121906293108(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__900561121906293108);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__900561121906293108);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__900561121906293108);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13888343110756026341(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__13888343110756026341(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13888343110756026341(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__13888343110756026341(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__13888343110756026341(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__13888343110756026341);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__13888343110756026341);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      2 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__226742211099475237(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__226742211099475237(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__226742211099475237(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__226742211099475237(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__226742211099475237(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__226742211099475237);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__226742211099475237);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      6 /* Input2 */, \
      6 /* Input3 */, \
      32 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8531958693186678756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 6} /* Input2 */, 
      {"input[3]", 6} /* Input3 */, 
      {"filter_count", 32} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8531958693186678756(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8531958693186678756(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8531958693186678756(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8531958693186678756(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8531958693186678756(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8531958693186678756(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8531958693186678756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8531958693186678756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8531958693186678756);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      528 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      256 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2484356237410403229(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 528} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__2484356237410403229(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2484356237410403229(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__2484356237410403229(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2484356237410403229(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2484356237410403229(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__2484356237410403229(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__2484356237410403229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__2484356237410403229);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__2484356237410403229);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      256 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17041236236436413484(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"filter_count", 256} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__17041236236436413484(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17041236236436413484(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__17041236236436413484(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17041236236436413484(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17041236236436413484(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__17041236236436413484(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__17041236236436413484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__17041236236436413484);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__17041236236436413484);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      416 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3174305310653683905(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 416} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3174305310653683905(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3174305310653683905(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3174305310653683905(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__3174305310653683905(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__3174305310653683905);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__3174305310653683905);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      128 /* Input1 */, \
      100 /* Input2 */, \
      100 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16630477190407833981(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 128} /* Input1 */, 
      {"input[2]", 100} /* Input2 */, 
      {"input[3]", 100} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16630477190407833981(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16630477190407833981(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16630477190407833981(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__16630477190407833981(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__16630477190407833981);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__16630477190407833981);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12613363784344162840(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12613363784344162840(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12613363784344162840(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12613363784344162840(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__12613363784344162840(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__12613363784344162840);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__12613363784344162840);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      136 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      136 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__286288181897110756(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 136} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 136} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__286288181897110756(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__286288181897110756(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__286288181897110756(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__286288181897110756(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__286288181897110756(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__286288181897110756(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__286288181897110756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__286288181897110756);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__286288181897110756);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      960 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      960 /* FilterCount */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      1 /* PadHeight */, \
      1 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1754608996109269404(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 960} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 960} /* FilterCount */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 1} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__1754608996109269404(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1754608996109269404(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__1754608996109269404(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1754608996109269404(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1754608996109269404(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__1754608996109269404(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__1754608996109269404);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__1754608996109269404);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__1754608996109269404);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      32 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      96 /* FilterCount */, \
      5 /* FilterHeight */, \
      5 /* FilterWidth */, \
      2 /* PadHeight */, \
      2 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15598793621867344147(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 32} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 5} /* FilterHeight */, 
      {"filter_width", 5} /* FilterWidth */, 
      {"pad_height", 2} /* PadHeight */, 
      {"pad_width", 2} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__15598793621867344147(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15598793621867344147(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__15598793621867344147(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15598793621867344147(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15598793621867344147(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__15598793621867344147(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__15598793621867344147);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__15598793621867344147);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__15598793621867344147);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_POOLING_FWD
#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      13 /* Input2 */, \
      13 /* Input3 */, \
      3 /* FilterHeight */, \
      3 /* FilterWidth */, \
      0 /* PadHeight */, \
      1 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
  }})

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width"})

static void BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9628937960967716226(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 13} /* Input2 */, 
      {"input[3]", 13} /* Input3 */, 
      {"filter_height", 3} /* FilterHeight */, 
      {"filter_width", 3} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 1} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
  });
}

  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT16__9628937960967716226(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<__half, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9628937960967716226(state);
  }
  
  template <cudnnPoolingMode_t pooling_mode>
  static void LAYER_CUDNN_POOLING_FWD_FLOAT32__9628937960967716226(benchmark::State& state) {
    LAYER_CUDNN_POOLING_FWD_Impl<float, pooling_mode>(state);
    BENCHMARK_LAYER_CUDNN_POOLING_FWD_ADD_COUNTERS__9628937960967716226(state);
  }
  

#define BENCHMARK_LAYER_CUDNN_POOLING_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_POOLING_MAX_DETERMINISTIC)-> \
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT16__9628937960967716226);
BENCHMARK_LAYER_CUDNN_POOLING_FWD(LAYER_CUDNN_POOLING_FWD_FLOAT32__9628937960967716226);

#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_POOLING_FWD
#endif // ENABLE_LAYER_CUDNN_POOLING_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      272 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      272 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6252850724220532152(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 272} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 272} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__6252850724220532152(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6252850724220532152(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__6252850724220532152(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6252850724220532152(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6252850724220532152(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__6252850724220532152(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__6252850724220532152);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__6252850724220532152);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__6252850724220532152);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      256 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      128 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      2 /* StrideHeight */, \
      2 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14701875724684111920(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 256} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"filter_count", 128} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 2} /* StrideHeight */, 
      {"stride_width", 2} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__14701875724684111920(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14701875724684111920(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__14701875724684111920(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14701875724684111920(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14701875724684111920(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__14701875724684111920(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__14701875724684111920);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__14701875724684111920);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__14701875724684111920);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      576 /* Input1 */, \
      14 /* Input2 */, \
      14 /* Input3 */, \
      96 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8915765536213573143(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 576} /* Input1 */, 
      {"input[2]", 14} /* Input2 */, 
      {"input[3]", 14} /* Input3 */, 
      {"filter_count", 96} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__8915765536213573143(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8915765536213573143(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__8915765536213573143(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8915765536213573143(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8915765536213573143(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__8915765536213573143(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__8915765536213573143);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__8915765536213573143);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__8915765536213573143);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      832 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      384 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3621211533282115927(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 832} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 384} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__3621211533282115927(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3621211533282115927(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__3621211533282115927(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3621211533282115927(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3621211533282115927(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__3621211533282115927(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__3621211533282115927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__3621211533282115927);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__3621211533282115927);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      224 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15196573346170993044(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 224} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15196573346170993044(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15196573346170993044(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15196573346170993044(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__15196573346170993044(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_PER_ACTIVATION)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__15196573346170993044);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__15196573346170993044);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_DROPOUT_FWD
#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      4096 /* Input1 */, \
      -1 /* Input2 */, \
      -1 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__8893079260029948025(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 4096} /* Input1 */, 
      {"input[2]", -1} /* Input2 */, 
      {"input[3]", -1} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


static void LAYER_CUDNN_DROPOUT_FWD_FLOAT16__8893079260029948025(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<__half>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__8893079260029948025(state);
}

static void LAYER_CUDNN_DROPOUT_FWD_FLOAT32__8893079260029948025(benchmark::State& state) {
  LAYER_CUDNN_DROPOUT_FWD_Impl<float>(state);
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_ADD_COUNTERS__8893079260029948025(state);
}




BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT16__8893079260029948025)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

BENCHMARK(LAYER_CUDNN_DROPOUT_FWD_FLOAT32__8893079260029948025)->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS()->\
  UseManualTime();

#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_DROPOUT_FWD_INPUT_ARG_NAMES
#endif // ENABLE_LAYER_CUDNN_DROPOUT_FWD

#ifdef ENABLE_LAYER_CUDNN_ACTIVATION_FWD
#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      192 /* Input1 */, \
      27 /* Input2 */, \
      27 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14655935227758701682(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 192} /* Input1 */, 
      {"input[2]", 27} /* Input2 */, 
      {"input[3]", 27} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14655935227758701682(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<__half, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14655935227758701682(state);
}

template <cudnnActivationMode_t activation_mode>
static void LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14655935227758701682(benchmark::State& state) {
  LAYER_CUDNN_ACTIVATION_FWD_Impl<float, activation_mode>(state);
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_ADD_COUNTERS__14655935227758701682(state);
}


#define BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_ACTIVATION_CLIPPED_RELU)-> \
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT16__14655935227758701682);
BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD(LAYER_CUDNN_ACTIVATION_FWD_FLOAT32__14655935227758701682);

#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_ACTIVATION_FWD
#endif // ENABLE_LAYER_CUDNN_ACTIVATION_FWD

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      96 /* Input1 */, \
      56 /* Input2 */, \
      56 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8795258516445131198(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 96} /* Input1 */, 
      {"input[2]", 56} /* Input2 */, 
      {"input[3]", 56} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8795258516445131198(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8795258516445131198(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8795258516445131198(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__8795258516445131198(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__8795258516445131198);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__8795258516445131198);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      144 /* Input1 */, \
      28 /* Input2 */, \
      28 /* Input3 */, \
      -1 /* Input4 */, \
      -1 /* Input5 */, \
      -1 /* Input6 */, \
      -1 /* Input7 */, \
  }})

#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "input[4]", "input[5]", "input[6]", "input[7]"})

static void BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4799595604915823162(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 144} /* Input1 */, 
      {"input[2]", 28} /* Input2 */, 
      {"input[3]", 28} /* Input3 */, 
      {"input[4]", -1} /* Input4 */, 
      {"input[5]", -1} /* Input5 */, 
      {"input[6]", -1} /* Input6 */, 
      {"input[7]", -1} /* Input7 */, 
  });
}


template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4799595604915823162(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<__half, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4799595604915823162(state);
}

template <cudnnBatchNormMode_t batchnorm_mode>
static void LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4799595604915823162(benchmark::State& state) {
  LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_Impl<float, batchnorm_mode>(state);
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_ADD_COUNTERS__4799595604915823162(state);
}


#define BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_BATCHNORM_SPATIAL_PERSISTENT)-> \
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT16__4799595604915823162);
BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE(LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_FLOAT32__4799595604915823162);

#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE
#endif // ENABLE_LAYER_CUDNN_BATCHNORM_FWD_INFERENCE

#ifdef ENABLE_LAYER_CUDNN_CONV_FWD
#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS() \
  Args({{ \
      1 /* Input0 */, \
      2048 /* Input1 */, \
      7 /* Input2 */, \
      7 /* Input3 */, \
      512 /* FilterCount */, \
      1 /* FilterHeight */, \
      1 /* FilterWidth */, \
      0 /* PadHeight */, \
      0 /* PadWidth */, \
      1 /* StrideHeight */, \
      1 /* StrideWidth */, \
      1 /* DilationWidth */, \
      1 /* DilationHeight */, \
  }})

#define BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES() \
  ArgNames({"input[0]", "input[1]", "input[2]", "input[3]", "filter_count", "filter_height", "filter_width", "pad_height", "pad_width", "stride_height", "stride_width", "dilation_height", "dilation_width"})

static void BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10825409433500925890(benchmark::State& state) {
  state.counters.insert({
      {"input[0]", 1} /* Input0 */, 
      {"input[1]", 2048} /* Input1 */, 
      {"input[2]", 7} /* Input2 */, 
      {"input[3]", 7} /* Input3 */, 
      {"filter_count", 512} /* FilterCount */, 
      {"filter_height", 1} /* FilterHeight */, 
      {"filter_width", 1} /* FilterWidth */, 
      {"pad_height", 0} /* PadHeight */, 
      {"pad_width", 0} /* PadWidth */, 
      {"stride_height", 1} /* StrideHeight */, 
      {"stride_width", 1} /* StrideWidth */, 
      {"dilation_height", 1} /* DilationWidth */, 
      {"dilation_width", 1} /* DilationHeight */, 
  });
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT16__10825409433500925890(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10825409433500925890(state);
}



template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_FLOAT32__10825409433500925890(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<float, convolution_algorithm>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10825409433500925890(state);
}



#ifdef CUDNN_SUPPORTS_TENSOR_OPS

template <cudnnConvolutionFwdAlgo_t convolution_algorithm>
static void LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10825409433500925890(benchmark::State& state) {
  
    LAYER_CUDNN_CONV_FWD_Impl<__half, convolution_algorithm, CUDNN_TENSOR_OP_MATH>(state);
  
  BENCHMARK_LAYER_CUDNN_CONV_FWD_ADD_COUNTERS__10825409433500925890(state);
}

#endif //  CUDNN_SUPPORTS_TENSOR_OPS



#define BENCHMARK_LAYER_CUDNN_CONV_FWD(b) \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_GEMM)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_DIRECT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_FFT_TILING)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \
 BENCHMARK_TEMPLATE(b, CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED)-> \
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES()->\
  BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS()->\
  UseManualTime(); \


BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT16__10825409433500925890);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_FLOAT32__10825409433500925890);
BENCHMARK_LAYER_CUDNN_CONV_FWD(LAYER_CUDNN_CONV_FWD_TENSORCOREHALF__10825409433500925890);

#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARGS
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD_INPUT_ARG_NAMES
#undef BENCHMARK_LAYER_CUDNN_CONV_FWD
#endif // ENABLE_LAYER_CUDNN_CONV_FWD
